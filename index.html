<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>AI Workstation & GPU Performance Report (2026)</title>
  <style>
    :root {
      --table-border-color: #ddd;
      --table-header-bg: #f2f2f2;
      --table-row-hover-bg: #f5f5f5;
      --font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
      --accent-color: #007bff;
      --text-color: #333;
      --bg-color: #fff;
      --note-bg-color: #f9f9f9;
      --border-radius: 6px;
    }
    body { 
      font-family: var(--font-family); 
      margin: 0; 
      padding: 1.5em; 
      background-color: var(--bg-color);
      color: var(--text-color);
      line-height: 1.6;
    }
    header {
      text-align: center;
      margin-bottom: 2em;
    }
    h1 {
      font-size: 2.2em;
      margin-bottom: 0.2em;
    }
    .subtitle {
      font-size: 1.1em;
      color: #666;
    }
    .tabs {
      display: flex;
      border-bottom: 2px solid var(--table-border-color);
      margin-bottom: 1.5em;
    }
    .tab-button {
      padding: 10px 20px;
      cursor: pointer;
      border: none;
      background-color: transparent;
      font-size: 1em;
      border-bottom: 2px solid transparent;
      margin-bottom: -2px;
    }
    .tab-button.active {
      border-bottom-color: var(--accent-color);
      font-weight: 600;
    }
    .tab-content { display: none; }
    .tab-content.active { display: block; }
    #controls {
      display: flex;
      gap: 1em;
      margin-bottom: 1.5em;
      align-items: center;
      flex-wrap: wrap;
    }
    #controls input, #controls select {
      padding: 0.6em;
      border: 1px solid #ccc;
      border-radius: var(--border-radius);
      font-size: 0.95em;
    }
    table { 
      border-collapse: collapse; 
      width: 100%; 
      margin-top: 1em;
      box-shadow: 0 1px 3px rgba(0,0,0,0.05);
    }
    th, td { 
      border: 1px solid var(--table-border-color); 
      padding: 0.75em; 
      text-align: left; 
      vertical-align: top;
    }
    th { 
      background: var(--table-header-bg); 
      cursor: pointer; 
      font-weight: 600;
      position: sticky;
      top: 0;
    }
    tr:hover { background: var(--table-row-hover-bg); }
    .notes-cell {
      font-size: 0.9em;
      line-height: 1.5;
      background-color: var(--note-bg-color);
    }
    .notes-cell ul {
      padding-left: 1.2em;
      margin: 0;
    }
    .notes-cell li {
      margin-bottom: 0.5em;
    }
    .highlight-best {
      background-color: #e8f5e9; /* Light green */
      font-weight: bold;
    }
    .highlight-best td:first-child::before {
      content: '';
    }
    #diy-comparison {
      padding: 1em;
    }
    #diy-comparison h3 {
      margin-top: 0;
    }
    .comparison-grid {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
      gap: 1.5em;
    }
    .comparison-card {
      border: 1px solid var(--table-border-color);
      border-radius: var(--border-radius);
      padding: 1.5em;
    }
    .comparison-card h4 {
      margin-top: 0;
      color: var(--accent-color);
    }
    .price-diff {
      font-weight: bold;
    }
    .price-diff.positive { color: #28a745; }
    .price-diff.negative { color: #dc3545; }

    /* ── GPU Rankings tab ─────────────────────────────────────── */
    #gpu-controls { display: flex; gap: 0.8em; flex-wrap: wrap; margin-bottom: 1.2em; align-items: center; }
    #gpu-controls select, #gpu-controls input { padding: 0.55em 0.75em; border: 1px solid #ccc; border-radius: var(--border-radius); font-size: 0.93em; }
    .mfr-filter-group { display: flex; gap: 0.4em; }
    .mfr-btn {
      padding: 0.45em 1em; border: 2px solid #ccc; border-radius: 20px;
      background: #fff; cursor: pointer; font-size: 0.9em; font-weight: 500; transition: all 0.15s;
    }
    .mfr-btn.active-nvidia  { border-color: #76b900; background: #76b900; color: #fff; }
    .mfr-btn.active-amd     { border-color: #ed1c24; background: #ed1c24; color: #fff; }
    .mfr-btn.active-apple   { border-color: #555;    background: #555;    color: #fff; }
    .mfr-btn.active-all     { border-color: var(--accent-color); background: var(--accent-color); color: #fff; }
    .score-bar-wrap { display: flex; align-items: center; gap: 0.5em; min-width: 120px; }
    .score-bar { height: 10px; border-radius: 5px; background: var(--accent-color); transition: width 0.3s; }
    .score-num { font-size: 0.85em; white-space: nowrap; }
    .mfr-badge {
      display: inline-block; padding: 2px 8px; border-radius: 10px; font-size: 0.78em;
      font-weight: 700; letter-spacing: 0.04em;
    }
    .badge-nvidia { background: #d4edda; color: #155724; }
    .badge-amd    { background: #fde8e8; color: #721c24; }
    .badge-apple  { background: #e2e3e5; color: #383d41; }
    .mfr-comparison-section { margin-top: 2em; }
    .mfr-comparison-section h3 { margin-bottom: 0.8em; }
    .mfr-cards { display: grid; grid-template-columns: repeat(auto-fit, minmax(280px, 1fr)); gap: 1.2em; }
    .mfr-card { border-radius: var(--border-radius); padding: 1.4em; border: 1px solid var(--table-border-color); }
    .mfr-card.nvidia { border-top: 4px solid #76b900; }
    .mfr-card.amd    { border-top: 4px solid #ed1c24; }
    .mfr-card.apple  { border-top: 4px solid #555; }
    .mfr-card h4 { margin: 0 0 0.7em; font-size: 1.05em; }
    .mfr-card ul { margin: 0; padding-left: 1.2em; font-size: 0.92em; line-height: 1.7; }
    .mfr-card .verdict { margin-top: 0.8em; padding: 0.6em 0.9em; background: var(--note-bg-color); border-radius: 4px; font-size: 0.88em; }
    #gpuTable th { white-space: nowrap; }

    /* ── Best Match Finder tab ────────────────────────────────── */
    #bestmatch { padding: 0; }
    .finder-layout { display: grid; grid-template-columns: 340px 1fr; gap: 2em; align-items: start; }
    @media (max-width: 800px) { .finder-layout { grid-template-columns: 1fr; } }
    .finder-form { background: var(--note-bg-color); border: 1px solid var(--table-border-color); border-radius: var(--border-radius); padding: 1.6em; }
    .finder-form h3 { margin-top: 0; }
    .form-group { margin-bottom: 1.1em; }
    .form-group label { display: block; font-weight: 600; margin-bottom: 0.35em; font-size: 0.95em; }
    .form-group select, .form-group input[type=range] { width: 100%; padding: 0.55em 0.7em; border: 1px solid #ccc; border-radius: var(--border-radius); font-size: 0.93em; box-sizing: border-box; }
    .budget-display { font-size: 1.05em; font-weight: 700; color: var(--accent-color); margin-top: 0.25em; }
    .os-btn-group { display: flex; gap: 0.4em; flex-wrap: wrap; }
    .os-btn {
      flex: 1; padding: 0.55em 0.5em; border: 2px solid #ccc; border-radius: var(--border-radius);
      background: #fff; cursor: pointer; font-size: 0.9em; font-weight: 500; text-align: center; transition: all 0.15s;
    }
    .os-btn.active { border-color: var(--accent-color); background: var(--accent-color); color: #fff; }
    .find-btn {
      width: 100%; padding: 0.75em; background: var(--accent-color); color: #fff;
      border: none; border-radius: var(--border-radius); font-size: 1em; font-weight: 700;
      cursor: pointer; margin-top: 0.4em; transition: opacity 0.15s;
    }
    .find-btn:hover { opacity: 0.88; }
    .result-area { min-height: 200px; }
    .result-placeholder { color: #999; font-style: italic; margin-top: 1em; }
    .result-card {
      border: 2px solid var(--accent-color); border-radius: var(--border-radius);
      padding: 1.6em; background: #f0f7ff;
    }
    .result-card .result-rank { font-size: 0.82em; color: #666; text-transform: uppercase; letter-spacing: 0.06em; }
    .result-card .result-name { font-size: 1.5em; font-weight: 700; margin: 0.15em 0 0.5em; }
    .result-card .result-meta { display: flex; flex-wrap: wrap; gap: 0.6em; margin-bottom: 1em; }
    .result-meta-chip {
      background: #fff; border: 1px solid #ccc; border-radius: 20px;
      padding: 0.25em 0.75em; font-size: 0.85em; font-weight: 500;
    }
    .result-card .result-why { font-size: 0.93em; line-height: 1.6; }
    .result-card .result-why strong { color: var(--accent-color); }
    .alt-results { margin-top: 1.4em; }
    .alt-results h4 { margin-bottom: 0.6em; font-size: 0.95em; color: #555; }
    .alt-card {
      border: 1px solid var(--table-border-color); border-radius: var(--border-radius);
      padding: 0.9em 1.1em; margin-bottom: 0.7em; background: #fff;
      display: flex; justify-content: space-between; align-items: flex-start; gap: 1em;
    }
    .alt-card .alt-name { font-weight: 600; }
    .alt-card .alt-reason { font-size: 0.85em; color: #666; margin-top: 0.2em; }
    .alt-card .alt-price { font-weight: 700; white-space: nowrap; color: #333; }
    .no-result { padding: 1.5em; background: #fff8e1; border: 1px solid #ffe082; border-radius: var(--border-radius); }

    /* ── Compare Systems tab ──────────────────────────────────── */
    #compare { padding: 0; }
    .compare-layout { display: grid; grid-template-columns: 280px 1fr; gap: 2em; align-items: start; }
    @media (max-width: 860px) { .compare-layout { grid-template-columns: 1fr; } }
    .compare-sidebar { position: sticky; top: 1em; background: var(--note-bg-color); border: 1px solid var(--table-border-color); border-radius: var(--border-radius); padding: 1.4em; }
    .compare-sidebar h3 { margin-top: 0; font-size: 1em; }
    .cmp-group { margin-bottom: 1.1em; }
    .cmp-group label { display: block; font-weight: 600; font-size: 0.86em; margin-bottom: 0.3em; color: #555; text-transform: uppercase; letter-spacing: 0.05em; }
    .cmp-group select { width: 100%; padding: 0.5em 0.65em; border: 1px solid #ccc; border-radius: var(--border-radius); font-size: 0.92em; box-sizing: border-box; }
    .cmp-run-btn { width: 100%; padding: 0.7em; background: var(--accent-color); color: #fff; border: none; border-radius: var(--border-radius); font-size: 0.97em; font-weight: 700; cursor: pointer; margin-top: 0.5em; }
    .cmp-run-btn:hover { opacity: 0.88; }
    .compare-main { min-height: 300px; }
    .cmp-placeholder { color: #999; font-style: italic; padding: 1em 0; }
    .cmp-systems-bar { display: grid; gap: 1em; margin-bottom: 1.4em; }
    .cmp-sys-header { padding: 1em 1.3em; border-radius: var(--border-radius); border-top: 4px solid var(--accent-color); background: #f0f7ff; }
    .cmp-sys-header.sys-nvidia  { border-top-color: #76b900; background: #f1fbe8; }
    .cmp-sys-header.sys-amd     { border-top-color: #ed1c24; background: #fff0f0; }
    .cmp-sys-header.sys-apple   { border-top-color: #555;    background: #f4f4f4; }
    .cmp-sys-header .sys-name   { font-size: 1.1em; font-weight: 700; margin-bottom: 0.2em; }
    .cmp-sys-header .sys-sub    { font-size: 0.82em; color: #666; }
    .cmp-section { margin-bottom: 1.6em; border: 1px solid var(--table-border-color); border-radius: var(--border-radius); overflow: hidden; }
    .cmp-section-title { background: var(--table-header-bg); padding: 0.6em 1em; font-weight: 700; font-size: 0.88em; border-bottom: 1px solid var(--table-border-color); }
    .cmp-table { width: 100%; border-collapse: collapse; font-size: 0.9em; }
    .cmp-table td, .cmp-table th { padding: 0.6em 0.9em; border-bottom: 1px solid var(--table-border-color); vertical-align: middle; }
    .cmp-table th { background: var(--table-header-bg); font-weight: 600; font-size: 0.82em; white-space: nowrap; }
    .cmp-table tr:last-child td, .cmp-table tr:last-child th { border-bottom: none; }
    .cmp-table .row-label { color: #555; font-size: 0.85em; width: 160px; font-weight: 600; }
    .cmp-val { font-weight: 600; }
    .cmp-bar-wrap { display: flex; align-items: center; gap: 0.5em; }
    .cmp-bar { height: 8px; border-radius: 4px; background: var(--accent-color); min-width: 2px; display: inline-block; }
    .cmp-bar.bar-nvidia { background: #76b900; }
    .cmp-bar.bar-amd    { background: #ed1c24; }
    .cmp-bar.bar-apple  { background: #888; }
    .cmp-delta { display: inline-block; padding: 0.12em 0.55em; border-radius: 3px; font-size: 0.78em; font-weight: 700; margin-left: 0.4em; }
    .cmp-delta.faster  { background: #d4edda; color: #155724; }
    .cmp-delta.slower  { background: #f8d7da; color: #721c24; }
    .cmp-delta.same    { background: #e2e3e5; color: #383d41; }
    .cmp-winner-badge  { display: inline-block; background: var(--accent-color); color: #fff; font-size: 0.72em; padding: 0.15em 0.55em; border-radius: 3px; margin-left: 0.4em; vertical-align: middle; font-weight: 700; }
    .model-fit-grid { display: grid; grid-template-columns: repeat(auto-fill, minmax(120px, 1fr)); gap: 0.6em; padding: 0.9em; }
    .mf-cell { border: 1px solid var(--table-border-color); border-radius: 4px; padding: 0.55em 0.7em; text-align: center; font-size: 0.82em; }
    .mf-cell .mf-size { font-weight: 700; font-size: 1em; }
    .mf-cell .mf-label { font-size: 0.72em; color: #666; margin-top: 0.1em; }
    .mf-cell.fit-yes     { background: #d4edda; border-color: #a3d4ae; }
    .mf-cell.fit-partial { background: #fff3cd; border-color: #e5c44a; }
    .mf-cell.fit-no      { background: #f8d7da; border-color: #e4a0a6; }
    .impact-positive { color: #28a745; font-weight: 600; }
    .impact-negative { color: #dc3545; font-weight: 600; }
    .factor-key { display: inline-block; width: 10px; height: 10px; border-radius: 2px; margin-right: 0.3em; vertical-align: middle; }

    /* ── Optimization Guide tab ───────────────────────────────── */
    #optimization { padding: 0; }
    .opt-intro { font-size: 0.93em; color: #555; margin-bottom: 1.5em; line-height: 1.7; }
    .opt-section { margin-bottom: 2em; }
    .opt-section > h3 { margin-bottom: 0.6em; font-size: 1.05em; padding-bottom: 0.4em; border-bottom: 2px solid var(--table-border-color); }
    .opt-cards { display: grid; grid-template-columns: repeat(auto-fit, minmax(290px, 1fr)); gap: 1.2em; }
    .opt-card { border: 1px solid var(--table-border-color); border-radius: var(--border-radius); overflow: hidden; }
    .opt-card-header { padding: 0.75em 1.1em; font-weight: 700; font-size: 0.92em; display: flex; align-items: center; gap: 0.6em; border-bottom: 1px solid var(--table-border-color); }
    .opt-card-header.opt-nvidia { background: #f1fbe8; border-left: 4px solid #76b900; }
    .opt-card-header.opt-amd    { background: #fff0f0; border-left: 4px solid #ed1c24; }
    .opt-card-header.opt-apple  { background: #f4f4f4; border-left: 4px solid #555; }
    .opt-card-header.opt-general{ background: #f0f7ff; border-left: 4px solid var(--accent-color); }
    .opt-card-body { padding: 1em 1.1em; font-size: 0.9em; }
    .opt-card-body ul { margin: 0; padding-left: 1.2em; }
    .opt-card-body li { margin-bottom: 0.6em; line-height: 1.5; }
    .opt-table { width: 100%; border-collapse: collapse; margin-top: 0.8em; font-size: 0.88em; }
    .opt-table th, .opt-table td { padding: 0.55em 0.75em; border: 1px solid var(--table-border-color); text-align: left; }
    .opt-table th { background: var(--table-header-bg); font-weight: 600; }
    .speedup-tag { display: inline-block; padding: 0.15em 0.55em; border-radius: 3px; font-size: 0.82em; font-weight: 700; }
    .speedup-high   { background: #d4edda; color: #155724; }
    .speedup-medium { background: #fff3cd; color: #856404; }
    .speedup-low    { background: #f8d7da; color: #721c24; }
    .speedup-neutral{ background: #e2e3e5; color: #383d41; }
  </style>
</head>
<body>
<header>
  <h1>AI Workstation & GPU Performance Report (2026)</h1>
  <p class="subtitle">A comparative analysis of pre-built and DIY systems for local AI development.</p>
</header>

<div class="tabs">
  <button class="tab-button active" onclick="openTab(event, 'prebuilt')">Pre-built Systems</button>
  <button class="tab-button" onclick="openTab(event, 'diy')">DIY vs. Pre-built</button>
  <button class="tab-button" onclick="openTab(event, 'gpus'); initGpuTab()">GPU Rankings</button>
  <button class="tab-button" onclick="openTab(event, 'bestmatch')">Best Match Finder</button>
  <button class="tab-button" onclick="openTab(event, 'compare')">Compare Systems</button>
  <button class="tab-button" onclick="openTab(event, 'optimization')">Optimization Guide</button>
  <a href="benchmark.html" class="tab-button" style="text-decoration:none;margin-left:auto">Benchmark My System</a>
</div>

<div id="prebuilt" class="tab-content active">
  <div id="controls">
    <input type="text" id="searchInput" onkeyup="filterTable()" placeholder="Search by model, GPU, brand...">
    <select id="modelFilter" onchange="filterTable()">
      <option value="">Filter by Best Model...</option>
      <option>70B+ (Unquantized)</option>
      <option>120B+ (Quantized)</option>
      <option>30B-70B</option>
      <option>7B-30B</option>
    </select>
    <select id="typeFilter" onchange="filterTable()">
      <option value="">Filter by Type...</option>
      <option>Desktop</option>
      <option>Laptop</option>
    </select>
    <select id="osFilter" onchange="filterTable()">
      <option value="">Filter by OS...</option>
      <option>Windows</option>
      <option>macOS</option>
      <option>Linux</option>
    </select>
  </div>
  <p>Table columns are sortable. All systems are evaluated for AI/ML workloads. TG = Tokens/sec Generation (batch size 1).</p>
  <table id="hardwareTable">
    <thead>
      <tr>
        <th onclick="sortTable(0)">Rank</th>
        <th onclick="sortTable(1)">Model</th>
        <th onclick="sortTable(2)">Type</th>
        <th onclick="sortTable(3)">GPU</th>
        <th onclick="sortTable(4)">VRAM</th>
        <th onclick="sortTable(5)">Best Model (Size)</th>
        <th onclick="sortTable(6)">Tokens/sec (TG)</th>
        <th onclick="sortTable(7)">Price</th>
        <th onclick="sortTable(8)">Notes & Evidence</th>
      </tr>
    </thead>
    <tbody>
      <tr class="highlight-best">
        <td>1</td>
        <td>HP Z8 G4 (Custom)</td>
        <td>Desktop</td>
        <td>Quad NVIDIA A100 40GB</td>
        <td>160 GB</td>
        <td>70B+ (Unquantized)</td>
        <td>~540</td>
        <td>$30000</td>
        <td class="notes-cell"><ul>
          <li><strong>Data Center Power:</strong> Can run unquantized 70B models entirely in VRAM.</li>
          <li><strong>NVLink Advantage:</strong> Full peer-to-peer communication between GPUs is critical for massive model inference.</li>
          <li><strong>Secondary Market:</strong> Often available on the used market, making it a "budget" entry into high-end AI.</li>
          <li><strong>Power/Cooling:</strong> Requires significant power and cooling infrastructure not suitable for a typical office.</li>
        </ul></td>
      </tr>
      <tr>
        <td>2</td>
        <td>Dell Precision 7960 Tower</td>
        <td>Desktop</td>
        <td>Quad NVIDIA RTX 5000 Ada</td>
        <td>128 GB</td>
        <td>120B+ (Quantized)</td>
        <td>~450</td>
        <td>$20000</td>
        <td class="notes-cell"><ul>
          <li><strong>Enterprise Grade:</strong> Built for high-concurrency professional AI, with certified drivers and support.</li>
          <li><strong>RTX 6000 Ada Recommended:</strong> For max performance, users often upgrade to RTX 6000 Ada GPUs for more VRAM and CUDA cores.</li>
          <li><strong>NVLink Bridge:</strong> Essential for combining GPU memory pools effectively.</li>
        </ul></td>
      </tr>
      <tr>
        <td>3</td>
        <td>Apple Mac Studio (M3 Ultra)</td>
        <td>Desktop</td>
        <td>Apple 80-core GPU</td>
        <td>192 GB (Unified)</td>
        <td>120B+ (Quantized)</td>
        <td>~97</td>
        <td>$8000</td>
        <td class="notes-cell"><ul>
          <li><strong>Memory Bandwidth King:</strong> Unmatched unified memory is ideal for huge model inference.</li>
          <li><strong>Underwhelming TG?:</strong> Some users report tokens/sec (TG) is lower than expected vs. high-end NVIDIA cards (e.g., 97 t/s vs 136 t/s on an RTX 3090 for some models).</li>
          <li><strong>Silent & Efficient:</strong> Consumes far less power and is virtually silent compared to NVIDIA workstations.</li>
          <li><strong>MLX Framework:</strong> Apple's MLX framework can be 20-30% faster than other libraries like Ollama on Apple Silicon.</li>
        </ul></td>
      </tr>
      <tr>
        <td>4</td>
        <td>HP OMEN 45L (2025)</td>
        <td>Desktop</td>
        <td>NVIDIA RTX 5090</td>
        <td>32 GB</td>
        <td>70B+ (Quantized)</td>
        <td>~140</td>
        <td>$4500</td>
        <td class="notes-cell"><ul>
          <li><strong>New Flagship:</strong> The RTX 5090 offers ~1,792 GB/s bandwidth, a significant leap for memory-bound LLM tasks.</li>
          <li><strong>MoE Performance:</strong> Excels at Mixture-of-Experts models, with benchmarks showing 100-140 t/s generation.</li>
          <li><strong>"Space Heater":</strong> Early reports suggest very high power draw and thermal output, requiring excellent case airflow.</li>
        </ul></td>
      </tr>
       <tr>
        <td>5</td>
        <td>Alienware Aurora R16</td>
        <td>Desktop</td>
        <td>NVIDIA RTX 4090</td>
        <td>24 GB</td>
        <td>30B-70B</td>
        <td>~95</td>
        <td>$4000</td>
        <td class="notes-cell"><ul>
          <li><strong>Community Favorite:</strong> Widely regarded as the best value for running 7B to 30B models locally.</li>
          <li><strong>30% Slower than 5090:</strong> A significant performance gap, but at a much lower price point.</li>
          <li><strong>Stable & Supported:</strong> Mature drivers and extensive community support for AI/ML workloads.</li>
        </ul></td>
      </tr>
      <tr>
        <td>6</td>
        <td>Lenovo ThinkStation P620</td>
        <td>Desktop</td>
        <td>Dual NVIDIA RTX A6000</td>
        <td>96 GB</td>
        <td>120B+ (Quantized)</td>
        <td>~300</td>
        <td>$15000</td>
        <td class="notes-cell"><ul>
            <li><strong>CPU Powerhouse:</strong> The Threadripper Pro 5995WX CPU provides massive PCIe bandwidth for multiple GPUs.</li>
            <li><strong>PSU/Fitment Issues:</strong> Real-world user threads mention PSU limitations and tight physical space when fitting two high-end GPUs.</li>
            <li><strong>Dual A6000 Recommended:</strong> Community advice points to using two RTX A6000s as a potent, albeit expensive, combination.</li>
        </ul></td>
      </tr>
      <tr>
        <td>7</td>
        <td>Asus ROG Strix Scar 18</td>
        <td>Laptop</td>
        <td>NVIDIA RTX 5090 (Laptop)</td>
        <td>24 GB</td>
        <td>30B-70B</td>
        <td>~80</td>
        <td>$4500</td>
        <td class="notes-cell"><ul>
            <li><strong>Desktop-Class Laptop GPU:</strong> Benchmarks from similar MSI models show impressive performance but with caveats.</li>
            <li><strong>Thermal Throttling:</strong> The main challenge is keeping the GPU cool enough to sustain peak performance.</li>
            <li><strong>VRAM is 24GB:</strong> Note: Leaked specs showing 32GB were incorrect for the laptop version.</li>
        </ul></td>
      </tr>
      <tr>
        <td>8</td>
        <td>Apple MacBook Pro 16 (M3 Max)</td>
        <td>Laptop</td>
        <td>Apple 30-core GPU</td>
        <td>64 GB (Unified)</td>
        <td>30B-70B</td>
        <td>~40</td>
        <td>$3000</td>
        <td class="notes-cell"><ul>
            <li><strong>Popular with ML Engineers:</strong> Excellent for development and testing on the go due to its large unified memory.</li>
            <li><strong>"Not for Heavy Training":</strong> Community consensus is that it's an inference machine, not for training large models from scratch.</li>
            <li><strong>Performance:</strong> Can achieve 35-40 t/s on 14B parameter models (Q4 quantized).</li>
        </ul></td>
      </tr>
      <tr>
        <td>9</td>
        <td>Asus ROG Flow Z13 (2025)</td>
        <td>Laptop</td>
        <td>AMD Radeon 780M (iGPU)</td>
        <td>32 GB (Shared)</td>
        <td>7B-30B</td>
        <td>~25</td>
        <td>$2100</td>
        <td class="notes-cell"><ul>
            <li><strong>The "Unicorn" APU:</strong> The Ryzen AI 300 series is highly praised for its powerful integrated GPU.</li>
            <li><strong>ROCm vs. Vulkan:</strong> Performance depends heavily on the backend used; vLLM support is a major plus.</li>
            <li><strong>BIOS Gotcha:</strong> Users report needing to manually allocate 32GB of system RAM to the iGPU in BIOS for best performance.</li>
        </ul></td>
      </tr>
      <tr>
        <td>10</td>
        <td>ASUS Zenbook A14 (2025)</td>
        <td>Laptop</td>
        <td>Qualcomm Adreno NPU</td>
        <td>32 GB (Shared)</td>
        <td>7B-30B</td>
        <td>~6</td>
        <td>$1800</td>
        <td class="notes-cell"><ul>
            <li><strong>On-Device AI Focus:</strong> Designed for efficient, low-power AI tasks.</li>
            <li><strong>NPU Not Yet Supported:</strong> The powerful NPU is not currently usable by most open-source LLM tools (e.g., llama.cpp).</li>
            <li><strong>Community Verdict:</strong> Currently not recommended for serious local LLM work until software support matures. Performance is low (~6 t/s on 32B models).</li>
        </ul></td>
      </tr>
    </tbody>
  </table>
</div>

<div id="diy" class="tab-content">
  <div id="diy-comparison">
    <h3>DIY Cost Savings & Performance Analysis</h3>
    <p>Building your own PC can offer significant savings and tailored performance. Here’s a comparison against popular pre-built options.</p>
    <div class="comparison-grid">
      <div class="comparison-card">
        <h4>DIY Build: "The 4090 Killer"</h4>
        <p><strong>Components:</strong> Intel Core i7-14700K, NVIDIA RTX 4090 (24GB), 64GB DDR5 RAM, 2TB NVMe SSD, 1000W PSU.</p>
        <p><strong>Estimated DIY Cost:</strong> $3,200</p>
        <p><strong>Comparable Pre-built:</strong> Alienware Aurora R16 (~$4,000)</p>
        <p><strong>Savings:</strong> <span class="price-diff positive">~$800 (20%)</span></p>
        <p><strong>Notes:</strong> Offers identical core performance for LLMs as the pre-built but requires assembly. You get to choose higher quality components (e.g., PSU, motherboard).</p>
      </div>
      <div class="comparison-card">
        <h4>DIY Build: "Dual 3090 Workstation"</h4>
        <p><strong>Components:</strong> AMD Ryzen 9 7950X, 2x NVIDIA RTX 3090 (24GB, used), 128GB DDR5 RAM, 4TB NVMe SSD, 1600W PSU.</p>
        <p><strong>Estimated DIY Cost:</strong> $4,500 (with used GPUs)</p>
        <p><strong>Comparable Pre-built:</strong> Older high-end workstations (~$8,000+ new)</p>
        <p><strong>Savings:</strong> <span class="price-diff positive">~$3,500+ (40-50%)</span></p>
        <p><strong>Notes:</strong> By using second-hand RTX 3090s (known for their 24GB VRAM), you can build a 48GB VRAM machine for a fraction of the cost of a new professional workstation. NVLink support is absent, but performance is still excellent for the price.</p>
      </div>
       <div class="comparison-card">
        <h4>When to Buy Pre-built</h4>
        <p><strong>Warranty & Support:</strong> A single point of contact for troubleshooting is invaluable for mission-critical work.</p>
        <p><strong>Certified Drivers:</strong> Workstations from Dell, HP, and Lenovo come with optimized and certified drivers for professional applications.</p>
        <p><strong>Time is Money:</strong> The time spent building, configuring, and troubleshooting a DIY machine can outweigh the cost savings for professionals.</p>
        <p><strong>Specialized Hardware:</strong> Systems with multiple pro-grade GPUs (like Quad A100s) require chassis and motherboards not typically available to consumers.</p>
      </div>
    </div>
  </div>
</div>

<!-- ═══════════════════════════════════════════════════════════ GPU RANKINGS TAB -->
<div id="gpus" class="tab-content">
  <div id="gpu-controls">
    <div class="mfr-filter-group">
      <button class="mfr-btn active-all" id="mfr-all"    onclick="setMfrFilter('all')">All</button>
      <button class="mfr-btn"            id="mfr-nvidia" onclick="setMfrFilter('NVIDIA')">NVIDIA</button>
      <button class="mfr-btn"            id="mfr-amd"    onclick="setMfrFilter('AMD')">AMD</button>
      <button class="mfr-btn"            id="mfr-apple"  onclick="setMfrFilter('Apple')">Apple</button>
    </div>
    <select id="gpuSortSelect" onchange="renderGpuTable()">
      <option value="score">Sort: Effectiveness Score</option>
      <option value="tg7b">Sort: Tokens/sec (7B Q4)</option>
      <option value="bw">Sort: Memory Bandwidth</option>
      <option value="vram">Sort: VRAM</option>
      <option value="price">Sort: Price (Low→High)</option>
      <option value="valueScore">Sort: Value (t/s per $)</option>
    </select>
    <input type="text" id="gpuSearch" oninput="renderGpuTable()" placeholder="Search GPUs…" style="min-width:160px;">
  </div>
  <p style="margin-top:0;font-size:0.92em;color:#555;">
    <strong>Effectiveness Score</strong> blends raw token generation speed, memory bandwidth, and VRAM capacity into a single 0–100 index.
    <strong>Value Score</strong> = tokens/sec per $100 spent. Click any column header to sort.
  </p>
  <table id="gpuTable">
    <thead>
      <tr>
        <th onclick="gpuSortCol('rank')">#</th>
        <th onclick="gpuSortCol('name')">GPU</th>
        <th onclick="gpuSortCol('mfr')">Maker</th>
        <th onclick="gpuSortCol('vram')">VRAM (GB)</th>
        <th onclick="gpuSortCol('bw')">Bandwidth (GB/s)</th>
        <th onclick="gpuSortCol('tg7b')">TG 7B Q4 (t/s)</th>
        <th onclick="gpuSortCol('tg70b')">TG 70B Q4 (t/s)</th>
        <th onclick="gpuSortCol('price')">Price (USD)</th>
        <th onclick="gpuSortCol('valueScore')">Value Score</th>
        <th onclick="gpuSortCol('score')">Effectiveness</th>
        <th onclick="gpuSortCol('bestFor')">Best For</th>
      </tr>
    </thead>
    <tbody id="gpuTableBody"></tbody>
  </table>

  <div class="mfr-comparison-section">
    <h3>Manufacturer Tradeoffs</h3>
    <div class="mfr-cards">
      <div class="mfr-card nvidia">
        <h4>NVIDIA</h4>
        <ul>
          <li><strong>Speed king:</strong> Highest tokens/sec for models that fit in VRAM — RTX 5090 at 1,792 GB/s is unmatched on the consumer market.</li>
          <li><strong>CUDA ecosystem:</strong> Every AI framework (PyTorch, vLLM, TensorRT, llama.cpp) works out of the box. Zero configuration headaches.</li>
          <li><strong>VRAM ceiling:</strong> Consumer cards top out at 32GB (5090). Professional cards (RTX 6000 Ada: 48GB, A100: 80GB) cost significantly more.</li>
          <li><strong>Training:</strong> Best-in-class for fine-tuning and training with full CUDA support.</li>
          <li><strong>Power draw:</strong> RTX 5090 ≈ 575W — a space heater. Dedicated airflow/cooling needed.</li>
          <li><strong>P2P blocked:</strong> Consumer multi-GPU peer-to-peer is disabled by default in drivers (workaround requires patched kernel modules).</li>
        </ul>
        <div class="verdict"><strong>Choose NVIDIA if:</strong> you run models &le;30B and want maximum speed, or need fine-tuning &amp; training.</div>
      </div>
      <div class="mfr-card amd">
        <h4>AMD</h4>
        <ul>
          <li><strong>Ryzen AI Max+ 395:</strong> The standout — 128GB unified memory at ~$1,500–2,500 is the cheapest way to run 70B models on x86.</li>
          <li><strong>Bandwidth:</strong> Radeon RX 7900 XTX has 960 GB/s — competitive with NVIDIA RTX 4090 (1,008 GB/s), at a lower price.</li>
          <li><strong>ROCm ecosystem:</strong> Improving rapidly (vLLM support added 2024), but still lags CUDA. Occasional bugs, fewer tutorials.</li>
          <li><strong>Vulkan backend:</strong> llama.cpp Vulkan is often faster than ROCm for pure inference; ~20% advantage in recent tests.</li>
          <li><strong>Discrete VRAM limit:</strong> RX 7900 XTX tops at 24GB — same ceiling as RTX 4090.</li>
          <li><strong>BIOS gotcha:</strong> Ryzen AI Max laptops default to 32GB GPU allocation; must manually set to 96–128GB for proper performance.</li>
        </ul>
        <div class="verdict"><strong>Choose AMD if:</strong> you need 70B+ on a budget (Ryzen AI Max+ 395), or want competitive discrete GPU performance at lower cost than NVIDIA.</div>
      </div>
      <div class="mfr-card apple">
        <h4>Apple Silicon</h4>
        <ul>
          <li><strong>Unified memory:</strong> M3 Ultra at 192–512GB is the only consumer path to running 200B+ models locally without a multi-GPU rig.</li>
          <li><strong>MLX framework:</strong> Apple-optimized; 20–30% faster than Ollama for inference. Speculative decoding can roughly double token generation.</li>
          <li><strong>Token generation speed:</strong> M3 Ultra (~800 GB/s) loses to RTX 5090 (1,792 GB/s) for models that fit in VRAM — ~97 t/s vs ~250 t/s on 7B.</li>
          <li><strong>Power efficiency:</strong> Mac Studio at ~150W vs RTX 5090 system at ~800W+. Silent under moderate load.</li>
          <li><strong>Concurrency:</strong> Handles concurrent requests worse than NVIDIA; not ideal for multi-user serving.</li>
          <li><strong>No GPU-only option:</strong> Apple Silicon is integrated — you buy the whole computer, not a card.</li>
          <li><strong>macOS only:</strong> No Linux, no CUDA. Only MLX, llama.cpp, and Ollama with Metal.</li>
        </ul>
        <div class="verdict"><strong>Choose Apple if:</strong> you need to run very large models (70B+) in a quiet, efficient package, or you're already in the Apple ecosystem.</div>
      </div>
    </div>
  </div>
</div>

<!-- ═══════════════════════════════════════════════════════ BEST MATCH FINDER TAB -->
<div id="bestmatch" class="tab-content">
  <h3 style="margin-top:0;">Find Your Best Match</h3>
  <p style="margin-top:-0.5em;color:#555;font-size:0.94em;">Tell us your preferences and we'll narrow the field to the best hardware option for you.</p>
  <div class="finder-layout">
    <div class="finder-form">
      <h3 style="margin-top:0;">Your Requirements</h3>

      <div class="form-group">
        <label>Preferred OS / Platform</label>
        <div class="os-btn-group">
          <button class="os-btn" id="os-macos"   onclick="setOs('macos')">macOS</button>
          <button class="os-btn" id="os-windows" onclick="setOs('windows')">Windows</button>
          <button class="os-btn" id="os-linux"   onclick="setOs('linux')">Linux</button>
          <button class="os-btn active" id="os-any" onclick="setOs('any')">Any</button>
        </div>
      </div>

      <div class="form-group">
        <label>Form Factor</label>
        <select id="ff-select">
          <option value="any">Any (desktop or laptop)</option>
          <option value="desktop">Desktop only</option>
          <option value="laptop">Laptop / portable</option>
        </select>
      </div>

      <div class="form-group">
        <label>Budget <span class="budget-display" id="budgetLabel">$5,000</span></label>
        <input type="range" id="budgetSlider" min="500" max="35000" step="500" value="5000"
               oninput="document.getElementById('budgetLabel').textContent = '$' + Number(this.value).toLocaleString()">
        <div style="display:flex;justify-content:space-between;font-size:0.78em;color:#888;margin-top:2px;">
          <span>$500</span><span>$35,000</span>
        </div>
      </div>

      <div class="form-group">
        <label>Largest model I want to run</label>
        <select id="modelSize-select">
          <option value="7">7B – Small, fast (Mistral 7B, Gemma 7B)</option>
          <option value="14">14B – Medium (Qwen2.5 14B, Phi-4)</option>
          <option value="30" selected>30B – Large (Qwen 30B, Gemma 27B)</option>
          <option value="70">70B – XL (Llama 3 70B, Qwen 72B)</option>
          <option value="120">120B+ – Massive (Llama 3 405B, DeepSeek 671B)</option>
        </select>
      </div>

      <div class="form-group">
        <label>Priority</label>
        <select id="priority-select">
          <option value="speed">Speed (tokens/sec)</option>
          <option value="value" selected>Value (best for budget)</option>
          <option value="capacity">Capacity (fit largest model)</option>
          <option value="portable">Portability</option>
        </select>
      </div>

      <button class="find-btn" onclick="runFinder()">Find Best Match →</button>
    </div>

    <div class="result-area" id="finderResults">
      <p class="result-placeholder">&#8592; Set your requirements and click "Find Best Match"</p>
    </div>
  </div>
</div>

<!-- ═══════════════════════════════════════════════════════ COMPARE SYSTEMS TAB -->
<div id="compare" class="tab-content">
  <h3 style="margin-top:0">Compare Hardware for Local AI</h3>
  <p style="margin-top:-0.4em;color:#555;font-size:0.94em;margin-bottom:1.4em">
    Select up to three systems or chips. Every metric shows the absolute value plus a percentage difference relative to the first system you choose.
    RAM and VRAM capacity directly determine which model sizes fit; memory bandwidth is the primary driver of token generation speed.
  </p>
  <div class="compare-layout">
    <div class="compare-sidebar">
      <h3>Select Systems</h3>
      <div class="cmp-group">
        <label>System A (baseline)</label>
        <select id="cmpA">
          <optgroup label="Apple Silicon — Mac Studio">
            <option value="m4_ultra_base">Mac Studio M4 Ultra 24-core CPU / 60-core GPU / 192 GB</option>
            <option value="m4_ultra_high">Mac Studio M4 Ultra 24-core CPU / 76-core GPU / 192 GB</option>
            <option value="m4_ultra_xl">Mac Studio M4 Ultra 24-core CPU / 76-core GPU / 512 GB</option>
            <option value="m3_ultra_base" selected>Mac Studio M3 Ultra 24-core CPU / 60-core GPU / 192 GB</option>
            <option value="m3_ultra_high">Mac Studio M3 Ultra 24-core CPU / 76-core GPU / 192 GB</option>
            <option value="m3_ultra_xl">Mac Studio M3 Ultra 24-core CPU / 76-core GPU / 512 GB</option>
          </optgroup>
          <optgroup label="Apple Silicon — MacBook Pro">
            <option value="m4_max_14_32">MacBook Pro M4 Max 14-core CPU / 32-core GPU / 64 GB</option>
            <option value="m4_max_16_40">MacBook Pro M4 Max 16-core CPU / 40-core GPU / 128 GB</option>
            <option value="m3_max_14_30">MacBook Pro M3 Max 14-core CPU / 30-core GPU / 64 GB</option>
            <option value="m3_max_16_40">MacBook Pro M3 Max 16-core CPU / 40-core GPU / 128 GB</option>
            <option value="m3_pro_12_18">MacBook Pro M3 Pro 12-core CPU / 18-core GPU / 36 GB</option>
          </optgroup>
          <optgroup label="NVIDIA GPUs (discrete)">
            <option value="rtx5090">NVIDIA RTX 5090 (32 GB VRAM)</option>
            <option value="rtx4090">NVIDIA RTX 4090 (24 GB VRAM)</option>
            <option value="rtx3090">NVIDIA RTX 3090 (24 GB VRAM)</option>
            <option value="rtxa6000">NVIDIA RTX A6000 (48 GB VRAM)</option>
            <option value="a100_80">NVIDIA A100 80 GB</option>
          </optgroup>
          <optgroup label="AMD">
            <option value="rx7900xtx">AMD RX 7900 XTX (24 GB VRAM)</option>
            <option value="ryzen_ai_max">AMD Ryzen AI Max+ 395 (128 GB unified)</option>
          </optgroup>
        </select>
      </div>
      <div class="cmp-group">
        <label>System B</label>
        <select id="cmpB">
          <option value="none">— none —</option>
          <optgroup label="Apple Silicon — Mac Studio">
            <option value="m4_ultra_base">Mac Studio M4 Ultra 24-core CPU / 60-core GPU / 192 GB</option>
            <option value="m4_ultra_high">Mac Studio M4 Ultra 24-core CPU / 76-core GPU / 192 GB</option>
            <option value="m4_ultra_xl">Mac Studio M4 Ultra 24-core CPU / 76-core GPU / 512 GB</option>
            <option value="m3_ultra_base">Mac Studio M3 Ultra 24-core CPU / 60-core GPU / 192 GB</option>
            <option value="m3_ultra_high">Mac Studio M3 Ultra 24-core CPU / 76-core GPU / 192 GB</option>
            <option value="m3_ultra_xl">Mac Studio M3 Ultra 24-core CPU / 76-core GPU / 512 GB</option>
          </optgroup>
          <optgroup label="Apple Silicon — MacBook Pro">
            <option value="m4_max_14_32" selected>MacBook Pro M4 Max 16-core CPU / 40-core GPU / 128 GB</option>
            <option value="m4_max_16_40">MacBook Pro M4 Max 16-core CPU / 40-core GPU / 128 GB</option>
            <option value="m3_max_14_30">MacBook Pro M3 Max 14-core CPU / 30-core GPU / 64 GB</option>
            <option value="m3_max_16_40">MacBook Pro M3 Max 16-core CPU / 40-core GPU / 128 GB</option>
            <option value="m3_pro_12_18">MacBook Pro M3 Pro 12-core CPU / 18-core GPU / 36 GB</option>
          </optgroup>
          <optgroup label="NVIDIA GPUs (discrete)">
            <option value="rtx5090">NVIDIA RTX 5090 (32 GB VRAM)</option>
            <option value="rtx4090">NVIDIA RTX 4090 (24 GB VRAM)</option>
            <option value="rtx3090">NVIDIA RTX 3090 (24 GB VRAM)</option>
            <option value="rtxa6000">NVIDIA RTX A6000 (48 GB VRAM)</option>
            <option value="a100_80">NVIDIA A100 80 GB</option>
          </optgroup>
          <optgroup label="AMD">
            <option value="rx7900xtx">AMD RX 7900 XTX (24 GB VRAM)</option>
            <option value="ryzen_ai_max">AMD Ryzen AI Max+ 395 (128 GB unified)</option>
          </optgroup>
        </select>
      </div>
      <div class="cmp-group">
        <label>System C (optional)</label>
        <select id="cmpC">
          <option value="none" selected>— none —</option>
          <optgroup label="Apple Silicon — Mac Studio">
            <option value="m4_ultra_base">Mac Studio M4 Ultra 24-core CPU / 60-core GPU / 192 GB</option>
            <option value="m4_ultra_high">Mac Studio M4 Ultra 24-core CPU / 76-core GPU / 192 GB</option>
            <option value="m4_ultra_xl">Mac Studio M4 Ultra 24-core CPU / 76-core GPU / 512 GB</option>
            <option value="m3_ultra_base">Mac Studio M3 Ultra 24-core CPU / 60-core GPU / 192 GB</option>
            <option value="m3_ultra_high">Mac Studio M3 Ultra 24-core CPU / 76-core GPU / 192 GB</option>
            <option value="m3_ultra_xl">Mac Studio M3 Ultra 24-core CPU / 76-core GPU / 512 GB</option>
          </optgroup>
          <optgroup label="Apple Silicon — MacBook Pro">
            <option value="m4_max_14_32">MacBook Pro M4 Max 16-core CPU / 40-core GPU / 128 GB</option>
            <option value="m4_max_16_40">MacBook Pro M4 Max 16-core CPU / 40-core GPU / 128 GB</option>
            <option value="m3_max_14_30">MacBook Pro M3 Max 14-core CPU / 30-core GPU / 64 GB</option>
            <option value="m3_max_16_40">MacBook Pro M3 Max 16-core CPU / 40-core GPU / 128 GB</option>
            <option value="m3_pro_12_18">MacBook Pro M3 Pro 12-core CPU / 18-core GPU / 36 GB</option>
          </optgroup>
          <optgroup label="NVIDIA GPUs (discrete)">
            <option value="rtx5090">NVIDIA RTX 5090 (32 GB VRAM)</option>
            <option value="rtx4090">NVIDIA RTX 4090 (24 GB VRAM)</option>
            <option value="rtx3090">NVIDIA RTX 3090 (24 GB VRAM)</option>
            <option value="rtxa6000">NVIDIA RTX A6000 (48 GB VRAM)</option>
            <option value="a100_80">NVIDIA A100 80 GB</option>
          </optgroup>
          <optgroup label="AMD">
            <option value="rx7900xtx">AMD RX 7900 XTX (24 GB VRAM)</option>
            <option value="ryzen_ai_max">AMD Ryzen AI Max+ 395 (128 GB unified)</option>
          </optgroup>
        </select>
      </div>
      <button class="cmp-run-btn" onclick="runCompare()">Compare &rarr;</button>
      <p style="font-size:0.78em;color:#888;margin-top:0.8em;line-height:1.5">
        Token speed figures are real-world medians from llama.cpp benchmarks. RAM impact on model size follows Q4 quantisation rules (~1.1 GB per 1B params).
      </p>
    </div>

    <div class="compare-main" id="compareMain">
      <p class="cmp-placeholder">Select systems on the left and click Compare.</p>
    </div>
  </div>
</div>

<!-- ═══════════════════════════════════════════════════════ OPTIMIZATION GUIDE TAB -->
<div id="optimization" class="tab-content">
  <h3 style="margin-top:0">Optimization Guide — Getting Maximum Performance from Local LLMs</h3>
  <p class="opt-intro">
    Inference speed for locally self-hosted LLMs is almost entirely <strong>memory-bandwidth-bound</strong>, not compute-bound.
    Each token generated requires reading the full model weights from memory once. The faster your memory bus, the faster your tokens.
    GPU VRAM bandwidth (&gt;1,000 GB/s) dwarfs CPU DRAM bandwidth (30–100 GB/s), which is why a GPU can be 10–30&times; faster than CPU-only.
    Knowing which bottleneck you have lets you choose the right optimization.
  </p>

  <div class="opt-section">
    <h3>What Actually Limits Performance — by Hardware Path</h3>
    <table class="opt-table">
      <thead><tr>
        <th>Hardware Path</th><th>Primary Bottleneck</th><th>Secondary Bottleneck</th>
        <th>Max RAM Available</th><th>Typical 7B t/s</th><th>Typical 70B t/s</th>
      </tr></thead>
      <tbody>
        <tr>
          <td><strong>NVIDIA GPU (e.g. RTX 5090)</strong></td>
          <td>VRAM capacity (32 GB ceiling on consumer cards)</td>
          <td>VRAM bandwidth when model fits</td>
          <td>32 GB VRAM + system RAM for overflow</td>
          <td>~250</td><td>N/A (model too large for VRAM)</td>
        </tr>
        <tr>
          <td><strong>NVIDIA Pro GPU (e.g. A100 80GB)</strong></td>
          <td>VRAM bandwidth (2,000 GB/s)</td>
          <td>PCIe bandwidth for multi-GPU</td>
          <td>80 GB per card</td>
          <td>~260</td><td>~22 (full VRAM)</td>
        </tr>
        <tr>
          <td><strong>Apple Silicon (e.g. M3 Ultra)</strong></td>
          <td>Unified memory bandwidth (~800 GB/s)</td>
          <td>Metal compute shader efficiency</td>
          <td>192–512 GB (model &amp; OS share same pool)</td>
          <td>~120</td><td>~18–20</td>
        </tr>
        <tr>
          <td><strong>AMD RX 7900 XTX</strong></td>
          <td>VRAM capacity (24 GB ceiling)</td>
          <td>ROCm/HIP software overhead</td>
          <td>24 GB VRAM</td>
          <td>~135</td><td>N/A</td>
        </tr>
        <tr>
          <td><strong>AMD Ryzen AI Max+ 395</strong></td>
          <td>Unified memory bandwidth (~256 GB/s)</td>
          <td>ROCm vs. Vulkan backend selection</td>
          <td>128 GB unified</td>
          <td>~90</td><td>~6</td>
        </tr>
        <tr>
          <td><strong>CPU-only (e.g. i9-13900K, DDR5)</strong></td>
          <td>System RAM bandwidth (40–80 GB/s)</td>
          <td>Thread count, NUMA topology</td>
          <td>Addressable system RAM</td>
          <td>~15–25</td><td>~2–4</td>
        </tr>
      </tbody>
    </table>
    <p style="font-size:0.82em;color:#666;margin-top:0.5em">Token speeds are batch-size-1 (single-user) generation using Q4_K_M quantisation. Multi-user batch inference favours higher-compute GPUs more.</p>
  </div>

  <div class="opt-section">
    <h3>Quantisation — The Most Impactful Software Setting</h3>
    <p style="font-size:0.9em;margin-top:0;color:#555">Quantisation reduces model precision to fit more into memory and speed up loads. Choosing the right level is the single biggest lever available regardless of hardware.</p>
    <table class="opt-table">
      <thead><tr>
        <th>Format</th><th>Bits/Weight</th><th>RAM vs FP16</th><th>Quality vs FP16</th><th>Best Use</th><th>Speedup vs FP16</th>
      </tr></thead>
      <tbody>
        <tr>
          <td><strong>FP32</strong></td><td>32</td><td>2&times; more</td><td>Identical</td><td>Training only</td>
          <td><span class="speedup-tag speedup-low">0.5&times;</span></td>
        </tr>
        <tr>
          <td><strong>FP16 / BF16</strong></td><td>16</td><td>1&times; (baseline)</td><td>Near-lossless</td><td>High-VRAM GPU inference, fine-tuning</td>
          <td><span class="speedup-tag speedup-neutral">1&times;</span></td>
        </tr>
        <tr>
          <td><strong>Q8_0</strong></td><td>8</td><td>~0.5&times;</td><td>Effectively lossless (&lt;0.1% perplexity)</td><td>When you have VRAM to spare</td>
          <td><span class="speedup-tag speedup-medium">1.8&times;</span></td>
        </tr>
        <tr>
          <td><strong>Q6_K</strong></td><td>6</td><td>~0.37&times;</td><td>Minimal quality loss</td><td>Large models on capacity-limited systems</td>
          <td><span class="speedup-tag speedup-medium">2.2&times;</span></td>
        </tr>
        <tr>
          <td><strong>Q5_K_M</strong></td><td>5</td><td>~0.31&times;</td><td>Slight loss, noticeable only on coding</td><td>Best quality when RAM is just enough</td>
          <td><span class="speedup-tag speedup-high">2.8&times;</span></td>
        </tr>
        <tr style="background:#fffdf0;">
          <td><strong>Q4_K_M</strong></td><td>4</td><td>~0.26&times;</td><td>Good — recommended default</td><td>
            <strong>Default recommendation for all hardware</strong></td>
          <td><span class="speedup-tag speedup-high">3.2&times;</span></td>
        </tr>
        <tr>
          <td><strong>Q3_K_M</strong></td><td>3</td><td>~0.19&times;</td><td>Noticeable degradation</td><td>Only if Q4 doesn't fit in RAM</td>
          <td><span class="speedup-tag speedup-high">3.8&times;</span></td>
        </tr>
        <tr>
          <td><strong>Q2_K</strong></td><td>2</td><td>~0.13&times;</td><td>Significant quality loss — avoid</td><td>Emergency fallback</td>
          <td><span class="speedup-tag speedup-high">4.5&times;</span></td>
        </tr>
        <tr>
          <td><strong>IQ4_XS (imatrix)</strong></td><td>~4.25</td><td>~0.27&times;</td><td>Better than Q4_K_M at same size</td><td>Best Q4-tier quality</td>
          <td><span class="speedup-tag speedup-high">3.0&times;</span></td>
        </tr>
      </tbody>
    </table>
    <p style="font-size:0.82em;color:#666;margin-top:0.5em">
      <strong>Rule of thumb:</strong> Q4_K_M uses approximately 1.1 GB per 1B parameters.
      A 70B model &#8776; 40 GB Q4; a 13B model &#8776; 8 GB Q4; a 7B model &#8776; 4.4 GB Q4.
      Importance-matrix (imatrix) quants (IQ series) give better quality at the same bit depth.
    </p>
  </div>

  <div class="opt-section">
    <h3>Platform-Specific Optimization Paths</h3>
    <div class="opt-cards">

      <div class="opt-card">
        <div class="opt-card-header opt-nvidia">
          <svg width="16" height="16" viewBox="0 0 16 16" fill="none"><rect width="16" height="16" rx="3" fill="#76b900"/><text x="3" y="12" font-size="10" fill="white" font-family="sans-serif" font-weight="bold">N</text></svg>
          NVIDIA — CUDA Ecosystem
        </div>
        <div class="opt-card-body">
          <ul>
            <li><strong>CUDA + cuBLAS (default):</strong> All major frameworks use this. Best throughput for models that fully fit in VRAM. Zero additional configuration in Ollama or LM Studio.</li>
            <li><strong>TensorRT-LLM:</strong> NVIDIA's production inference engine. Requires model compilation; delivers up to 2&times; the throughput of vanilla llama.cpp for continuous batching workloads.</li>
            <li><strong>FlashAttention-2:</strong> Reduces attention memory complexity from O(n&sup2;) to O(n). Enabled by default in vLLM and llama.cpp — provides 1.5–2&times; speedup on long contexts.</li>
            <li><strong>Multi-GPU NVLink:</strong> Pro cards (A100, H100, RTX 6000 Ada) support NVLink for full bandwidth pooling. Consumer cards do not — PCIE bandwidth becomes the bottleneck with 2&times; GPUs.</li>
            <li><strong>GPU offload layers (<code>-ngl</code>):</strong> In llama.cpp, set <code>-ngl 99</code> to send all transformer layers to GPU. If VRAM is too small, reduce layers incrementally; each layer in VRAM improves speed.</li>
            <li><strong>FP8 quantisation (Hopper GPUs):</strong> H100/H200 support FP8 native precision — 2&times; throughput over BF16 with &lt;1% quality loss. Not available on consumer RTX cards.</li>
          </ul>
        </div>
      </div>

      <div class="opt-card">
        <div class="opt-card-header opt-apple">
          <svg width="16" height="16" viewBox="0 0 16 16" fill="none"><rect width="16" height="16" rx="3" fill="#555"/><text x="3" y="12" font-size="10" fill="white" font-family="sans-serif" font-weight="bold">A</text></svg>
          Apple Silicon — Metal / MLX
        </div>
        <div class="opt-card-body">
          <ul>
            <li><strong>MLX framework (Apple-native):</strong> 20–30% faster than llama.cpp with Ollama for inference on Apple Silicon. Install via <code>pip install mlx-lm</code>. Best choice for Apple hardware in 2025.</li>
            <li><strong>Metal Performance Shaders (MPS):</strong> Used by PyTorch on Apple Silicon. Not as optimised as MLX for LLM inference but works for fine-tuning and training experiments.</li>
            <li><strong>Speculative decoding (MLX):</strong> Use a small draft model (e.g. 1B) to propose tokens, verified by the main model. Can nearly double tokens/sec on large models with no quality change.</li>
            <li><strong>llama.cpp Metal backend:</strong> Used by Ollama. Slower than native MLX but has broader model format support. Set <code>LLAMA_METAL=1</code> at compile time.</li>
            <li><strong>Unified memory — use it all:</strong> Unlike a GPU, macOS can dynamically share RAM between GPU and CPU. Run models that are 70–80% of total RAM; leave ~20% for macOS and applications.</li>
            <li><strong>Context length and memory:</strong> Increasing context window (e.g. 8K → 128K) consumes significant KV-cache RAM. At 128K context a 70B model can use &gt;60 GB extra RAM. Set <code>--ctx-size</code> conservatively.</li>
          </ul>
        </div>
      </div>

      <div class="opt-card">
        <div class="opt-card-header opt-amd">
          <svg width="16" height="16" viewBox="0 0 16 16" fill="none"><rect width="16" height="16" rx="3" fill="#ed1c24"/><text x="4" y="12" font-size="9" fill="white" font-family="sans-serif" font-weight="bold">AMD</text></svg>
          AMD — ROCm / Vulkan / HIP
        </div>
        <div class="opt-card-body">
          <ul>
            <li><strong>ROCm (HIP):</strong> AMD's CUDA-equivalent. Now supported in llama.cpp, vLLM, and PyTorch. Requires ROCm 6.0+ on compatible RX 6000/7000 series GPUs. Performance is within ~10% of CUDA on inference.</li>
            <li><strong>Vulkan backend (llama.cpp):</strong> Cross-platform GPU acceleration using Vulkan compute shaders. Often 10–20% faster than ROCm on discrete GPUs for pure inference. Use <code>-DGGML_VULKAN=ON</code> at build time.</li>
            <li><strong>Ryzen AI Max+ 395 — BIOS RAM allocation:</strong> By default, the NPU/GPU shares 32 GB. Open BIOS, set iGPU VRAM allocation to 96 or 128 GB for full performance. Without this, large models page to slow system RAM.</li>
            <li><strong>hipBLAS (ROCm BLAS):</strong> Replaces cuBLAS for AMD systems. Install via ROCm SDK and set <code>HIP_VISIBLE_DEVICES=0</code>. Used automatically by torch and llama.cpp ROCm builds.</li>
            <li><strong>AMDGPU target:</strong> Set <code>HSA_OVERRIDE_GFX_VERSION=11.0.0</code> for RDNA3 cards (RX 7000 series) if ROCm doesn't detect the GPU correctly — common issue on consumer ARC/RDNA setups.</li>
            <li><strong>Concurrent requests:</strong> AMD's ROCm handles batching less efficiently than CUDA for high concurrency. For single-user inference, the gap is small (&lt;10%). For serving multiple users, NVIDIA holds a clear advantage.</li>
          </ul>
        </div>
      </div>

      <div class="opt-card">
        <div class="opt-card-header opt-general">
          <svg width="16" height="16" viewBox="0 0 16 16" fill="none"><rect width="16" height="16" rx="3" fill="#007bff"/><text x="3" y="12" font-size="10" fill="white" font-family="sans-serif" font-weight="bold">+</text></svg>
          CPU-Only Inference (x86 / ARM)
        </div>
        <div class="opt-card-body">
          <ul>
            <li><strong>AVX-512 (Intel Icelake+, AMD Zen4):</strong> llama.cpp auto-detects and uses AVX-512 VNNI instructions for 4-bit matrix multiply. ~1.4&times; faster than AVX2. Use a compiled binary or set <code>CMAKE_CXX_FLAGS=-march=native</code>.</li>
            <li><strong>Thread count:</strong> Set threads to equal physical cores (not hyperthreads). Hyperthreading adds latency on memory-bound tasks. E.g. for a 16P + 8E core i9-13900K, use <code>-t 16</code> (P-cores only).</li>
            <li><strong>NUMA pinning:</strong> On multi-socket systems, pin to one NUMA domain. Cross-socket memory access halves effective bandwidth. Use <code>numactl --cpunodebind=0 --membind=0</code>.</li>
            <li><strong>Dual-channel / quad-channel RAM:</strong> Each additional memory channel doubles bandwidth. DDR5-6000 quad-channel gives ~190 GB/s vs ~48 GB/s for DDR4 single-channel — a 4&times; inference speedup.</li>
            <li><strong>mmap and mlock:</strong> llama.cpp by default mmaps model files. Use <code>--mlock</code> to pin the model in RAM and prevent page faults during inference. Requires sufficient free RAM and root/CAP_IPC_LOCK.</li>
          </ul>
        </div>
      </div>

      <div class="opt-card">
        <div class="opt-card-header opt-general">
          <svg width="16" height="16" viewBox="0 0 16 16" fill="none"><rect width="16" height="16" rx="3" fill="#007bff"/><text x="4" y="12" font-size="9" fill="white" font-family="sans-serif" font-weight="bold">SW</text></svg>
          Inference Framework Selection
        </div>
        <div class="opt-card-body">
          <ul>
            <li><strong>Ollama:</strong> Easiest setup; wraps llama.cpp with automatic hardware detection (CUDA, Metal, ROCm). Good for development and single-user. Not optimised for high-throughput multi-user serving.</li>
            <li><strong>LM Studio:</strong> GUI-based; good for non-technical users. Uses llama.cpp backend. GPU offload via "GPU Layers" slider. Match layers to VRAM — rule of thumb: each 7B layer ~= 100 MB VRAM.</li>
            <li><strong>llama.cpp (direct):</strong> Maximum control. Supports every acceleration path. Use <code>-ngl 99 -t 8 --ctx-size 4096 --mlock</code> as a good starting configuration.</li>
            <li><strong>vLLM:</strong> Best for production multi-user serving on NVIDIA. PagedAttention enables efficient KV-cache sharing. Not supported on Apple Silicon. Requires CUDA 11.8+.</li>
            <li><strong>ExLlamaV2:</strong> Fastest CUDA inference for quantised models. Uses EXL2 format (superior to GGUF on NVIDIA). Can exceed TensorRT-LLM speed for chat workloads at Q4–Q5.</li>
          </ul>
        </div>
      </div>

      <div class="opt-card">
        <div class="opt-card-header opt-general">
          <svg width="16" height="16" viewBox="0 0 16 16" fill="none"><rect width="16" height="16" rx="3" fill="#007bff"/><text x="4" y="12" font-size="9" fill="white" font-family="sans-serif" font-weight="bold">KV</text></svg>
          Context Length and KV-Cache
        </div>
        <div class="opt-card-body">
          <ul>
            <li><strong>KV-cache size scales quadratically:</strong> A 7B model at 4K context uses ~500 MB KV-cache; at 128K context it uses ~16 GB. Reduce <code>--ctx-size</code> to free memory for the model itself.</li>
            <li><strong>KV quantisation (<code>--cache-type-k q8_0</code>):</strong> Quantising the KV cache from FP16 to Q8 or Q4 halves its memory footprint with minor quality impact. Supported in llama.cpp ≥ b2300.</li>
            <li><strong>Grouped-query attention (GQA) models:</strong> Llama 3, Mistral, and Qwen2+ use GQA — their KV cache is inherently smaller than older models. Prefer GQA models when context length matters.</li>
            <li><strong>Flash Attention 2:</strong> Required for efficient long-context inference. Enabled automatically in vLLM and supported in llama.cpp with <code>--flash-attn</code>.</li>
          </ul>
        </div>
      </div>

    </div>
  </div>

  <div class="opt-section">
    <h3>RAM / VRAM Impact on Model Access — Reference Table</h3>
    <p style="font-size:0.9em;color:#555;margin-top:0">
      This table shows what changes when you increase or reduce available RAM/VRAM. The key insight: 
      <strong>more memory expands which models fit; more bandwidth increases how fast they run.</strong>
      Adding RAM without adding bandwidth does not improve token speed — it only enables larger models.
    </p>
    <table class="opt-table">
      <thead><tr>
        <th>Available RAM / VRAM</th>
        <th>Largest model (Q4_K_M)</th>
        <th>Token speed change vs. prior tier</th>
        <th>70B Q4 accessible?</th>
        <th>Notes</th>
      </tr></thead>
      <tbody>
        <tr><td>4 GB</td><td>~3B Q4</td><td>—</td><td class="impact-negative">No</td><td>OS overhead leaves ~2.5 GB usable. 3B models run well.</td></tr>
        <tr><td>8 GB</td><td>~7B Q4</td><td class="impact-positive">+0% speed (same BW), +model tier</td><td class="impact-negative">No</td><td>Most popular tier. 7B Q4 = ~4.4 GB. Small context headroom.</td></tr>
        <tr><td>16 GB</td><td>~13B Q4</td><td class="impact-positive">+0% speed, +model tier</td><td class="impact-negative">No</td><td>13B Q4 ≈ 7.9 GB. 8K context adds ~1 GB. Comfortable headroom.</td></tr>
        <tr><td>24 GB</td><td>~20B Q4</td><td class="impact-positive">+0% speed, +model tier</td><td class="impact-negative">No</td><td>Maximum for consumer discrete GPUs (NVIDIA RTX 4090, AMD RX 7900 XTX). 20B Q4 ≈ 12 GB.</td></tr>
        <tr><td>32 GB</td><td>~26B Q4, 70B Q2</td><td class="impact-positive">+0% speed, marginal 70B Q2</td><td style="color:#856404;font-weight:600">Marginal (Q2 only)</td><td>RTX 5090 ceiling. Q2 70B runs but quality suffers. Strong for 13–26B models.</td></tr>
        <tr><td>48 GB</td><td>~40B Q4, 70B Q3</td><td class="impact-positive">+0% speed, +model tier</td><td style="color:#856404;font-weight:600">Q3 only (~3.8 t/s)</td><td>RTX A6000. 70B Q3 just fits; expect lower quality than Q4.</td></tr>
        <tr style="background:#fffdf0;"><td><strong>64 GB+</strong></td><td><strong>70B Q4 (full)</strong></td><td class="impact-positive"><strong>+0% speed, full 70B access</strong></td><td class="impact-positive"><strong>Yes — Q4_K_M</strong></td><td>First tier enabling 70B Q4 fully. Apple M-series, Ryzen AI Max. 70B Q4 ≈ 40 GB; leaves 20+ GB for context.</td></tr>
        <tr><td>96 GB</td><td>~80B Q4, 120B Q3</td><td class="impact-positive">+0% speed, 120B tier</td><td class="impact-positive">Yes — with headroom</td><td>Dual 3090/A6000, M2 Ultra 96 GB. Llama 3.1 405B in Q2 just fits.</td></tr>
        <tr><td>128 GB</td><td>~105B Q4, 120B Q3</td><td class="impact-positive">+0% speed (CPU BW limited)</td><td class="impact-positive">Yes — comfortably</td><td>Ryzen AI Max+ 395, M-series 128 GB. 70B + 32K context fits easily.</td></tr>
        <tr><td>192 GB</td><td>~160B Q4, 200B+ Q3</td><td class="impact-positive">+0% speed, 200B tier (Apple BW)</td><td class="impact-positive">Yes — fast (~18 t/s)</td><td>M3 Ultra 192 GB. Llama 3.1 405B Q2 ≈ 202 GB — just barely fits.</td></tr>
        <tr><td>512 GB</td><td>405B Q4 full</td><td class="impact-positive">+0% speed, 400B tier</td><td class="impact-positive">Yes</td><td>M3 Ultra 512 GB. 405B Q4 ≈ 230 GB. Full unquantised 70B fits.</td></tr>
      </tbody>
    </table>
  </div>

  <div class="opt-section">
    <h3>Key Takeaways by Use Case</h3>
    <table class="opt-table">
      <thead><tr><th>Goal</th><th>Recommended Path</th><th>Key Setting</th></tr></thead>
      <tbody>
        <tr><td>Fastest possible 7B inference</td><td>NVIDIA RTX 5090 + ExLlamaV2</td><td>EXL2 Q4 format, enable Flash Attention</td></tr>
        <tr><td>Run 70B on a budget</td><td>AMD Ryzen AI Max+ 395 mini PC (~$1,500)</td><td>Set BIOS iGPU VRAM to 128 GB, use Vulkan backend</td></tr>
        <tr><td>Quiet, large-model desktop</td><td>Apple Mac Studio M3 Ultra 192 GB + MLX</td><td>Use <code>mlx_lm.generate</code>, enable speculative decoding</td></tr>
        <tr><td>Multi-user production serving</td><td>NVIDIA A100/H100 + vLLM</td><td>Enable PagedAttention, continuous batching, tensor parallelism</td></tr>
        <tr><td>Maximum context length (&gt;64K tokens)</td><td>Apple M3 Ultra 192 GB or 512 GB</td><td>Set <code>--ctx-size 65536</code>, quantise KV cache to Q8</td></tr>
        <tr><td>Fine-tuning / LoRA training</td><td>NVIDIA GPU with 24+ GB VRAM</td><td>Use QLoRA (4-bit base + FP16 adapters), gradient checkpointing</td></tr>
        <tr><td>Best CPU-only on x86</td><td>Dual-channel DDR5 + Zen4/Raptor Lake</td><td>Compile llama.cpp with <code>-march=native</code>, use <code>-t &lt;physical cores&gt;</code></td></tr>
      </tbody>
    </table>
  </div>
</div>

<script>
  function openTab(evt, tabName) {
    var i, tabcontent, tablinks;
    tabcontent = document.getElementsByClassName("tab-content");
    for (i = 0; i < tabcontent.length; i++) {
      tabcontent[i].style.display = "none";
    }
    tablinks = document.getElementsByClassName("tab-button");
    for (i = 0; i < tablinks.length; i++) {
      tablinks[i].className = tablinks[i].className.replace(" active", "");
    }
    document.getElementById(tabName).style.display = "block";
    evt.currentTarget.className += " active";
  }

  function sortTable(n) {
    var table, rows, switching, i, x, y, shouldSwitch, dir, switchcount = 0;
    table = document.getElementById("hardwareTable");
    switching = true;
    var currentDir = table.getAttribute("data-sort-dir-" + n) || "asc";
    dir = (currentDir === "asc") ? "desc" : "asc";
    table.setAttribute("data-sort-dir-" + n, dir);
    
    // Clear existing sort indicators
    var headers = table.getElementsByTagName("TH");
    for (i = 0; i < headers.length; i++) {
        headers[i].innerHTML = headers[i].innerHTML.replace(/ (↑|↓)$/, "");
    }

    while (switching) {
      switching = false;
      rows = table.rows;
      for (i = 1; i < (rows.length - 1); i++) {
        shouldSwitch = false;
        x = rows[i].getElementsByTagName("TD")[n];
        y = rows[i + 1].getElementsByTagName("TD")[n];
        
        var xContent = x.innerText;
        var yContent = y.innerText;

        // For numeric columns, parse the numbers
        if (n === 0 || n === 4 || n === 6 || n === 7) {
            xContent = parseFloat(xContent.replace(/[^0-9.\-]+/g,""));
            yContent = parseFloat(yContent.replace(/[^0-9.\-]+/g,""));
        } else {
            xContent = xContent.toLowerCase();
            yContent = yContent.toLowerCase();
        }

        if (dir == "asc") {
          if (xContent > yContent) {
            shouldSwitch = true;
            break;
          }
        } else if (dir == "desc") {
          if (xContent < yContent) {
            shouldSwitch = true;
            break;
          }
        }
      }
      if (shouldSwitch) {
        rows[i].parentNode.insertBefore(rows[i + 1], rows[i]);
        switching = true;
        switchcount++;
      }
    }
    // Add sort indicator to header
    var sortedHeader = headers[n];
    if (dir === 'asc') {
        sortedHeader.innerHTML += ' &uarr;';
    } else {
        sortedHeader.innerHTML += ' &darr;';
    }
  }

  function filterTable() {
    var input, filter, table, tr, td, i, txtValue;
    var searchInput = document.getElementById("searchInput").value.toUpperCase();
    var modelFilter = document.getElementById("modelFilter").value;
    var typeFilter = document.getElementById("typeFilter").value.toUpperCase();
    table = document.getElementById("hardwareTable");
    tr = table.getElementsByTagName("tr");

    for (i = 1; i < tr.length; i++) { // Start from 1 to skip header
        // Ignore rows that are part of the notes
        if (tr[i].classList.contains('notes-row')) {
            continue;
        }
        tr[i].style.display = ""; // Reset display
        
        let isVisible = true;

        // Search filter
        let rowText = tr[i].textContent || tr[i].innerText;
        if (rowText.toUpperCase().indexOf(searchInput) === -1) {
            isVisible = false;
        }

        // Model filter
        if (isVisible && modelFilter) {
            let modelCell = tr[i].getElementsByTagName("td")[5];
            if (modelCell) {
                let modelText = modelCell.innerText;
                if (!modelText.includes(modelFilter)) {
                    isVisible = false;
                }
            }
        }

        // Type filter
        if (isVisible && typeFilter) {
            let typeCell = tr[i].getElementsByTagName("td")[2];
            if (typeCell) {
                if (typeCell.innerText.toUpperCase().indexOf(typeFilter) === -1) {
                    isVisible = false;
                }
            }
        }
        
        tr[i].style.display = isVisible ? "" : "none";
    }
  }

  /* ══════════════════════════════════════════════════════════════
     GPU RANKINGS DATA + LOGIC
  ══════════════════════════════════════════════════════════════ */
  const GPU_DATA = [
    // name, mfr, vram, bw (GB/s), tg7b (t/s), tg70b (t/s or null), price ($USD), tier, bestFor
    { name:"RTX 5090",           mfr:"NVIDIA", vram:32,  bw:1792, tg7b:250, tg70b:null,  price:1999,  tier:"consumer",     bestFor:"Fastest consumer GPU for ≤30B models" },
    { name:"RTX 4090",           mfr:"NVIDIA", vram:24,  bw:1008, tg7b:140, tg70b:null,  price:1200,  tier:"consumer",     bestFor:"Best value NVIDIA for 7B–24B models" },
    { name:"RTX 3090",           mfr:"NVIDIA", vram:24,  bw:936,  tg7b:130, tg70b:null,  price:600,   tier:"consumer",     bestFor:"Budget NVIDIA; great 24GB VRAM value" },
    { name:"RTX 5090 (Laptop)",  mfr:"NVIDIA", vram:24,  bw:960,  tg7b:140, tg70b:null,  price:3500,  tier:"laptop",       bestFor:"Portable CUDA workload; watch thermals" },
    { name:"RTX 4090 (Laptop)",  mfr:"NVIDIA", vram:16,  bw:576,  tg7b:80,  tg70b:null,  price:2500,  tier:"laptop",       bestFor:"Mobile CUDA inference up to 13B" },
    { name:"A100 80GB (PCIe)",   mfr:"NVIDIA", vram:80,  bw:1935, tg7b:220, tg70b:18,    price:5000,  tier:"professional", bestFor:"70B unquantized; production inference" },
    { name:"RTX 6000 Ada (48GB)",mfr:"NVIDIA", vram:48,  bw:960,  tg7b:200, tg70b:8,     price:6000,  tier:"professional", bestFor:"High-VRAM pro GPU; NVLink + ECC" },
    { name:"RTX A6000 (48GB)",   mfr:"NVIDIA", vram:48,  bw:768,  tg7b:180, tg70b:7,     price:3000,  tier:"professional", bestFor:"72B Q4 capable; strong multi-GPU value" },
    { name:"H100 80GB (PCIe)",   mfr:"NVIDIA", vram:80,  bw:2000, tg7b:260, tg70b:22,    price:25000, tier:"datacenter",   bestFor:"High-throughput production serving" },
    { name:"RX 7900 XTX",        mfr:"AMD",    vram:24,  bw:960,  tg7b:135, tg70b:null,  price:800,   tier:"consumer",     bestFor:"Best AMD discrete; rival to RTX 4090" },
    { name:"RX 7900 XT",         mfr:"AMD",    vram:20,  bw:800,  tg7b:115, tg70b:null,  price:650,   tier:"consumer",     bestFor:"Budget AMD; solid 20B performer" },
    { name:"Ryzen AI Max+ 395",  mfr:"AMD",    vram:128, bw:256,  tg7b:90,  tg70b:6,     price:1800,  tier:"laptop/mini",  bestFor:"Cheapest 70B-capable x86 system" },
    { name:"M3 Ultra (192GB)",   mfr:"Apple",  vram:192, bw:800,  tg7b:120, tg70b:20,    price:8000,  tier:"desktop",      bestFor:"Up to 120B+ models; silent & efficient" },
    { name:"M3 Ultra (512GB)",   mfr:"Apple",  vram:512, bw:800,  tg7b:120, tg70b:20,    price:10000, tier:"desktop",      bestFor:"200B+ models; unmatched capacity" },
    { name:"M3 Max (128GB)",     mfr:"Apple",  vram:128, bw:300,  tg7b:60,  tg70b:5,     price:4000,  tier:"laptop",       bestFor:"Portable 70B inference; macOS dev" },
    { name:"M4 Max (128GB)",     mfr:"Apple",  vram:128, bw:410,  tg7b:75,  tg70b:6,     price:4200,  tier:"laptop",       bestFor:"Improved MacBook for 70B inference" },
    { name:"M3 Pro (36GB)",      mfr:"Apple",  vram:36,  bw:150,  tg7b:35,  tg70b:null,  price:2200,  tier:"laptop",       bestFor:"Entry Apple; up to 30B Q4" },
  ];

  // Compute derived scores
  GPU_DATA.forEach(g => {
    // Value score: tokens/sec per $100
    g.valueScore = g.price > 0 ? parseFloat((g.tg7b / g.price * 100).toFixed(2)) : 0;
    // Effectiveness: weighted blend of normalized bw (40%), tg7b (40%), vram (20%)
    // Will normalize after computing raw values
  });
  const maxBw   = Math.max(...GPU_DATA.map(g => g.bw));
  const maxTg7b = Math.max(...GPU_DATA.map(g => g.tg7b));
  const maxVram = Math.max(...GPU_DATA.map(g => g.vram));
  GPU_DATA.forEach(g => {
    g.score = parseFloat(((g.bw/maxBw)*40 + (g.tg7b/maxTg7b)*40 + (g.vram/maxVram)*20).toFixed(1));
  });
  // Assign rank by default score desc
  const sorted = [...GPU_DATA].sort((a,b) => b.score - a.score);
  sorted.forEach((g, i) => { g.rank = i + 1; });

  let gpuActiveMfr = 'all';
  let gpuSortField = 'score';
  let gpuSortDir   = 'desc';
  let gpuInitDone  = false;

  function initGpuTab() {
    if (!gpuInitDone) { gpuInitDone = true; renderGpuTable(); }
  }

  function setMfrFilter(mfr) {
    gpuActiveMfr = mfr;
    ['all','nvidia','amd','apple'].forEach(id => {
      const el = document.getElementById('mfr-' + id);
      el.className = 'mfr-btn';
    });
    const activeId = 'mfr-' + (mfr === 'NVIDIA' ? 'nvidia' : mfr === 'AMD' ? 'amd' : mfr === 'Apple' ? 'apple' : 'all');
    const activeClass = mfr === 'NVIDIA' ? 'active-nvidia' : mfr === 'AMD' ? 'active-amd' : mfr === 'Apple' ? 'active-apple' : 'active-all';
    document.getElementById(activeId).className = 'mfr-btn ' + activeClass;
    renderGpuTable();
  }

  function gpuSortCol(field) {
    if (gpuSortField === field) {
      gpuSortDir = gpuSortDir === 'desc' ? 'asc' : 'desc';
    } else {
      gpuSortField = field;
      gpuSortDir = (field === 'price' || field === 'name' || field === 'mfr' || field === 'bestFor') ? 'asc' : 'desc';
    }
    document.getElementById('gpuSortSelect').value = ['score','tg7b','bw','vram','price','valueScore'].includes(field) ? field : 'score';
    renderGpuTable();
  }

  function renderGpuTable() {
    const sortBy   = document.getElementById('gpuSortSelect').value || gpuSortField;
    gpuSortField   = sortBy;
    const search   = (document.getElementById('gpuSearch').value || '').toLowerCase();

    let data = GPU_DATA.filter(g => {
      if (gpuActiveMfr !== 'all' && g.mfr !== gpuActiveMfr) return false;
      if (search && !g.name.toLowerCase().includes(search) && !g.mfr.toLowerCase().includes(search) && !g.bestFor.toLowerCase().includes(search)) return false;
      return true;
    });

    const numericDesc = ['score','tg7b','bw','vram','valueScore'];
    const numericAsc  = ['price'];
    data.sort((a, b) => {
      let av = a[sortBy], bv = b[sortBy];
      if (typeof av === 'string') av = av.toLowerCase();
      if (typeof bv === 'string') bv = bv.toLowerCase();
      if (av === null) av = -1;
      if (bv === null) bv = -1;
      if (numericDesc.includes(sortBy)) return bv - av;
      if (numericAsc.includes(sortBy))  return av - bv;
      return av < bv ? -1 : av > bv ? 1 : 0;
    });

    const tbody = document.getElementById('gpuTableBody');
    tbody.innerHTML = '';
    const maxScore      = Math.max(...GPU_DATA.map(g => g.score));
    const maxValueScore = Math.max(...GPU_DATA.map(g => g.valueScore));

    data.forEach((g, idx) => {
      const badgeClass = g.mfr === 'NVIDIA' ? 'badge-nvidia' : g.mfr === 'AMD' ? 'badge-amd' : 'badge-apple';
      const barWidth   = Math.round((g.score / 100) * 100);
      const valBarW    = Math.round((g.valueScore / maxValueScore) * 100);
      const tg70bStr   = g.tg70b !== null ? g.tg70b : '<span style="color:#aaa">—</span>';
      const tr = document.createElement('tr');
      tr.innerHTML = `
        <td>${g.rank}</td>
        <td><strong>${g.name}</strong></td>
        <td><span class="mfr-badge ${badgeClass}">${g.mfr}</span></td>
        <td>${g.vram}</td>
        <td>${g.bw.toLocaleString()}</td>
        <td>${g.tg7b}</td>
        <td>${tg70bStr}</td>
        <td>$${g.price.toLocaleString()}</td>
        <td>
          <div class="score-bar-wrap">
            <div class="score-bar" style="width:${valBarW}%;background:#fd7e14;"></div>
            <span class="score-num">${g.valueScore}</span>
          </div>
        </td>
        <td>
          <div class="score-bar-wrap">
            <div class="score-bar" style="width:${barWidth}%;"></div>
            <span class="score-num">${g.score}/100</span>
          </div>
        </td>
        <td style="font-size:0.88em;">${g.bestFor}</td>
      `;
      tbody.appendChild(tr);
    });

    // Update header sort arrows
    const headers = document.querySelectorAll('#gpuTable thead th');
    const colMap = ['rank','name','mfr','vram','bw','tg7b','tg70b','price','valueScore','score','bestFor'];
    headers.forEach((th, i) => {
      th.innerHTML = th.innerHTML.replace(/ [↑↓]$/, '');
      if (colMap[i] === gpuSortField) th.innerHTML += gpuSortDir === 'desc' ? ' ↓' : ' ↑';
    });
  }

  /* ══════════════════════════════════════════════════════════════
     BEST MATCH FINDER LOGIC
  ══════════════════════════════════════════════════════════════ */

  // Systems dataset for recommendation engine
  const SYSTEMS = [
    { name:"HP Z8 G4 (Quad A100 40GB)",    os:["linux","windows"], ff:"desktop",  minPrice:25000, maxPrice:35000, maxModel:120, tg7b:540,  tg70b:50,  priority:["speed","capacity"] },
    { name:"Dell Precision 7960 Tower",    os:["linux","windows"], ff:"desktop",  minPrice:15000, maxPrice:25000, maxModel:120, tg7b:450,  tg70b:40,  priority:["speed","capacity"] },
    { name:"Apple Mac Studio (M3 Ultra 512GB)", os:["macos"],   ff:"desktop",  minPrice:9500,  maxPrice:11000, maxModel:200, tg7b:120,  tg70b:20,  priority:["capacity","value"] },
    { name:"Apple Mac Studio (M3 Ultra 192GB)", os:["macos"],   ff:"desktop",  minPrice:7500,  maxPrice:9500,  maxModel:120, tg7b:120,  tg70b:18,  priority:["capacity","value"] },
    { name:"Lenovo ThinkStation P620 (2×A6000)", os:["linux","windows"], ff:"desktop", minPrice:12000, maxPrice:18000, maxModel:70, tg7b:300, tg70b:20, priority:["capacity","speed"] },
    { name:"HP OMEN 45L (RTX 5090)",       os:["linux","windows"], ff:"desktop",  minPrice:3500,  maxPrice:5500,  maxModel:30,  tg7b:250,  tg70b:null, priority:["speed","value"] },
    { name:"Alienware Aurora R16 (RTX 4090)", os:["linux","windows"], ff:"desktop", minPrice:3000, maxPrice:5000, maxModel:30, tg7b:140, tg70b:null, priority:["speed","value"] },
    { name:"DIY RTX 5090 Build",           os:["linux","windows"], ff:"desktop",  minPrice:2500,  maxPrice:4000,  maxModel:30,  tg7b:250,  tg70b:null, priority:["speed","value"] },
    { name:"DIY RTX 4090 Build",           os:["linux","windows"], ff:"desktop",  minPrice:2000,  maxPrice:3500,  maxModel:30,  tg7b:140,  tg70b:null, priority:["speed","value"] },
    { name:"DIY RTX 3090 Build (used)",    os:["linux","windows"], ff:"desktop",  minPrice:1200,  maxPrice:2000,  maxModel:14,  tg7b:130,  tg70b:null, priority:["value"] },
    { name:"Apple MacBook Pro 16 (M4 Max 128GB)", os:["macos"], ff:"laptop",   minPrice:3800,  maxPrice:5000,  maxModel:70,  tg7b:75,   tg70b:6,   priority:["capacity","portable"] },
    { name:"Apple MacBook Pro 16 (M3 Max 128GB)", os:["macos"], ff:"laptop",   minPrice:3200,  maxPrice:4200,  maxModel:70,  tg7b:60,   tg70b:5,   priority:["capacity","portable"] },
    { name:"Asus ROG Strix Scar 18 (RTX 5090 Laptop)", os:["linux","windows"], ff:"laptop", minPrice:3500, maxPrice:5000, maxModel:30, tg7b:140, tg70b:null, priority:["speed","portable"] },
    { name:"Asus ROG Flow Z13 / Ryzen AI Max+ 395", os:["linux","windows"], ff:"laptop", minPrice:1400, maxPrice:2800, maxModel:70, tg7b:90, tg70b:6, priority:["capacity","value","portable"] },
    { name:"Minisforum AI MAX+ 395 Mini PC", os:["linux","windows"], ff:"laptop", minPrice:1200, maxPrice:2000, maxModel:70, tg7b:90, tg70b:6, priority:["capacity","value","portable"] },
    { name:"ASUS Zenbook A14 (Snapdragon X Elite)", os:["windows"], ff:"laptop",  minPrice:900,  maxPrice:1600,  maxModel:7,   tg7b:25,   tg70b:null, priority:["portable"] },
  ];

  let selectedOs = 'any';

  function setOs(os) {
    selectedOs = os;
    ['macos','windows','linux','any'].forEach(id => {
      document.getElementById('os-' + id).classList.remove('active');
    });
    document.getElementById('os-' + os).classList.add('active');
  }

  function runFinder() {
    const budget    = parseInt(document.getElementById('budgetSlider').value);
    const modelSize = parseInt(document.getElementById('modelSize-select').value);
    const ff        = document.getElementById('ff-select').value;
    const priority  = document.getElementById('priority-select').value;

    // Filter candidates
    let candidates = SYSTEMS.filter(s => {
      if (selectedOs !== 'any' && !s.os.includes(selectedOs)) return false;
      if (ff !== 'any' && s.ff !== ff) return false;
      if (s.minPrice > budget) return false;
      if (s.maxModel < modelSize) return false;
      return true;
    });

    const resultsEl = document.getElementById('finderResults');

    if (candidates.length === 0) {
      resultsEl.innerHTML = `
        <div class="no-result">
          <strong>No exact match found.</strong><br>
          Your current filters (OS: ${selectedOs}, budget: $${budget.toLocaleString()}, model size: ${modelSize}B) don't match any system in our database.
          Try increasing your budget, relaxing the OS requirement, or choosing a smaller model size.
        </div>`;
      return;
    }

    // Score candidates by priority
    candidates.forEach(s => {
      let pts = 0;
      if (priority === 'speed')    pts = s.tg7b;
      if (priority === 'value')    pts = s.tg7b / s.minPrice * 1000;
      if (priority === 'capacity') pts = s.maxModel + (s.tg70b || 0) * 2;
      if (priority === 'portable') pts = (s.ff === 'laptop' ? 50 : 0) + s.tg7b / s.minPrice * 500;
      s._pts = pts;
    });
    candidates.sort((a, b) => b._pts - a._pts);

    const best = candidates[0];
    const alts = candidates.slice(1, 4);

    const priorityLabel = { speed:"Maximum Speed", value:"Best Value", capacity:"Largest Model Support", portable:"Portability" }[priority];
    const osLabel = selectedOs === 'any' ? 'Any OS' : selectedOs === 'macos' ? 'macOS' : selectedOs === 'windows' ? 'Windows' : 'Linux';
    const modelLabel = modelSize >= 120 ? '120B+' : modelSize + 'B';

    const whyMap = {
      speed:    `delivers the highest token generation speed (~${best.tg7b} t/s on 7B models) within your budget`,
      value:    `gives the best tokens-per-dollar ratio within your $${budget.toLocaleString()} budget`,
      capacity: `supports up to ${best.maxModel}B parameter models${best.tg70b ? ', including 70B at ~' + best.tg70b + ' t/s' : ''}, the largest available in range`,
      portable: `is the best portable option${best.ff === 'laptop' ? ' (laptop form factor)' : ''} with solid inference speed`
    };

    resultsEl.innerHTML = `
      <div class="result-card">
        <div class="result-rank">Best match for ${osLabel} &middot; ${modelLabel} target &middot; ${priorityLabel}</div>
        <div class="result-name">${best.name}</div>
        <div class="result-meta">
          <span class="result-meta-chip">From $${best.minPrice.toLocaleString()}</span>
          <span class="result-meta-chip">~${best.tg7b} t/s (7B Q4)</span>
          <span class="result-meta-chip">Up to ${best.maxModel >= 200 ? '200B+' : best.maxModel + 'B'} models</span>
          <span class="result-meta-chip">${best.ff === 'laptop' ? 'Portable' : 'Desktop'}</span>
          <span class="result-meta-chip">${best.os.join(' / ')}</span>
        </div>
        <div class="result-why">
          <strong>Why this one?</strong> This system ${whyMap[priority]}.
          ${modelSize >= 70 && best.maxModel >= 70 ? ' It can handle your target model size (' + modelLabel + ') fully in memory.' : ''}
          ${priority === 'value' && best.os.includes('windows') ? ' Building a custom PC around this GPU can cut costs further — see the DIY tab.' : ''}
        </div>
      </div>
      ${alts.length > 0 ? `
      <div class="alt-results">
        <h4>Also consider</h4>
        ${alts.map(s => `
          <div class="alt-card">
            <div>
              <div class="alt-name">${s.name}</div>
              <div class="alt-reason">${s.tg7b} t/s · up to ${s.maxModel}B · ${s.ff} · ${s.os.join('/')}</div>
            </div>
            <div class="alt-price">from $${s.minPrice.toLocaleString()}</div>
          </div>`).join('')}
      </div>` : ''}
    `;
  }

  /* ══════════════════════════════════════════════════════════════
     COMPARE SYSTEMS DATA + LOGIC
  ══════════════════════════════════════════════════════════════ */

  // Each entry: id, name, mfr, type (unified/discrete/cpu), 
  // memGB (usable memory for models), bwGBs (memory bandwidth GB/s),
  // cpuCores, gpuCores, tg7b, tg13b, tg70b (null = can't fit),
  // price, notes
  const CMP_DATA = {
    // ── Apple Mac Studio ──────────────────────────────────────
    m3_ultra_base: {
      name: 'Mac Studio M3 Ultra', sub: '24-core CPU / 60-core GPU / 192 GB',
      mfr: 'apple', memGB: 192, bwGBs: 800, cpuCores: 24, gpuCores: 60, unified: true,
      tg7b: 115, tg13b: 68, tg70b: 18, tg120b: 9, price: 8000,
      framework: 'MLX / llama.cpp Metal', vramGB: 192,
      notes: 'Baseline M3 Ultra. 60-core GPU. Single-user 70B runs comfortably.'
    },
    m3_ultra_high: {
      name: 'Mac Studio M3 Ultra', sub: '24-core CPU / 76-core GPU / 192 GB',
      mfr: 'apple', memGB: 192, bwGBs: 800, cpuCores: 24, gpuCores: 76, unified: true,
      tg7b: 120, tg13b: 72, tg70b: 20, tg120b: 10, price: 8500,
      framework: 'MLX / llama.cpp Metal', vramGB: 192,
      notes: '76-core GPU adds ~5% token speed vs 60-core at same bandwidth — GPU core count is secondary to memory bandwidth.'
    },
    m3_ultra_xl: {
      name: 'Mac Studio M3 Ultra', sub: '24-core CPU / 76-core GPU / 512 GB',
      mfr: 'apple', memGB: 512, bwGBs: 800, cpuCores: 24, gpuCores: 76, unified: true,
      tg7b: 120, tg13b: 72, tg70b: 20, tg120b: 11, price: 10000,
      framework: 'MLX / llama.cpp Metal', vramGB: 512,
      notes: '512 GB unified. Enables 405B and MoE 671B (quantised). Token speed identical to 192 GB — bandwidth is unchanged; only capacity increases.'
    },
    m4_ultra_base: {
      name: 'Mac Studio M4 Ultra', sub: '24-core CPU / 60-core GPU / 192 GB',
      mfr: 'apple', memGB: 192, bwGBs: 900, cpuCores: 24, gpuCores: 60, unified: true,
      tg7b: 130, tg13b: 78, tg70b: 22, tg120b: 11, price: 8500,
      framework: 'MLX / llama.cpp Metal', vramGB: 192,
      notes: 'M4 Ultra. ~12% bandwidth increase over M3 Ultra at same memory tier.'
    },
    m4_ultra_high: {
      name: 'Mac Studio M4 Ultra', sub: '24-core CPU / 76-core GPU / 192 GB',
      mfr: 'apple', memGB: 192, bwGBs: 900, cpuCores: 24, gpuCores: 76, unified: true,
      tg7b: 135, tg13b: 80, tg70b: 23, tg120b: 12, price: 9000,
      framework: 'MLX / llama.cpp Metal', vramGB: 192,
      notes: 'M4 Ultra 76-core GPU. Marginal gain over 60-core at same bandwidth.'
    },
    m4_ultra_xl: {
      name: 'Mac Studio M4 Ultra', sub: '24-core CPU / 76-core GPU / 512 GB',
      mfr: 'apple', memGB: 512, bwGBs: 900, cpuCores: 24, gpuCores: 76, unified: true,
      tg7b: 135, tg13b: 80, tg70b: 23, tg120b: 12, price: 11000,
      framework: 'MLX / llama.cpp Metal', vramGB: 512,
      notes: 'Maximum capacity M4 Ultra. 405B unquantised possible. Bandwidth, not capacity, sets speed.'
    },
    // ── Apple MacBook Pro ─────────────────────────────────────
    m4_max_14_32: {
      name: 'MacBook Pro M4 Max', sub: '14-core CPU / 32-core GPU / 64 GB',
      mfr: 'apple', memGB: 64, bwGBs: 410, cpuCores: 14, gpuCores: 32, unified: true,
      tg7b: 65, tg13b: 38, tg70b: null, tg120b: null, price: 3200,
      framework: 'MLX / llama.cpp Metal', vramGB: 64,
      notes: 'Entry M4 Max. 64 GB allows 30B Q4 comfortably; 70B Q4 requires ~40 GB so just fits with limited context.'
    },
    m4_max_16_40: {
      name: 'MacBook Pro M4 Max', sub: '16-core CPU / 40-core GPU / 128 GB',
      mfr: 'apple', memGB: 128, bwGBs: 410, cpuCores: 16, gpuCores: 40, unified: true,
      tg7b: 75, tg13b: 45, tg70b: 7, tg120b: null, price: 4200,
      framework: 'MLX / llama.cpp Metal', vramGB: 128,
      notes: 'Top M4 Max config. 128 GB enables 70B Q4 (~40 GB) with headroom. +25% GPU cores vs 32-core — contributes ~10% extra speed.'
    },
    m3_max_14_30: {
      name: 'MacBook Pro M3 Max', sub: '14-core CPU / 30-core GPU / 64 GB',
      mfr: 'apple', memGB: 64, bwGBs: 300, cpuCores: 14, gpuCores: 30, unified: true,
      tg7b: 55, tg13b: 32, tg70b: null, tg120b: null, price: 3000,
      framework: 'MLX / llama.cpp Metal', vramGB: 64,
      notes: 'Entry M3 Max. 300 GB/s bandwidth; noticeably slower than M4 Max on the same model.'
    },
    m3_max_16_40: {
      name: 'MacBook Pro M3 Max', sub: '16-core CPU / 40-core GPU / 128 GB',
      mfr: 'apple', memGB: 128, bwGBs: 300, cpuCores: 16, gpuCores: 40, unified: true,
      tg7b: 60, tg13b: 36, tg70b: 5, tg120b: null, price: 4000,
      framework: 'MLX / llama.cpp Metal', vramGB: 128,
      notes: 'Top M3 Max. 70B narrowly fits. M4 Max is ~25% faster at same memory tier due to bandwidth.'
    },
    m3_pro_12_18: {
      name: 'MacBook Pro M3 Pro', sub: '12-core CPU / 18-core GPU / 36 GB',
      mfr: 'apple', memGB: 36, bwGBs: 150, cpuCores: 12, gpuCores: 18, unified: true,
      tg7b: 35, tg13b: 18, tg70b: null, tg120b: null, price: 2200,
      framework: 'MLX / llama.cpp Metal', vramGB: 36,
      notes: 'M3 Pro. 36 GB max; 30B Q4 fits. Significantly slower bandwidth (150 GB/s) limits token speed.'
    },
    // ── NVIDIA ────────────────────────────────────────────────
    rtx5090: {
      name: 'NVIDIA RTX 5090', sub: '32 GB GDDR7 — 1,792 GB/s',
      mfr: 'nvidia', memGB: 32, bwGBs: 1792, cpuCores: null, gpuCores: null, unified: false,
      tg7b: 250, tg13b: 170, tg70b: null, tg120b: null, price: 1999,
      framework: 'CUDA / llama.cpp / ExLlamaV2', vramGB: 32,
      notes: 'Fastest consumer GPU. 32 GB caps at ~26B Q4 models. Needs 64+ GB system RAM for larger models via CPU offload (slow).'
    },
    rtx4090: {
      name: 'NVIDIA RTX 4090', sub: '24 GB GDDR6X — 1,008 GB/s',
      mfr: 'nvidia', memGB: 24, bwGBs: 1008, cpuCores: null, gpuCores: null, unified: false,
      tg7b: 140, tg13b: 95, tg70b: null, tg120b: null, price: 1200,
      framework: 'CUDA / llama.cpp / ExLlamaV2', vramGB: 24,
      notes: 'Previous gen consumer flagship. 24 GB supports up to ~20B Q4. Best value NVIDIA for 7–13B workloads.'
    },
    rtx3090: {
      name: 'NVIDIA RTX 3090', sub: '24 GB GDDR6X — 936 GB/s',
      mfr: 'nvidia', memGB: 24, bwGBs: 936, cpuCores: null, gpuCores: null, unified: false,
      tg7b: 130, tg13b: 85, tg70b: null, tg120b: null, price: 600,
      framework: 'CUDA / llama.cpp / ExLlamaV2', vramGB: 24,
      notes: 'Used market bargain. Nearly identical VRAM and bandwidth to RTX 4090. Poor at fine-tuning vs newer cards (3x slower).'
    },
    rtxa6000: {
      name: 'NVIDIA RTX A6000', sub: '48 GB GDDR6 — 768 GB/s',
      mfr: 'nvidia', memGB: 48, bwGBs: 768, cpuCores: null, gpuCores: null, unified: false,
      tg7b: 180, tg13b: 120, tg70b: 3, tg120b: null, price: 3000,
      framework: 'CUDA / llama.cpp / vLLM', vramGB: 48,
      notes: '48 GB VRAM. 70B just fits in Q3 but is slow (~3 t/s) due to lower bandwidth vs RTX 5090. Excellent for concurrent multi-user via vLLM.'
    },
    a100_80: {
      name: 'NVIDIA A100 80 GB', sub: '80 GB HBM2e — 1,935 GB/s',
      mfr: 'nvidia', memGB: 80, bwGBs: 1935, cpuCores: null, gpuCores: null, unified: false,
      tg7b: 220, tg13b: 155, tg70b: 18, tg120b: null, price: 5000,
      framework: 'CUDA / vLLM / TensorRT-LLM', vramGB: 80,
      notes: 'Data-center card. 80 GB HBM2e. Fastest bandwidth-per-dollar for 70B models. Requires PCIe x16 slot and external PSU connector.'
    },
    // ── AMD ───────────────────────────────────────────────────
    rx7900xtx: {
      name: 'AMD RX 7900 XTX', sub: '24 GB GDDR6 — 960 GB/s',
      mfr: 'amd', memGB: 24, bwGBs: 960, cpuCores: null, gpuCores: null, unified: false,
      tg7b: 135, tg13b: 88, tg70b: null, tg120b: null, price: 800,
      framework: 'ROCm / Vulkan / llama.cpp', vramGB: 24,
      notes: 'Best AMD discrete. Near-RTX 4090 performance at lower price. ROCm has occasional compatibility issues; Vulkan backend recommended for pure inference.'
    },
    ryzen_ai_max: {
      name: 'AMD Ryzen AI Max+ 395', sub: 'x86 + RDNA3.5 iGPU — 128 GB unified',
      mfr: 'amd', memGB: 128, bwGBs: 256, cpuCores: 16, gpuCores: 40, unified: true,
      tg7b: 90, tg13b: 52, tg70b: 6, tg120b: null, price: 1800,
      framework: 'Vulkan / ROCm / llama.cpp', vramGB: 128,
      notes: 'Cheapest 70B-capable x86 system. 256 GB/s bandwidth is the bottleneck — 3x slower than M3 Ultra on 70B. BIOS must set VRAM to 128 GB.'
    },
  };

  // Model Q4 sizes in GB, used for fit calculations
  const MODEL_SIZES = [
    { label:'1B Q4',   gb:0.8,  desc:'SmolLM2 1.7B'},
    { label:'3B Q4',   gb:2.0,  desc:'Llama 3.2 3B'},
    { label:'7B Q4',   gb:4.4,  desc:'Llama 3.1 8B, Mistral 7B'},
    { label:'13B Q4',  gb:8.0,  desc:'Qwen2.5 14B'},
    { label:'20B Q4',  gb:12.0, desc:'Phi-4 20B, DeepSeek-R1 20B'},
    { label:'30B Q4',  gb:17.0, desc:'Qwen 32B, Gemma 27B'},
    { label:'70B Q4',  gb:40.0, desc:'Llama 3.1 70B, Qwen 72B'},
    { label:'70B Q8',  gb:75.0, desc:'Llama 3.1 70B full quality'},
    { label:'120B Q4', gb:70.0, desc:'Llama 3.1 405B Q2 / MoE models'},
    { label:'405B Q4', gb:230.0,desc:'Llama 3.1 405B'},
  ];

  function getBarClass(mfr) {
    return mfr === 'nvidia' ? 'bar-nvidia' : mfr === 'amd' ? 'bar-amd' : 'bar-apple';
  }
  function getSysClass(mfr) {
    return mfr === 'nvidia' ? 'sys-nvidia' : mfr === 'amd' ? 'sys-amd' : 'sys-apple';
  }

  function pctDiff(base, val) {
    if (!base || !val) return null;
    return Math.round((val - base) / base * 100);
  }
  function deltaTag(pct) {
    if (pct === null) return '';
    if (Math.abs(pct) < 2) return `<span class="cmp-delta same">same</span>`;
    if (pct > 0) return `<span class="cmp-delta faster">+${pct}%</span>`;
    return `<span class="cmp-delta slower">${pct}%</span>`;
  }
  function barHtml(val, maxVal, mfr) {
    const w = Math.round(Math.min(val / maxVal, 1) * 140);
    return `<div class="cmp-bar-wrap"><div class="cmp-bar ${getBarClass(mfr)}" style="width:${w}px"></div><span style="font-size:0.85em;font-weight:600">${val.toLocaleString()}</span></div>`;
  }

  function runCompare() {
    const ids = ['cmpA','cmpB','cmpC'].map(id => document.getElementById(id).value).filter(v => v !== 'none');
    if (ids.length < 2) { alert('Please select at least two systems to compare.'); return; }

    const systems = ids.map(id => CMP_DATA[id]).filter(Boolean);
    const base = systems[0];

    const maxBw  = Math.max(...systems.map(s => s.bwGBs));
    const maxMem = Math.max(...systems.map(s => s.memGB));
    const maxTg7b = Math.max(...systems.map(s => s.tg7b));
    const maxTg70b = Math.max(...systems.filter(s => s.tg70b).map(s => s.tg70b), 1);

    // ── Header cards ──
    const colCount = systems.length;
    const headerHtml = `
      <div class="cmp-systems-bar" style="grid-template-columns: repeat(${colCount}, 1fr)">
        ${systems.map(s => `
          <div class="cmp-sys-header ${getSysClass(s.mfr)}">
            <div class="sys-name">${s.name}</div>
            <div class="sys-sub">${s.sub}</div>
            <div style="font-size:0.78em;color:#666;margin-top:0.3em">$${s.price.toLocaleString()} &middot; ${s.framework}</div>
          </div>`).join('')}
      </div>`;

    // ── Core specs table ──
    const makeRow = (label, vals, unit, maxVal, isLowerBetter) => {
      const cells = vals.map((v, i) => {
        if (v == null) return `<td>&mdash;</td>`;
        const pct = i === 0 ? null : pctDiff(vals[0], v);
        const adjustedPct = isLowerBetter && pct !== null ? -pct : pct;
        return `<td>
          ${barHtml(v, maxVal, systems[i].mfr)}
          ${unit ? `<div style="font-size:0.78em;color:#666">${unit}</div>` : ''}
          ${i > 0 ? deltaTag(adjustedPct) : '<span style="font-size:0.75em;color:#888">baseline</span>'}
        </td>`;
      });
      return `<tr><th class="row-label">${label}</th>${cells.join('')}</tr>`;
    };

    const specsHtml = `
      <div class="cmp-section">
        <div class="cmp-section-title">Core Hardware Specs</div>
        <table class="cmp-table">
          <thead><tr>
            <th class="row-label">Spec</th>
            ${systems.map(s => `<th>${s.name.split(' ').slice(-2).join(' ')}</th>`).join('')}
          </tr></thead>
          <tbody>
            ${makeRow('Memory / VRAM', systems.map(s=>s.memGB), 'GB', maxMem, false)}
            ${makeRow('Memory Bandwidth', systems.map(s=>s.bwGBs), 'GB/s', maxBw, false)}
            ${makeRow('Price', systems.map(s=>s.price), 'USD', Math.max(...systems.map(s=>s.price)), true)}
            <tr>
              <th class="row-label">Memory Type</th>
              ${systems.map(s => `<td>${s.unified ? 'Unified (CPU+GPU share)' : 'Discrete VRAM'}</td>`).join('')}
            </tr>
            <tr>
              <th class="row-label">CPU Cores</th>
              ${systems.map(s => `<td>${s.cpuCores ? s.cpuCores + ' cores' : 'N/A (add-in card)'}</td>`).join('')}
            </tr>
            <tr>
              <th class="row-label">GPU Cores</th>
              ${systems.map(s => `<td>${s.gpuCores ? s.gpuCores + ' cores' : 'CUDA/CU — varies'}</td>`).join('')}
            </tr>
            <tr>
              <th class="row-label">Notes</th>
              ${systems.map(s => `<td style="font-size:0.82em;color:#555">${s.notes}</td>`).join('')}
            </tr>
          </tbody>
        </table>
      </div>`;

    // ── Token speed table ──
    const tgVals = [
      {label:'7B Q4 (t/s)',   field:'tg7b',  maxV: maxTg7b},
      {label:'13B Q4 (t/s)',  field:'tg13b', maxV: Math.max(...systems.filter(s=>s.tg13b).map(s=>s.tg13b),1)},
      {label:'70B Q4 (t/s)',  field:'tg70b', maxV: maxTg70b},
      {label:'120B Q4 (t/s)', field:'tg120b',maxV: Math.max(...systems.filter(s=>s.tg120b).map(s=>s.tg120b),1)},
    ];

    const tokHtml = `
      <div class="cmp-section">
        <div class="cmp-section-title">Token Generation Speed (single-user, batch size 1, Q4_K_M)</div>
        <table class="cmp-table">
          <thead><tr>
            <th class="row-label">Model Size</th>
            ${systems.map(s => `<th>${s.name.split(' ').slice(-2).join(' ')}</th>`).join('')}
          </tr></thead>
          <tbody>
            ${tgVals.map(tv => {
              const vals = systems.map(s => s[tv.field]);
              const cells = vals.map((v, i) => {
                if (v == null) return `<td style="color:#bbb">Does not fit</td>`;
                const pct = i === 0 ? null : pctDiff(vals[0], v);
                return `<td>
                  ${barHtml(v, tv.maxV, systems[i].mfr)}
                  <div style="font-size:0.75em;color:#666">tokens/sec</div>
                  ${i > 0 ? deltaTag(pct) : '<span style="font-size:0.75em;color:#888">baseline</span>'}
                </td>`;
              });
              return `<tr><th class="row-label">${tv.label}</th>${cells.join('')}</tr>`;
            }).join('')}
          </tbody>
        </table>
        <p style="font-size:0.78em;color:#888;padding:0.5em 0.9em;margin:0">
          "Does not fit" = Q4 model exceeds 90% of available memory. Speed scales with bandwidth; capacity determines fit. 
          GPU cores affect speed by ~5–10% within the same chip family — bandwidth is the primary driver.
        </p>
      </div>`;

    // ── Model fit grid ──
    const fitHtml = `
      <div class="cmp-section">
        <div class="cmp-section-title">Model Size Compatibility (Green = fits fully in Q4, Yellow = fits in Q3/Q2 only, Red = too large)</div>
        ${systems.map(s => {
          const cells = MODEL_SIZES.map(m => {
            const usable = s.memGB * 0.88;
            const cls = m.gb <= usable ? 'fit-yes' : m.gb <= usable * 1.25 ? 'fit-partial' : 'fit-no';
            return `<div class="mf-cell ${cls}">
              <div class="mf-size">${m.label}</div>
              <div class="mf-label">${m.desc}</div>
              <div class="mf-label" style="margin-top:0.2em">${m.gb} GB Q4</div>
            </div>`;
          });
          return `<div style="padding:0 0 0 0.9em;margin-bottom:0.4em"><strong style="font-size:0.88em">${s.name} — ${s.sub}</strong></div>
          <div class="model-fit-grid">${cells.join('')}</div>`;
        }).join('<hr style="margin:0.5em 0;border:none;border-top:1px solid #eee">')}
      </div>`;

    // ── RAM impact explanation ──
    const ramImpactHtml = `
      <div class="cmp-section">
        <div class="cmp-section-title">What More RAM / VRAM Changes for Each System</div>
        <table class="cmp-table">
          <thead><tr>
            <th class="row-label">System</th>
            <th>Current Memory</th>
            <th>Effect of more RAM on speed</th>
            <th>Effect of more RAM on model access</th>
            <th>Upgrade path</th>
          </tr></thead>
          <tbody>
            ${systems.map(s => {
              const speedEffect = s.unified
                ? 'None — bandwidth is fixed by chip die. More RAM adds capacity only, not speed.'
                : 'None — VRAM bandwidth is fixed. More VRAM (next-tier card) could improve speed only if bandwidth also increases.';
              const capacityEffect = s.memGB >= 192
                ? `Already at 70B+ tier. Adding RAM enables 200B+ and 405B models.`
                : s.memGB >= 64
                  ? `+64 GB would unlock 70B Q4 (needs ~40 GB). +128 GB allows 120B Q3.`
                  : s.memGB >= 32
                    ? `+32 GB reaches 70B Q4 threshold (~40 GB needed). Significant step up.`
                    : `Need 3× more RAM to reach 70B Q4 tier.`;
              const upgrade = s.unified && s.mfr === 'apple'
                ? 'Apple RAM is soldered at purchase — choose higher config at ordering time.'
                : s.mfr === 'nvidia'
                  ? 'Upgrade to next VRAM tier (e.g. RTX 4090→A6000→A100) or add a second GPU (use vLLM with tensor parallel).'
                  : s.mfr === 'amd' && s.unified
                    ? 'RAM is soldered in Ryzen AI Max laptops. Select 128 GB at purchase and set BIOS iGPU VRAM allocation to 96–128 GB.'
                    : 'Consult system spec sheet.';
              return `<tr>
                <td style="font-weight:600;font-size:0.88em">${s.name}<br><span style="font-weight:400;color:#666">${s.sub}</span></td>
                <td>${s.memGB} GB ${s.unified ? 'unified' : 'VRAM'}</td>
                <td class="impact-neutral">${speedEffect}</td>
                <td>${capacityEffect}</td>
                <td style="font-size:0.84em;color:#555">${upgrade}</td>
              </tr>`;
            }).join('')}
          </tbody>
        </table>
      </div>`;

    // ── Factor weighting explanation ──
    const factorIntro = `
      <div class="cmp-section">
        <div class="cmp-section-title">What Each Factor Contributes to AI Inference Performance</div>
        <table class="cmp-table">
          <thead><tr>
            <th>Factor</th><th>Impact on Speed</th><th>Impact on Model Access</th><th>Notes</th>
          </tr></thead>
          <tbody>
            <tr>
              <td><strong>Memory Bandwidth</strong></td>
              <td><span class="speedup-tag speedup-high">Primary driver</span></td>
              <td>None</td>
              <td>Token generation speed scales linearly with bandwidth. Doubling bandwidth doubles t/s.</td>
            </tr>
            <tr>
              <td><strong>RAM / VRAM Capacity</strong></td>
              <td><span class="speedup-tag speedup-neutral">No effect</span></td>
              <td><span class="speedup-tag speedup-high">Primary driver</span></td>
              <td>More capacity lets larger models fit. Does not speed up inference of models that already fit.</td>
            </tr>
            <tr>
              <td><strong>GPU Compute (CUDA/GPU cores)</strong></td>
              <td><span class="speedup-tag speedup-low">Secondary</span></td>
              <td>None</td>
              <td>For inference at batch-size 1, compute is rarely the bottleneck. Matters more for batched/parallel requests and training.</td>
            </tr>
            <tr>
              <td><strong>CPU Core Count</strong></td>
              <td><span class="speedup-tag speedup-medium">Moderate (CPU-only)</span></td>
              <td>None</td>
              <td>On CPU-only inference, more physical cores allow parallel matrix rows. Limited by memory bandwidth ceiling.</td>
            </tr>
            <tr>
              <td><strong>Quantisation Level</strong></td>
              <td><span class="speedup-tag speedup-high">Major (software)</span></td>
              <td><span class="speedup-tag speedup-high">Major (software)</span></td>
              <td>Reduces bytes per weight — directly cuts bandwidth demand and memory footprint. Q4 is ~3× faster than FP16 for same bandwidth.</td>
            </tr>
            <tr>
              <td><strong>Storage Speed (NVMe)</strong></td>
              <td><span class="speedup-tag speedup-neutral">No effect at runtime</span></td>
              <td>None</td>
              <td>Only affects initial model load time. A 7 GB model with a 3 GB/s NVMe loads in ~2s vs ~10s on SATA. No effect on tokens/sec.</td>
            </tr>
            <tr>
              <td><strong>CPU Architecture (AVX-512, AMX)</strong></td>
              <td><span class="speedup-tag speedup-medium">Moderate (CPU-only)</span></td>
              <td>None</td>
              <td>AVX-512 VNNI (Icelake+) can give 1.4× CPU inference vs AVX2. Intel AMX (Sapphire Rapids) provides further acceleration for INT8.</td>
            </tr>
          </tbody>
        </table>
      </div>`;

    document.getElementById('compareMain').innerHTML =
      headerHtml + factorIntro + specsHtml + tokHtml + fitHtml + ramImpactHtml;
  }
</script>
</body>
</html>