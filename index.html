<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>AI Workstation & GPU Performance Report (2026)</title>
  <style>
    :root {
      --table-border-color: #ddd;
      --table-header-bg: #f2f2f2;
      --table-row-hover-bg: #f5f5f5;
      --font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
      --accent-color: #007bff;
      --text-color: #333;
      --bg-color: #fff;
      --note-bg-color: #f9f9f9;
      --border-radius: 6px;
    }
    body { 
      font-family: var(--font-family); 
      margin: 0; 
      padding: 1.5em; 
      background-color: var(--bg-color);
      color: var(--text-color);
      line-height: 1.6;
    }
    header {
      text-align: center;
      margin-bottom: 2em;
    }
    h1 {
      font-size: 2.2em;
      margin-bottom: 0.2em;
    }
    .subtitle {
      font-size: 1.1em;
      color: #666;
    }
    .tabs {
      display: flex;
      border-bottom: 2px solid var(--table-border-color);
      margin-bottom: 1.5em;
    }
    .tab-button {
      padding: 10px 20px;
      cursor: pointer;
      border: none;
      background-color: transparent;
      font-size: 1em;
      border-bottom: 2px solid transparent;
      margin-bottom: -2px;
    }
    .tab-button.active {
      border-bottom-color: var(--accent-color);
      font-weight: 600;
    }
    .tab-content { display: none; }
    .tab-content.active { display: block; }
    #controls {
      display: flex;
      gap: 1em;
      margin-bottom: 1.5em;
      align-items: center;
      flex-wrap: wrap;
    }
    #controls input, #controls select {
      padding: 0.6em;
      border: 1px solid #ccc;
      border-radius: var(--border-radius);
      font-size: 0.95em;
    }
    table { 
      border-collapse: collapse; 
      width: 100%; 
      margin-top: 1em;
      box-shadow: 0 1px 3px rgba(0,0,0,0.05);
    }
    th, td { 
      border: 1px solid var(--table-border-color); 
      padding: 0.75em; 
      text-align: left; 
      vertical-align: top;
    }
    th { 
      background: var(--table-header-bg); 
      cursor: pointer; 
      font-weight: 600;
      position: sticky;
      top: 0;
    }
    tr:hover { background: var(--table-row-hover-bg); }
    .notes-cell {
      font-size: 0.9em;
      line-height: 1.5;
      background-color: var(--note-bg-color);
    }
    .notes-cell ul {
      padding-left: 1.2em;
      margin: 0;
    }
    .notes-cell li {
      margin-bottom: 0.5em;
    }
    .highlight-best {
      background-color: #e8f5e9; /* Light green */
      font-weight: bold;
    }
    .highlight-best td:first-child::before {
      content: '';
    }
    #diy-comparison {
      padding: 1em;
    }
    #diy-comparison h3 {
      margin-top: 0;
    }
    .comparison-grid {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
      gap: 1.5em;
    }
    .comparison-card {
      border: 1px solid var(--table-border-color);
      border-radius: var(--border-radius);
      padding: 1.5em;
    }
    .comparison-card h4 {
      margin-top: 0;
      color: var(--accent-color);
    }
    .price-diff {
      font-weight: bold;
    }
    .price-diff.positive { color: #28a745; }
    .price-diff.negative { color: #dc3545; }

    /* ── GPU Rankings tab ─────────────────────────────────────── */
    #gpu-controls { display: flex; gap: 0.8em; flex-wrap: wrap; margin-bottom: 1.2em; align-items: center; }
    #gpu-controls select, #gpu-controls input { padding: 0.55em 0.75em; border: 1px solid #ccc; border-radius: var(--border-radius); font-size: 0.93em; }
    .mfr-filter-group { display: flex; gap: 0.4em; }
    .mfr-btn {
      padding: 0.45em 1em; border: 2px solid #ccc; border-radius: 20px;
      background: #fff; cursor: pointer; font-size: 0.9em; font-weight: 500; transition: all 0.15s;
    }
    .mfr-btn.active-nvidia  { border-color: #76b900; background: #76b900; color: #fff; }
    .mfr-btn.active-amd     { border-color: #ed1c24; background: #ed1c24; color: #fff; }
    .mfr-btn.active-apple   { border-color: #555;    background: #555;    color: #fff; }
    .mfr-btn.active-all     { border-color: var(--accent-color); background: var(--accent-color); color: #fff; }
    .score-bar-wrap { display: flex; align-items: center; gap: 0.5em; min-width: 120px; }
    .score-bar { height: 10px; border-radius: 5px; background: var(--accent-color); transition: width 0.3s; }
    .score-num { font-size: 0.85em; white-space: nowrap; }
    .mfr-badge {
      display: inline-block; padding: 2px 8px; border-radius: 10px; font-size: 0.78em;
      font-weight: 700; letter-spacing: 0.04em;
    }
    .badge-nvidia { background: #d4edda; color: #155724; }
    .badge-amd    { background: #fde8e8; color: #721c24; }
    .badge-apple  { background: #e2e3e5; color: #383d41; }
    .mfr-comparison-section { margin-top: 2em; }
    .mfr-comparison-section h3 { margin-bottom: 0.8em; }
    .mfr-cards { display: grid; grid-template-columns: repeat(auto-fit, minmax(280px, 1fr)); gap: 1.2em; }
    .mfr-card { border-radius: var(--border-radius); padding: 1.4em; border: 1px solid var(--table-border-color); }
    .mfr-card.nvidia { border-top: 4px solid #76b900; }
    .mfr-card.amd    { border-top: 4px solid #ed1c24; }
    .mfr-card.apple  { border-top: 4px solid #555; }
    .mfr-card h4 { margin: 0 0 0.7em; font-size: 1.05em; }
    .mfr-card ul { margin: 0; padding-left: 1.2em; font-size: 0.92em; line-height: 1.7; }
    .mfr-card .verdict { margin-top: 0.8em; padding: 0.6em 0.9em; background: var(--note-bg-color); border-radius: 4px; font-size: 0.88em; }
    #gpuTable th { white-space: nowrap; }

    /* ── Best Match Finder tab ────────────────────────────────── */
    #bestmatch { padding: 0; }
    .finder-layout { display: grid; grid-template-columns: 340px 1fr; gap: 2em; align-items: start; }
    @media (max-width: 800px) { .finder-layout { grid-template-columns: 1fr; } }
    .finder-form { background: var(--note-bg-color); border: 1px solid var(--table-border-color); border-radius: var(--border-radius); padding: 1.6em; }
    .finder-form h3 { margin-top: 0; }
    .form-group { margin-bottom: 1.1em; }
    .form-group label { display: block; font-weight: 600; margin-bottom: 0.35em; font-size: 0.95em; }
    .form-group select, .form-group input[type=range] { width: 100%; padding: 0.55em 0.7em; border: 1px solid #ccc; border-radius: var(--border-radius); font-size: 0.93em; box-sizing: border-box; }
    .budget-display { font-size: 1.05em; font-weight: 700; color: var(--accent-color); margin-top: 0.25em; }
    .os-btn-group { display: flex; gap: 0.4em; flex-wrap: wrap; }
    .os-btn {
      flex: 1; padding: 0.55em 0.5em; border: 2px solid #ccc; border-radius: var(--border-radius);
      background: #fff; cursor: pointer; font-size: 0.9em; font-weight: 500; text-align: center; transition: all 0.15s;
    }
    .os-btn.active { border-color: var(--accent-color); background: var(--accent-color); color: #fff; }
    .find-btn {
      width: 100%; padding: 0.75em; background: var(--accent-color); color: #fff;
      border: none; border-radius: var(--border-radius); font-size: 1em; font-weight: 700;
      cursor: pointer; margin-top: 0.4em; transition: opacity 0.15s;
    }
    .find-btn:hover { opacity: 0.88; }
    .result-area { min-height: 200px; }
    .result-placeholder { color: #999; font-style: italic; margin-top: 1em; }
    .result-card {
      border: 2px solid var(--accent-color); border-radius: var(--border-radius);
      padding: 1.6em; background: #f0f7ff;
    }
    .result-card .result-rank { font-size: 0.82em; color: #666; text-transform: uppercase; letter-spacing: 0.06em; }
    .result-card .result-name { font-size: 1.5em; font-weight: 700; margin: 0.15em 0 0.5em; }
    .result-card .result-meta { display: flex; flex-wrap: wrap; gap: 0.6em; margin-bottom: 1em; }
    .result-meta-chip {
      background: #fff; border: 1px solid #ccc; border-radius: 20px;
      padding: 0.25em 0.75em; font-size: 0.85em; font-weight: 500;
    }
    .result-card .result-why { font-size: 0.93em; line-height: 1.6; }
    .result-card .result-why strong { color: var(--accent-color); }
    .alt-results { margin-top: 1.4em; }
    .alt-results h4 { margin-bottom: 0.6em; font-size: 0.95em; color: #555; }
    .alt-card {
      border: 1px solid var(--table-border-color); border-radius: var(--border-radius);
      padding: 0.9em 1.1em; margin-bottom: 0.7em; background: #fff;
      display: flex; justify-content: space-between; align-items: flex-start; gap: 1em;
    }
    .alt-card .alt-name { font-weight: 600; }
    .alt-card .alt-reason { font-size: 0.85em; color: #666; margin-top: 0.2em; }
    .alt-card .alt-price { font-weight: 700; white-space: nowrap; color: #333; }
    .no-result { padding: 1.5em; background: #fff8e1; border: 1px solid #ffe082; border-radius: var(--border-radius); }

    /* ── Compare Systems tab ──────────────────────────────────── */
    #compare { padding: 0; }
    .compare-layout { display: grid; grid-template-columns: 280px 1fr; gap: 2em; align-items: start; }
    @media (max-width: 860px) { .compare-layout { grid-template-columns: 1fr; } }
    .compare-sidebar { position: sticky; top: 1em; background: var(--note-bg-color); border: 1px solid var(--table-border-color); border-radius: var(--border-radius); padding: 1.4em; }
    .compare-sidebar h3 { margin-top: 0; font-size: 1em; }
    .cmp-group { margin-bottom: 1.1em; }
    .cmp-group label { display: block; font-weight: 600; font-size: 0.86em; margin-bottom: 0.3em; color: #555; text-transform: uppercase; letter-spacing: 0.05em; }
    .cmp-group select { width: 100%; padding: 0.5em 0.65em; border: 1px solid #ccc; border-radius: var(--border-radius); font-size: 0.92em; box-sizing: border-box; }
    .cmp-run-btn { width: 100%; padding: 0.7em; background: var(--accent-color); color: #fff; border: none; border-radius: var(--border-radius); font-size: 0.97em; font-weight: 700; cursor: pointer; margin-top: 0.5em; }
    .cmp-run-btn:hover { opacity: 0.88; }
    .compare-main { min-height: 300px; }
    .cmp-placeholder { color: #999; font-style: italic; padding: 1em 0; }
    .cmp-systems-bar { display: grid; gap: 1em; margin-bottom: 1.4em; }
    .cmp-sys-header { padding: 1em 1.3em; border-radius: var(--border-radius); border-top: 4px solid var(--accent-color); background: #f0f7ff; }
    .cmp-sys-header.sys-nvidia  { border-top-color: #76b900; background: #f1fbe8; }
    .cmp-sys-header.sys-amd     { border-top-color: #ed1c24; background: #fff0f0; }
    .cmp-sys-header.sys-apple   { border-top-color: #555;    background: #f4f4f4; }
    .cmp-sys-header .sys-name   { font-size: 1.1em; font-weight: 700; margin-bottom: 0.2em; }
    .cmp-sys-header .sys-sub    { font-size: 0.82em; color: #666; }
    .cmp-section { margin-bottom: 1.6em; border: 1px solid var(--table-border-color); border-radius: var(--border-radius); overflow: hidden; }
    .cmp-section-title { background: var(--table-header-bg); padding: 0.6em 1em; font-weight: 700; font-size: 0.88em; border-bottom: 1px solid var(--table-border-color); }
    .cmp-table { width: 100%; border-collapse: collapse; font-size: 0.9em; }
    .cmp-table td, .cmp-table th { padding: 0.6em 0.9em; border-bottom: 1px solid var(--table-border-color); vertical-align: middle; }
    .cmp-table th { background: var(--table-header-bg); font-weight: 600; font-size: 0.82em; white-space: nowrap; }
    .cmp-table tr:last-child td, .cmp-table tr:last-child th { border-bottom: none; }
    .cmp-table .row-label { color: #555; font-size: 0.85em; width: 160px; font-weight: 600; }
    .cmp-val { font-weight: 600; }
    .cmp-bar-wrap { display: flex; align-items: center; gap: 0.5em; }
    .cmp-bar { height: 8px; border-radius: 4px; background: var(--accent-color); min-width: 2px; display: inline-block; }
    .cmp-bar.bar-nvidia { background: #76b900; }
    .cmp-bar.bar-amd    { background: #ed1c24; }
    .cmp-bar.bar-apple  { background: #888; }
    .cmp-delta { display: inline-block; padding: 0.12em 0.55em; border-radius: 3px; font-size: 0.78em; font-weight: 700; margin-left: 0.4em; }
    .cmp-delta.faster  { background: #d4edda; color: #155724; }
    .cmp-delta.slower  { background: #f8d7da; color: #721c24; }
    .cmp-delta.same    { background: #e2e3e5; color: #383d41; }
    .cmp-winner-badge  { display: inline-block; background: var(--accent-color); color: #fff; font-size: 0.72em; padding: 0.15em 0.55em; border-radius: 3px; margin-left: 0.4em; vertical-align: middle; font-weight: 700; }
    .model-fit-grid { display: grid; grid-template-columns: repeat(auto-fill, minmax(120px, 1fr)); gap: 0.6em; padding: 0.9em; }
    .mf-cell { border: 1px solid var(--table-border-color); border-radius: 4px; padding: 0.55em 0.7em; text-align: center; font-size: 0.82em; }
    .mf-cell .mf-size { font-weight: 700; font-size: 1em; }
    .mf-cell .mf-label { font-size: 0.72em; color: #666; margin-top: 0.1em; }
    .mf-cell.fit-yes     { background: #d4edda; border-color: #a3d4ae; }
    .mf-cell.fit-partial { background: #fff3cd; border-color: #e5c44a; }
    .mf-cell.fit-no      { background: #f8d7da; border-color: #e4a0a6; }
    .impact-positive { color: #28a745; font-weight: 600; }
    .impact-negative { color: #dc3545; font-weight: 600; }
    .factor-key { display: inline-block; width: 10px; height: 10px; border-radius: 2px; margin-right: 0.3em; vertical-align: middle; }

    /* ── Optimization Guide tab ───────────────────────────────── */
    #optimization { padding: 0; }
    .opt-intro { font-size: 0.93em; color: #555; margin-bottom: 1.5em; line-height: 1.7; }
    .opt-section { margin-bottom: 2em; }
    .opt-section > h3 { margin-bottom: 0.6em; font-size: 1.05em; padding-bottom: 0.4em; border-bottom: 2px solid var(--table-border-color); }
    .opt-cards { display: grid; grid-template-columns: repeat(auto-fit, minmax(290px, 1fr)); gap: 1.2em; }
    .opt-card { border: 1px solid var(--table-border-color); border-radius: var(--border-radius); overflow: hidden; }
    .opt-card-header { padding: 0.75em 1.1em; font-weight: 700; font-size: 0.92em; display: flex; align-items: center; gap: 0.6em; border-bottom: 1px solid var(--table-border-color); }
    .opt-card-header.opt-nvidia { background: #f1fbe8; border-left: 4px solid #76b900; }
    .opt-card-header.opt-amd    { background: #fff0f0; border-left: 4px solid #ed1c24; }
    .opt-card-header.opt-apple  { background: #f4f4f4; border-left: 4px solid #555; }
    .opt-card-header.opt-general{ background: #f0f7ff; border-left: 4px solid var(--accent-color); }
    .opt-card-body { padding: 1em 1.1em; font-size: 0.9em; }
    .opt-card-body ul { margin: 0; padding-left: 1.2em; }
    .opt-card-body li { margin-bottom: 0.6em; line-height: 1.5; }
    .opt-table { width: 100%; border-collapse: collapse; margin-top: 0.8em; font-size: 0.88em; }
    .opt-table th, .opt-table td { padding: 0.55em 0.75em; border: 1px solid var(--table-border-color); text-align: left; }
    .opt-table th { background: var(--table-header-bg); font-weight: 600; }
    .speedup-tag { display: inline-block; padding: 0.15em 0.55em; border-radius: 3px; font-size: 0.82em; font-weight: 700; }
    .speedup-high   { background: #d4edda; color: #155724; }
    .speedup-medium { background: #fff3cd; color: #856404; }
    .speedup-low    { background: #f8d7da; color: #721c24; }
    .speedup-neutral{ background: #e2e3e5; color: #383d41; }
    /* ── Operating Systems tab ── */
    .opt-card-header.os-header-windows { background: #e8f4fd; border-left: 4px solid #0078d4; }
    .opt-card-header.os-header-mac     { background: #f4f4f4; border-left: 4px solid #555; }
    .opt-card-header.os-header-linux   { background: #fff8f0; border-left: 4px solid #e67e00; }
    .support-yes     { color: #155724; font-weight: 600; }
    .support-partial { color: #856404; font-weight: 600; }
    .support-no      { color: #721c24; font-weight: 600; }
    /* ── Software tab ── */
    .sw-tag { display: inline-block; padding: 0.1em 0.5em; border-radius: 3px; font-size: 0.78em; font-weight: 700; margin: 0 0.15em 0.25em 0; vertical-align: middle; }
    .sw-free     { background: #d4edda; color: #155724; }
    .sw-freemium { background: #fff3cd; color: #856404; }
    .sw-paid     { background: #f8d7da; color: #721c24; }
    .sw-win      { background: #e8f4fd; color: #004a8c; }
    .sw-mac      { background: #f0f0f0; color: #333; }
    .sw-lin      { background: #fff0e0; color: #8a3c00; }
    .sw-all      { background: #e9e9e9; color: #333; }
    .sw-tool-name { font-size: 1em; font-weight: 700; margin-bottom: 0.2em; }
    .sw-tool-desc { font-size: 0.88em; color: #555; margin-bottom: 0.5em; }
  </style>
</head>
<body>
<header>
  <h1>AI Workstation & GPU Performance Report (2026)</h1>
  <p class="subtitle">A comparative analysis of pre-built and DIY systems for local AI development.</p>
</header>

<div class="tabs">
  <button class="tab-button active" onclick="openTab(event, 'prebuilt')">Pre-built Systems</button>
  <button class="tab-button" onclick="openTab(event, 'diy')">DIY vs. Pre-built</button>
  <button class="tab-button" onclick="openTab(event, 'gpus'); initGpuTab()">GPU Rankings</button>
  <button class="tab-button" onclick="openTab(event, 'bestmatch')">Best Match Finder</button>
  <button class="tab-button" onclick="openTab(event, 'compare')">Compare Systems</button>
  <button class="tab-button" onclick="openTab(event, 'optimization')">Optimization Guide</button>
  <button class="tab-button" onclick="openTab(event, 'opsystems')">Operating Systems</button>
  <button class="tab-button" onclick="openTab(event, 'software')">Software</button>
  <a href="benchmark.html" class="tab-button" style="text-decoration:none;margin-left:auto">Benchmark My System</a>
</div>

<div id="prebuilt" class="tab-content active">
  <div id="controls">
    <input type="text" id="searchInput" onkeyup="filterTable()" placeholder="Search by model, GPU, brand...">
    <select id="modelFilter" onchange="filterTable()">
      <option value="">Filter by Best Model...</option>
      <option>70B+ (Unquantized)</option>
      <option>120B+ (Quantized)</option>
      <option>30B-70B</option>
      <option>7B-30B</option>
    </select>
    <select id="typeFilter" onchange="filterTable()">
      <option value="">Filter by Type...</option>
      <option>Desktop</option>
      <option>Laptop</option>
    </select>
    <select id="osFilter" onchange="filterTable()">
      <option value="">Filter by OS...</option>
      <option>Windows</option>
      <option>macOS</option>
      <option>Linux</option>
    </select>
  </div>
  <p>Table columns are sortable. All systems are evaluated for AI/ML workloads. TG = Tokens/sec Generation (batch size 1).</p>
  <table id="hardwareTable">
    <thead>
      <tr>
        <th onclick="sortTable(0)">Rank</th>
        <th onclick="sortTable(1)">Model</th>
        <th onclick="sortTable(2)">Type</th>
        <th onclick="sortTable(3)">GPU</th>
        <th onclick="sortTable(4)">VRAM</th>
        <th onclick="sortTable(5)">Best Model (Size)</th>
        <th onclick="sortTable(6)">Tokens/sec (TG)</th>
        <th onclick="sortTable(7)">Price</th>
        <th onclick="sortTable(8)">Notes & Evidence</th>
      </tr>
    </thead>
    <tbody>
      <tr class="highlight-best">
        <td>1</td>
        <td>HP Z8 G4 (Custom)</td>
        <td>Desktop</td>
        <td>Quad NVIDIA A100 40GB</td>
        <td>160 GB</td>
        <td>70B+ (Unquantized)</td>
        <td>~540</td>
        <td>$30,000</td>
        <td class="notes-cell"><ul>
          <li><strong>Data Center Power:</strong> Can run unquantized 70B models entirely in VRAM.</li>
          <li><strong>NVLink Advantage:</strong> Full peer-to-peer communication between GPUs is critical for massive model inference.</li>
          <li><strong>Secondary Market:</strong> Often available on the used market, making it a "budget" entry into high-end AI.</li>
          <li><strong>Power/Cooling:</strong> Requires significant power and cooling infrastructure not suitable for a typical office.</li>
        </ul></td>
      </tr>
      <tr>
        <td>2</td>
        <td>Dell Precision 7960 Tower</td>
        <td>Desktop</td>
        <td>Quad NVIDIA RTX 5000 Ada</td>
        <td>128 GB</td>
        <td>120B+ (Quantized)</td>
        <td>~450</td>
        <td>$20,000</td>
        <td class="notes-cell"><ul>
          <li><strong>Enterprise Grade:</strong> Built for high-concurrency professional AI, with certified drivers and support.</li>
          <li><strong>RTX 6000 Ada Recommended:</strong> For max performance, users often upgrade to RTX 6000 Ada GPUs for more VRAM and CUDA cores.</li>
          <li><strong>NVLink Bridge:</strong> Essential for combining GPU memory pools effectively.</li>
        </ul></td>
      </tr>
      <tr>
        <td>3</td>
        <td>Apple Mac Studio (M3 Ultra)</td>
        <td>Desktop</td>
        <td>Apple 80-core GPU</td>
        <td>192 GB (Unified)</td>
        <td>120B+ (Quantized)</td>
        <td>~97</td>
        <td>$8,000</td>
        <td class="notes-cell"><ul>
          <li><strong>Memory Bandwidth King:</strong> Unmatched unified memory is ideal for huge model inference.</li>
          <li><strong>Underwhelming TG?:</strong> Some users report tokens/sec (TG) is lower than expected vs. high-end NVIDIA cards (e.g., 97 t/s vs 136 t/s on an RTX 3090 for some models).</li>
          <li><strong>Silent & Efficient:</strong> Consumes far less power and is virtually silent compared to NVIDIA workstations.</li>
          <li><strong>MLX Framework:</strong> Apple's MLX framework can be 20-30% faster than other libraries like Ollama on Apple Silicon.</li>
        </ul></td>
      </tr>
      <tr>
        <td>4</td>
        <td>HP OMEN 45L (2025)</td>
        <td>Desktop</td>
        <td>NVIDIA RTX 5090</td>
        <td>32 GB</td>
        <td>70B+ (Quantized)</td>
        <td>~140</td>
        <td>$4,500</td>
        <td class="notes-cell"><ul>
          <li><strong>New Flagship:</strong> The RTX 5090 offers ~1,792 GB/s bandwidth, a significant leap for memory-bound LLM tasks.</li>
          <li><strong>MoE Performance:</strong> Excels at Mixture-of-Experts models, with benchmarks showing 100-140 t/s generation.</li>
          <li><strong>"Space Heater":</strong> Early reports suggest very high power draw and thermal output, requiring excellent case airflow.</li>
        </ul></td>
      </tr>
       <tr>
        <td>5</td>
        <td>Alienware Aurora R16</td>
        <td>Desktop</td>
        <td>NVIDIA RTX 4090</td>
        <td>24 GB</td>
        <td>30B-70B</td>
        <td>~95</td>
        <td>$4,000</td>
        <td class="notes-cell"><ul>
          <li><strong>Community Favorite:</strong> Widely regarded as the best value for running 7B to 30B models locally.</li>
          <li><strong>30% Slower than 5090:</strong> A significant performance gap, but at a much lower price point.</li>
          <li><strong>Stable & Supported:</strong> Mature drivers and extensive community support for AI/ML workloads.</li>
        </ul></td>
      </tr>
      <tr>
        <td>6</td>
        <td>Lenovo ThinkStation P620</td>
        <td>Desktop</td>
        <td>Dual NVIDIA RTX A6000</td>
        <td>96 GB</td>
        <td>120B+ (Quantized)</td>
        <td>~300</td>
        <td>$15,000</td>
        <td class="notes-cell"><ul>
            <li><strong>CPU Powerhouse:</strong> The Threadripper Pro 5995WX CPU provides massive PCIe bandwidth for multiple GPUs.</li>
            <li><strong>PSU/Fitment Issues:</strong> Real-world user threads mention PSU limitations and tight physical space when fitting two high-end GPUs.</li>
            <li><strong>Dual A6000 Recommended:</strong> Community advice points to using two RTX A6000s as a potent, albeit expensive, combination.</li>
        </ul></td>
      </tr>
      <tr>
        <td>7</td>
        <td>Asus ROG Strix Scar 18</td>
        <td>Laptop</td>
        <td>NVIDIA RTX 5090 (Laptop)</td>
        <td>24 GB</td>
        <td>30B-70B</td>
        <td>~80</td>
        <td>$4,500</td>
        <td class="notes-cell"><ul>
            <li><strong>Desktop-Class Laptop GPU:</strong> Benchmarks from similar MSI models show impressive performance but with caveats.</li>
            <li><strong>Thermal Throttling:</strong> The main challenge is keeping the GPU cool enough to sustain peak performance.</li>
            <li><strong>VRAM is 24GB:</strong> Note: Leaked specs showing 32GB were incorrect for the laptop version.</li>
        </ul></td>
      </tr>
      <tr>
        <td>8</td>
        <td>Apple MacBook Pro 16 (M3 Max)</td>
        <td>Laptop</td>
        <td>Apple 30-core GPU</td>
        <td>64 GB (Unified)</td>
        <td>30B-70B</td>
        <td>~40</td>
        <td>$3,000</td>
        <td class="notes-cell"><ul>
            <li><strong>Popular with ML Engineers:</strong> Excellent for development and testing on the go due to its large unified memory.</li>
            <li><strong>"Not for Heavy Training":</strong> Community consensus is that it's an inference machine, not for training large models from scratch.</li>
            <li><strong>Performance:</strong> Can achieve 35-40 t/s on 14B parameter models (Q4 quantized).</li>
        </ul></td>
      </tr>
      <tr>
        <td>9</td>
        <td>Asus ROG Flow Z13 (2025)</td>
        <td>Laptop</td>
        <td>AMD Radeon 780M (iGPU)</td>
        <td>32 GB (Shared)</td>
        <td>7B-30B</td>
        <td>~25</td>
        <td>$2,100</td>
        <td class="notes-cell"><ul>
            <li><strong>The "Unicorn" APU:</strong> The Ryzen AI 300 series is highly praised for its powerful integrated GPU.</li>
            <li><strong>ROCm vs. Vulkan:</strong> Performance depends heavily on the backend used; vLLM support is a major plus.</li>
            <li><strong>BIOS Gotcha:</strong> Users report needing to manually allocate 32GB of system RAM to the iGPU in BIOS for best performance.</li>
        </ul></td>
      </tr>
      <tr>
        <td>10</td>
        <td>ASUS Zenbook A14 (2025)</td>
        <td>Laptop</td>
        <td>Qualcomm Adreno NPU</td>
        <td>32 GB (Shared)</td>
        <td>7B-30B</td>
        <td>~6</td>
        <td>$1,800</td>
        <td class="notes-cell"><ul>
            <li><strong>On-Device AI Focus:</strong> Designed for efficient, low-power AI tasks.</li>
            <li><strong>NPU Not Yet Supported:</strong> The powerful NPU is not currently usable by most open-source LLM tools (e.g., llama.cpp).</li>
            <li><strong>Community Verdict:</strong> Currently not recommended for serious local LLM work until software support matures. Performance is low (~6 t/s on 32B models).</li>
        </ul></td>
      </tr>
    </tbody>
  </table>
</div>

<div id="diy" class="tab-content">
  <div id="diy-comparison">
    <h3>DIY Cost Savings & Performance Analysis</h3>
    <p>Building your own PC can offer significant savings and tailored performance. Here’s a comparison against popular pre-built options.</p>
    <div class="comparison-grid">
      <div class="comparison-card">
        <h4>DIY Build: "The 4090 Killer"</h4>
        <p><strong>Components:</strong> Intel Core i7-14700K, NVIDIA RTX 4090 (24GB), 64GB DDR5 RAM, 2TB NVMe SSD, 1000W PSU.</p>
        <p><strong>Estimated DIY Cost:</strong> $3,200</p>
        <p><strong>Comparable Pre-built:</strong> Alienware Aurora R16 (~$4,000)</p>
        <p><strong>Savings:</strong> <span class="price-diff positive">~$800 (20%)</span></p>
        <p><strong>Notes:</strong> Offers identical core performance for LLMs as the pre-built but requires assembly. You get to choose higher quality components (e.g., PSU, motherboard).</p>
      </div>
      <div class="comparison-card">
        <h4>DIY Build: "Dual 3090 Workstation"</h4>
        <p><strong>Components:</strong> AMD Ryzen 9 7950X, 2x NVIDIA RTX 3090 (24GB, used), 128GB DDR5 RAM, 4TB NVMe SSD, 1600W PSU.</p>
        <p><strong>Estimated DIY Cost:</strong> $4,500 (with used GPUs)</p>
        <p><strong>Comparable Pre-built:</strong> Older high-end workstations (~$8,000+ new)</p>
        <p><strong>Savings:</strong> <span class="price-diff positive">~$3,500+ (40-50%)</span></p>
        <p><strong>Notes:</strong> By using second-hand RTX 3090s (known for their 24GB VRAM), you can build a 48GB VRAM machine for a fraction of the cost of a new professional workstation. NVLink support is absent, but performance is still excellent for the price.</p>
      </div>
       <div class="comparison-card">
        <h4>When to Buy Pre-built</h4>
        <p><strong>Warranty & Support:</strong> A single point of contact for troubleshooting is invaluable for mission-critical work.</p>
        <p><strong>Certified Drivers:</strong> Workstations from Dell, HP, and Lenovo come with optimized and certified drivers for professional applications.</p>
        <p><strong>Time is Money:</strong> The time spent building, configuring, and troubleshooting a DIY machine can outweigh the cost savings for professionals.</p>
        <p><strong>Specialized Hardware:</strong> Systems with multiple pro-grade GPUs (like Quad A100s) require chassis and motherboards not typically available to consumers.</p>
      </div>
    </div>
  </div>
</div>

<!-- ═══════════════════════════════════════════════════════════ GPU RANKINGS TAB -->
<div id="gpus" class="tab-content">
  <div id="gpu-controls">
    <div class="mfr-filter-group">
      <button class="mfr-btn active-all" id="mfr-all"    onclick="setMfrFilter('all')">All</button>
      <button class="mfr-btn"            id="mfr-nvidia" onclick="setMfrFilter('NVIDIA')">NVIDIA</button>
      <button class="mfr-btn"            id="mfr-amd"    onclick="setMfrFilter('AMD')">AMD</button>
      <button class="mfr-btn"            id="mfr-apple"  onclick="setMfrFilter('Apple')">Apple</button>
    </div>
    <select id="gpuSortSelect" onchange="renderGpuTable()">
      <option value="score">Sort: Effectiveness Score</option>
      <option value="tg7b">Sort: Tokens/sec (7B Q4)</option>
      <option value="bw">Sort: Memory Bandwidth</option>
      <option value="vram">Sort: VRAM</option>
      <option value="price">Sort: Price (Low→High)</option>
      <option value="valueScore">Sort: Value (t/s per $)</option>
    </select>
    <input type="text" id="gpuSearch" oninput="renderGpuTable()" placeholder="Search GPUs…" style="min-width:160px;">
  </div>
  <p style="margin-top:0;font-size:0.92em;color:#555;">
    <strong>Effectiveness Score</strong> blends raw token generation speed, memory bandwidth, and VRAM capacity into a single 0–100 index.
    <strong>Value Score</strong> = tokens/sec per $100 spent. Click any column header to sort.
  </p>
  <table id="gpuTable">
    <thead>
      <tr>
        <th onclick="gpuSortCol('rank')">#</th>
        <th onclick="gpuSortCol('name')">GPU</th>
        <th onclick="gpuSortCol('mfr')">Maker</th>
        <th onclick="gpuSortCol('vram')">VRAM (GB)</th>
        <th onclick="gpuSortCol('bw')">Bandwidth (GB/s)</th>
        <th onclick="gpuSortCol('tg7b')">TG 7B Q4 (t/s)</th>
        <th onclick="gpuSortCol('tg70b')">TG 70B Q4 (t/s)</th>
        <th onclick="gpuSortCol('price')">Price (USD)</th>
        <th onclick="gpuSortCol('valueScore')">Value Score</th>
        <th onclick="gpuSortCol('score')">Effectiveness</th>
        <th onclick="gpuSortCol('bestFor')">Best For</th>
      </tr>
    </thead>
    <tbody id="gpuTableBody"></tbody>
  </table>

  <div class="mfr-comparison-section">
    <h3>Manufacturer Tradeoffs</h3>
    <div class="mfr-cards">
      <div class="mfr-card nvidia">
        <h4>NVIDIA</h4>
        <ul>
          <li><strong>Speed king:</strong> Highest tokens/sec for models that fit in VRAM — RTX 5090 at 1,792 GB/s is unmatched on the consumer market.</li>
          <li><strong>CUDA ecosystem:</strong> Every AI framework (PyTorch, vLLM, TensorRT, llama.cpp) works out of the box. Zero configuration headaches.</li>
          <li><strong>VRAM ceiling:</strong> Consumer cards top out at 32GB (5090). Professional cards (RTX 6000 Ada: 48GB, A100: 80GB) cost significantly more.</li>
          <li><strong>Training:</strong> Best-in-class for fine-tuning and training with full CUDA support.</li>
          <li><strong>Power draw:</strong> RTX 5090 ≈ 575W — a space heater. Dedicated airflow/cooling needed.</li>
          <li><strong>P2P blocked:</strong> Consumer multi-GPU peer-to-peer is disabled by default in drivers (workaround requires patched kernel modules).</li>
        </ul>
        <div class="verdict"><strong>Choose NVIDIA if:</strong> you run models &le;30B and want maximum speed, or need fine-tuning &amp; training.</div>
      </div>
      <div class="mfr-card amd">
        <h4>AMD</h4>
        <ul>
          <li><strong>Ryzen AI Max+ 395:</strong> The standout — 128GB unified memory at ~$1,500–2,500 is the cheapest way to run 70B models on x86.</li>
          <li><strong>Bandwidth:</strong> Radeon RX 7900 XTX has 960 GB/s — competitive with NVIDIA RTX 4090 (1,008 GB/s), at a lower price.</li>
          <li><strong>ROCm ecosystem:</strong> Improving rapidly (vLLM support added 2024), but still lags CUDA. Occasional bugs, fewer tutorials.</li>
          <li><strong>Vulkan backend:</strong> llama.cpp Vulkan is often faster than ROCm for pure inference; ~20% advantage in recent tests.</li>
          <li><strong>Discrete VRAM limit:</strong> RX 7900 XTX tops at 24GB — same ceiling as RTX 4090.</li>
          <li><strong>BIOS gotcha:</strong> Ryzen AI Max laptops default to 32GB GPU allocation; must manually set to 96–128GB for proper performance.</li>
        </ul>
        <div class="verdict"><strong>Choose AMD if:</strong> you need 70B+ on a budget (Ryzen AI Max+ 395), or want competitive discrete GPU performance at lower cost than NVIDIA.</div>
      </div>
      <div class="mfr-card apple">
        <h4>Apple Silicon</h4>
        <ul>
          <li><strong>Unified memory:</strong> M3 Ultra at 192–512GB is the only consumer path to running 200B+ models locally without a multi-GPU rig.</li>
          <li><strong>MLX framework:</strong> Apple-optimized; 20–30% faster than Ollama for inference. Speculative decoding can roughly double token generation.</li>
          <li><strong>Token generation speed:</strong> M3 Ultra (~800 GB/s) loses to RTX 5090 (1,792 GB/s) for models that fit in VRAM — ~97 t/s vs ~250 t/s on 7B.</li>
          <li><strong>Power efficiency:</strong> Mac Studio at ~150W vs RTX 5090 system at ~800W+. Silent under moderate load.</li>
          <li><strong>Concurrency:</strong> Handles concurrent requests worse than NVIDIA; not ideal for multi-user serving.</li>
          <li><strong>No GPU-only option:</strong> Apple Silicon is integrated — you buy the whole computer, not a card.</li>
          <li><strong>macOS only:</strong> No Linux, no CUDA. Only MLX, llama.cpp, and Ollama with Metal.</li>
        </ul>
        <div class="verdict"><strong>Choose Apple if:</strong> you need to run very large models (70B+) in a quiet, efficient package, or you're already in the Apple ecosystem.</div>
      </div>
    </div>
  </div>
</div>

<!-- ═══════════════════════════════════════════════════════ BEST MATCH FINDER TAB -->
<div id="bestmatch" class="tab-content">
  <h3 style="margin-top:0;">Find Your Best Match</h3>
  <p style="margin-top:-0.5em;color:#555;font-size:0.94em;">Tell us your preferences and we'll narrow the field to the best hardware option for you.</p>
  <div class="finder-layout">
    <div class="finder-form">
      <h3 style="margin-top:0;">Your Requirements</h3>

      <div class="form-group">
        <label>Preferred OS / Platform</label>
        <div class="os-btn-group">
          <button class="os-btn" id="os-macos"   onclick="setOs('macos')">macOS</button>
          <button class="os-btn" id="os-windows" onclick="setOs('windows')">Windows</button>
          <button class="os-btn" id="os-linux"   onclick="setOs('linux')">Linux</button>
          <button class="os-btn active" id="os-any" onclick="setOs('any')">Any</button>
        </div>
      </div>

      <div class="form-group">
        <label>Form Factor</label>
        <select id="ff-select">
          <option value="any">Any (desktop or laptop)</option>
          <option value="desktop">Desktop only</option>
          <option value="laptop">Laptop / portable</option>
        </select>
      </div>

      <div class="form-group">
        <label>Budget <span class="budget-display" id="budgetLabel">$5,000</span></label>
        <input type="range" id="budgetSlider" min="500" max="35000" step="500" value="5000"
               oninput="document.getElementById('budgetLabel').textContent = '$' + Number(this.value).toLocaleString()">
        <div style="display:flex;justify-content:space-between;font-size:0.78em;color:#888;margin-top:2px;">
          <span>$500</span><span>$35,000</span>
        </div>
      </div>

      <div class="form-group">
        <label>Largest model I want to run</label>
        <select id="modelSize-select">
          <option value="7">7B – Small, fast (Mistral 7B, Gemma 7B)</option>
          <option value="14">14B – Medium (Qwen2.5 14B, Phi-4)</option>
          <option value="30" selected>30B – Large (Qwen 30B, Gemma 27B)</option>
          <option value="70">70B – XL (Llama 3 70B, Qwen 72B)</option>
          <option value="120">120B+ – Massive (Llama 3 405B, DeepSeek 671B)</option>
        </select>
      </div>

      <div class="form-group">
        <label>Priority</label>
        <select id="priority-select">
          <option value="speed">Speed (tokens/sec)</option>
          <option value="value" selected>Value (best for budget)</option>
          <option value="capacity">Capacity (fit largest model)</option>
          <option value="portable">Portability</option>
        </select>
      </div>

      <button class="find-btn" onclick="runFinder()">Find Best Match →</button>
    </div>

    <div class="result-area" id="finderResults">
      <p class="result-placeholder">&#8592; Set your requirements and click "Find Best Match"</p>
    </div>
  </div>
</div>

<!-- ═══════════════════════════════════════════════════════ COMPARE SYSTEMS TAB -->
<div id="compare" class="tab-content">
  <h3 style="margin-top:0">Compare Hardware for Local AI</h3>
  <p style="margin-top:-0.4em;color:#555;font-size:0.94em;margin-bottom:1.4em">
    Select up to three systems or chips. Every metric shows the absolute value plus a percentage difference relative to the first system you choose.
    RAM and VRAM capacity directly determine which model sizes fit; memory bandwidth is the primary driver of token generation speed.
  </p>
  <div class="compare-layout">
    <div class="compare-sidebar">
      <h3>Select Systems</h3>
      <div class="cmp-group">
        <label>System A (baseline)</label>
        <select id="cmpA">
          <optgroup label="Apple Silicon — Mac Studio">
            <option value="m4_ultra_base">Mac Studio M4 Ultra 24-core CPU / 60-core GPU / 192 GB</option>
            <option value="m4_ultra_high">Mac Studio M4 Ultra 24-core CPU / 76-core GPU / 192 GB</option>
            <option value="m4_ultra_xl">Mac Studio M4 Ultra 24-core CPU / 76-core GPU / 512 GB</option>
            <option value="m3_ultra_base" selected>Mac Studio M3 Ultra 24-core CPU / 60-core GPU / 192 GB</option>
            <option value="m3_ultra_high">Mac Studio M3 Ultra 24-core CPU / 76-core GPU / 192 GB</option>
            <option value="m3_ultra_xl">Mac Studio M3 Ultra 24-core CPU / 76-core GPU / 512 GB</option>
          </optgroup>
          <optgroup label="Apple Silicon — MacBook Pro">
            <option value="m4_max_14_32">MacBook Pro M4 Max 14-core CPU / 32-core GPU / 64 GB</option>
            <option value="m4_max_16_40">MacBook Pro M4 Max 16-core CPU / 40-core GPU / 128 GB</option>
            <option value="m3_max_14_30">MacBook Pro M3 Max 14-core CPU / 30-core GPU / 64 GB</option>
            <option value="m3_max_16_40">MacBook Pro M3 Max 16-core CPU / 40-core GPU / 128 GB</option>
            <option value="m3_pro_12_18">MacBook Pro M3 Pro 12-core CPU / 18-core GPU / 36 GB</option>
          </optgroup>
          <optgroup label="NVIDIA GPUs (discrete)">
            <option value="rtx5090">NVIDIA RTX 5090 (32 GB VRAM)</option>
            <option value="rtx4090">NVIDIA RTX 4090 (24 GB VRAM)</option>
            <option value="rtx3090">NVIDIA RTX 3090 (24 GB VRAM)</option>
            <option value="rtxa6000">NVIDIA RTX A6000 (48 GB VRAM)</option>
            <option value="a100_80">NVIDIA A100 80 GB</option>
          </optgroup>
          <optgroup label="AMD">
            <option value="rx7900xtx">AMD RX 7900 XTX (24 GB VRAM)</option>
            <option value="ryzen_ai_max">AMD Ryzen AI Max+ 395 (128 GB unified)</option>
          </optgroup>
        </select>
      </div>
      <div class="cmp-group">
        <label>System B</label>
        <select id="cmpB">
          <option value="none">— none —</option>
          <optgroup label="Apple Silicon — Mac Studio">
            <option value="m4_ultra_base">Mac Studio M4 Ultra 24-core CPU / 60-core GPU / 192 GB</option>
            <option value="m4_ultra_high">Mac Studio M4 Ultra 24-core CPU / 76-core GPU / 192 GB</option>
            <option value="m4_ultra_xl">Mac Studio M4 Ultra 24-core CPU / 76-core GPU / 512 GB</option>
            <option value="m3_ultra_base">Mac Studio M3 Ultra 24-core CPU / 60-core GPU / 192 GB</option>
            <option value="m3_ultra_high">Mac Studio M3 Ultra 24-core CPU / 76-core GPU / 192 GB</option>
            <option value="m3_ultra_xl">Mac Studio M3 Ultra 24-core CPU / 76-core GPU / 512 GB</option>
          </optgroup>
          <optgroup label="Apple Silicon — MacBook Pro">
            <option value="m4_max_14_32" selected>MacBook Pro M4 Max 16-core CPU / 40-core GPU / 128 GB</option>
            <option value="m4_max_16_40">MacBook Pro M4 Max 16-core CPU / 40-core GPU / 128 GB</option>
            <option value="m3_max_14_30">MacBook Pro M3 Max 14-core CPU / 30-core GPU / 64 GB</option>
            <option value="m3_max_16_40">MacBook Pro M3 Max 16-core CPU / 40-core GPU / 128 GB</option>
            <option value="m3_pro_12_18">MacBook Pro M3 Pro 12-core CPU / 18-core GPU / 36 GB</option>
          </optgroup>
          <optgroup label="NVIDIA GPUs (discrete)">
            <option value="rtx5090">NVIDIA RTX 5090 (32 GB VRAM)</option>
            <option value="rtx4090">NVIDIA RTX 4090 (24 GB VRAM)</option>
            <option value="rtx3090">NVIDIA RTX 3090 (24 GB VRAM)</option>
            <option value="rtxa6000">NVIDIA RTX A6000 (48 GB VRAM)</option>
            <option value="a100_80">NVIDIA A100 80 GB</option>
          </optgroup>
          <optgroup label="AMD">
            <option value="rx7900xtx">AMD RX 7900 XTX (24 GB VRAM)</option>
            <option value="ryzen_ai_max">AMD Ryzen AI Max+ 395 (128 GB unified)</option>
          </optgroup>
        </select>
      </div>
      <div class="cmp-group">
        <label>System C (optional)</label>
        <select id="cmpC">
          <option value="none" selected>— none —</option>
          <optgroup label="Apple Silicon — Mac Studio">
            <option value="m4_ultra_base">Mac Studio M4 Ultra 24-core CPU / 60-core GPU / 192 GB</option>
            <option value="m4_ultra_high">Mac Studio M4 Ultra 24-core CPU / 76-core GPU / 192 GB</option>
            <option value="m4_ultra_xl">Mac Studio M4 Ultra 24-core CPU / 76-core GPU / 512 GB</option>
            <option value="m3_ultra_base">Mac Studio M3 Ultra 24-core CPU / 60-core GPU / 192 GB</option>
            <option value="m3_ultra_high">Mac Studio M3 Ultra 24-core CPU / 76-core GPU / 192 GB</option>
            <option value="m3_ultra_xl">Mac Studio M3 Ultra 24-core CPU / 76-core GPU / 512 GB</option>
          </optgroup>
          <optgroup label="Apple Silicon — MacBook Pro">
            <option value="m4_max_14_32">MacBook Pro M4 Max 16-core CPU / 40-core GPU / 128 GB</option>
            <option value="m4_max_16_40">MacBook Pro M4 Max 16-core CPU / 40-core GPU / 128 GB</option>
            <option value="m3_max_14_30">MacBook Pro M3 Max 14-core CPU / 30-core GPU / 64 GB</option>
            <option value="m3_max_16_40">MacBook Pro M3 Max 16-core CPU / 40-core GPU / 128 GB</option>
            <option value="m3_pro_12_18">MacBook Pro M3 Pro 12-core CPU / 18-core GPU / 36 GB</option>
          </optgroup>
          <optgroup label="NVIDIA GPUs (discrete)">
            <option value="rtx5090">NVIDIA RTX 5090 (32 GB VRAM)</option>
            <option value="rtx4090">NVIDIA RTX 4090 (24 GB VRAM)</option>
            <option value="rtx3090">NVIDIA RTX 3090 (24 GB VRAM)</option>
            <option value="rtxa6000">NVIDIA RTX A6000 (48 GB VRAM)</option>
            <option value="a100_80">NVIDIA A100 80 GB</option>
          </optgroup>
          <optgroup label="AMD">
            <option value="rx7900xtx">AMD RX 7900 XTX (24 GB VRAM)</option>
            <option value="ryzen_ai_max">AMD Ryzen AI Max+ 395 (128 GB unified)</option>
          </optgroup>
        </select>
      </div>
      <button class="cmp-run-btn" onclick="runCompare()">Compare &rarr;</button>
      <p style="font-size:0.78em;color:#888;margin-top:0.8em;line-height:1.5">
        Token speed figures are real-world medians from llama.cpp benchmarks. RAM impact on model size follows Q4 quantisation rules (~1.1 GB per 1B params).
      </p>
    </div>

    <div class="compare-main" id="compareMain">
      <p class="cmp-placeholder">Select systems on the left and click Compare.</p>
    </div>
  </div>
</div>

<!-- ═══════════════════════════════════════════════════════ OPTIMIZATION GUIDE TAB -->
<div id="optimization" class="tab-content">
  <h3 style="margin-top:0">Optimization Guide — Getting Maximum Performance from Local LLMs</h3>
  <p class="opt-intro">
    Inference speed for locally self-hosted LLMs is almost entirely <strong>memory-bandwidth-bound</strong>, not compute-bound.
    Each token generated requires reading the full model weights from memory once. The faster your memory bus, the faster your tokens.
    GPU VRAM bandwidth (&gt;1,000 GB/s) dwarfs CPU DRAM bandwidth (30–100 GB/s), which is why a GPU can be 10–30&times; faster than CPU-only.
    Knowing which bottleneck you have lets you choose the right optimization.
  </p>

  <div class="opt-section">
    <h3>What Actually Limits Performance — by Hardware Path</h3>
    <table class="opt-table">
      <thead><tr>
        <th>Hardware Path</th><th>Primary Bottleneck</th><th>Secondary Bottleneck</th>
        <th>Max RAM Available</th><th>Typical 7B t/s</th><th>Typical 70B t/s</th>
      </tr></thead>
      <tbody>
        <tr>
          <td><strong>NVIDIA GPU (e.g. RTX 5090)</strong></td>
          <td>VRAM capacity (32 GB ceiling on consumer cards)</td>
          <td>VRAM bandwidth when model fits</td>
          <td>32 GB VRAM + system RAM for overflow</td>
          <td>~250</td><td>N/A (model too large for VRAM)</td>
        </tr>
        <tr>
          <td><strong>NVIDIA Pro GPU (e.g. A100 80GB)</strong></td>
          <td>VRAM bandwidth (2,000 GB/s)</td>
          <td>PCIe bandwidth for multi-GPU</td>
          <td>80 GB per card</td>
          <td>~260</td><td>~22 (full VRAM)</td>
        </tr>
        <tr>
          <td><strong>Apple Silicon (e.g. M3 Ultra)</strong></td>
          <td>Unified memory bandwidth (~800 GB/s)</td>
          <td>Metal compute shader efficiency</td>
          <td>192–512 GB (model &amp; OS share same pool)</td>
          <td>~120</td><td>~18–20</td>
        </tr>
        <tr>
          <td><strong>AMD RX 7900 XTX</strong></td>
          <td>VRAM capacity (24 GB ceiling)</td>
          <td>ROCm/HIP software overhead</td>
          <td>24 GB VRAM</td>
          <td>~135</td><td>N/A</td>
        </tr>
        <tr>
          <td><strong>AMD Ryzen AI Max+ 395</strong></td>
          <td>Unified memory bandwidth (~256 GB/s)</td>
          <td>ROCm vs. Vulkan backend selection</td>
          <td>128 GB unified</td>
          <td>~90</td><td>~6</td>
        </tr>
        <tr>
          <td><strong>CPU-only (e.g. i9-13900K, DDR5)</strong></td>
          <td>System RAM bandwidth (40–80 GB/s)</td>
          <td>Thread count, NUMA topology</td>
          <td>Addressable system RAM</td>
          <td>~15–25</td><td>~2–4</td>
        </tr>
      </tbody>
    </table>
    <p style="font-size:0.82em;color:#666;margin-top:0.5em">Token speeds are batch-size-1 (single-user) generation using Q4_K_M quantisation. Multi-user batch inference favours higher-compute GPUs more.</p>
  </div>

  <div class="opt-section">
    <h3>Quantisation — The Most Impactful Software Setting</h3>
    <p style="font-size:0.9em;margin-top:0;color:#555">Quantisation reduces model precision to fit more into memory and speed up loads. Choosing the right level is the single biggest lever available regardless of hardware.</p>
    <table class="opt-table">
      <thead><tr>
        <th>Format</th><th>Bits/Weight</th><th>RAM vs FP16</th><th>Quality vs FP16</th><th>Best Use</th><th>Speedup vs FP16</th>
      </tr></thead>
      <tbody>
        <tr>
          <td><strong>FP32</strong></td><td>32</td><td>2&times; more</td><td>Identical</td><td>Training only</td>
          <td><span class="speedup-tag speedup-low">0.5&times;</span></td>
        </tr>
        <tr>
          <td><strong>FP16 / BF16</strong></td><td>16</td><td>1&times; (baseline)</td><td>Near-lossless</td><td>High-VRAM GPU inference, fine-tuning</td>
          <td><span class="speedup-tag speedup-neutral">1&times;</span></td>
        </tr>
        <tr>
          <td><strong>Q8_0</strong></td><td>8</td><td>~0.5&times;</td><td>Effectively lossless (&lt;0.1% perplexity)</td><td>When you have VRAM to spare</td>
          <td><span class="speedup-tag speedup-medium">1.8&times;</span></td>
        </tr>
        <tr>
          <td><strong>Q6_K</strong></td><td>6</td><td>~0.37&times;</td><td>Minimal quality loss</td><td>Large models on capacity-limited systems</td>
          <td><span class="speedup-tag speedup-medium">2.2&times;</span></td>
        </tr>
        <tr>
          <td><strong>Q5_K_M</strong></td><td>5</td><td>~0.31&times;</td><td>Slight loss, noticeable only on coding</td><td>Best quality when RAM is just enough</td>
          <td><span class="speedup-tag speedup-high">2.8&times;</span></td>
        </tr>
        <tr style="background:#fffdf0;">
          <td><strong>Q4_K_M</strong></td><td>4</td><td>~0.26&times;</td><td>Good — recommended default</td><td>
            <strong>Default recommendation for all hardware</strong></td>
          <td><span class="speedup-tag speedup-high">3.2&times;</span></td>
        </tr>
        <tr>
          <td><strong>Q3_K_M</strong></td><td>3</td><td>~0.19&times;</td><td>Noticeable degradation</td><td>Only if Q4 doesn't fit in RAM</td>
          <td><span class="speedup-tag speedup-high">3.8&times;</span></td>
        </tr>
        <tr>
          <td><strong>Q2_K</strong></td><td>2</td><td>~0.13&times;</td><td>Significant quality loss — avoid</td><td>Emergency fallback</td>
          <td><span class="speedup-tag speedup-high">4.5&times;</span></td>
        </tr>
        <tr>
          <td><strong>IQ4_XS (imatrix)</strong></td><td>~4.25</td><td>~0.27&times;</td><td>Better than Q4_K_M at same size</td><td>Best Q4-tier quality</td>
          <td><span class="speedup-tag speedup-high">3.0&times;</span></td>
        </tr>
      </tbody>
    </table>
    <p style="font-size:0.82em;color:#666;margin-top:0.5em">
      <strong>Rule of thumb:</strong> Q4_K_M uses approximately 1.1 GB per 1B parameters.
      A 70B model &#8776; 40 GB Q4; a 13B model &#8776; 8 GB Q4; a 7B model &#8776; 4.4 GB Q4.
      Importance-matrix (imatrix) quants (IQ series) give better quality at the same bit depth.
    </p>
  </div>

  <div class="opt-section">
    <h3>Platform-Specific Optimization Paths</h3>
    <div class="opt-cards">

      <div class="opt-card">
        <div class="opt-card-header opt-nvidia">
          NVIDIA — CUDA Ecosystem
        </div>
        <div class="opt-card-body">
          <ul>
            <li><strong>CUDA + cuBLAS (default):</strong> All major frameworks use this. Best throughput for models that fully fit in VRAM. Zero additional configuration in Ollama or LM Studio.</li>
            <li><strong>TensorRT-LLM:</strong> NVIDIA's production inference engine. Requires model compilation; delivers up to 2&times; the throughput of vanilla llama.cpp for continuous batching workloads.</li>
            <li><strong>FlashAttention-2:</strong> Reduces attention memory complexity from O(n&sup2;) to O(n). Enabled by default in vLLM and llama.cpp — provides 1.5–2&times; speedup on long contexts.</li>
            <li><strong>Multi-GPU NVLink:</strong> Pro cards (A100, H100, RTX 6000 Ada) support NVLink for full bandwidth pooling. Consumer cards do not — PCIE bandwidth becomes the bottleneck with 2&times; GPUs.</li>
            <li><strong>GPU offload layers (<code>-ngl</code>):</strong> In llama.cpp, set <code>-ngl 99</code> to send all transformer layers to GPU. If VRAM is too small, reduce layers incrementally; each layer in VRAM improves speed.</li>
            <li><strong>FP8 quantisation (Hopper GPUs):</strong> H100/H200 support FP8 native precision — 2&times; throughput over BF16 with &lt;1% quality loss. Not available on consumer RTX cards.</li>
          </ul>
        </div>
      </div>

      <div class="opt-card">
        <div class="opt-card-header opt-apple">
          Apple Silicon — Metal / MLX
        </div>
        <div class="opt-card-body">
          <ul>
            <li><strong>MLX framework (Apple-native):</strong> 20–30% faster than llama.cpp with Ollama for inference on Apple Silicon. Install via <code>pip install mlx-lm</code>. Best choice for Apple hardware in 2025.</li>
            <li><strong>Metal Performance Shaders (MPS):</strong> Used by PyTorch on Apple Silicon. Not as optimised as MLX for LLM inference but works for fine-tuning and training experiments.</li>
            <li><strong>Speculative decoding (MLX):</strong> Use a small draft model (e.g. 1B) to propose tokens, verified by the main model. Can nearly double tokens/sec on large models with no quality change.</li>
            <li><strong>llama.cpp Metal backend:</strong> Used by Ollama. Slower than native MLX but has broader model format support. Set <code>LLAMA_METAL=1</code> at compile time.</li>
            <li><strong>Unified memory — use it all:</strong> Unlike a GPU, macOS can dynamically share RAM between GPU and CPU. Run models that are 70–80% of total RAM; leave ~20% for macOS and applications.</li>
            <li><strong>Context length and memory:</strong> Increasing context window (e.g. 8K → 128K) consumes significant KV-cache RAM. At 128K context a 70B model can use &gt;60 GB extra RAM. Set <code>--ctx-size</code> conservatively.</li>
          </ul>
        </div>
      </div>

      <div class="opt-card">
        <div class="opt-card-header opt-amd">
          AMD — ROCm / Vulkan / HIP
        </div>
        <div class="opt-card-body">
          <ul>
            <li><strong>ROCm (HIP):</strong> AMD's CUDA-equivalent. Now supported in llama.cpp, vLLM, and PyTorch. Requires ROCm 6.0+ on compatible RX 6000/7000 series GPUs. Performance is within ~10% of CUDA on inference.</li>
            <li><strong>Vulkan backend (llama.cpp):</strong> Cross-platform GPU acceleration using Vulkan compute shaders. Often 10–20% faster than ROCm on discrete GPUs for pure inference. Use <code>-DGGML_VULKAN=ON</code> at build time.</li>
            <li><strong>Ryzen AI Max+ 395 — BIOS RAM allocation:</strong> By default, the NPU/GPU shares 32 GB. Open BIOS, set iGPU VRAM allocation to 96 or 128 GB for full performance. Without this, large models page to slow system RAM.</li>
            <li><strong>hipBLAS (ROCm BLAS):</strong> Replaces cuBLAS for AMD systems. Install via ROCm SDK and set <code>HIP_VISIBLE_DEVICES=0</code>. Used automatically by torch and llama.cpp ROCm builds.</li>
            <li><strong>AMDGPU target:</strong> Set <code>HSA_OVERRIDE_GFX_VERSION=11.0.0</code> for RDNA3 cards (RX 7000 series) if ROCm doesn't detect the GPU correctly — common issue on consumer ARC/RDNA setups.</li>
            <li><strong>Concurrent requests:</strong> AMD's ROCm handles batching less efficiently than CUDA for high concurrency. For single-user inference, the gap is small (&lt;10%). For serving multiple users, NVIDIA holds a clear advantage.</li>
          </ul>
        </div>
      </div>

      <div class="opt-card">
        <div class="opt-card-header opt-general">
          CPU-Only Inference (x86 / ARM)
        </div>
        <div class="opt-card-body">
          <ul>
            <li><strong>AVX-512 (Intel Icelake+, AMD Zen4):</strong> llama.cpp auto-detects and uses AVX-512 VNNI instructions for 4-bit matrix multiply. ~1.4&times; faster than AVX2. Use a compiled binary or set <code>CMAKE_CXX_FLAGS=-march=native</code>.</li>
            <li><strong>Thread count:</strong> Set threads to equal physical cores (not hyperthreads). Hyperthreading adds latency on memory-bound tasks. E.g. for a 16P + 8E core i9-13900K, use <code>-t 16</code> (P-cores only).</li>
            <li><strong>NUMA pinning:</strong> On multi-socket systems, pin to one NUMA domain. Cross-socket memory access halves effective bandwidth. Use <code>numactl --cpunodebind=0 --membind=0</code>.</li>
            <li><strong>Dual-channel / quad-channel RAM:</strong> Each additional memory channel doubles bandwidth. DDR5-6000 quad-channel gives ~190 GB/s vs ~48 GB/s for DDR4 single-channel — a 4&times; inference speedup.</li>
            <li><strong>mmap and mlock:</strong> llama.cpp by default mmaps model files. Use <code>--mlock</code> to pin the model in RAM and prevent page faults during inference. Requires sufficient free RAM and root/CAP_IPC_LOCK.</li>
          </ul>
        </div>
      </div>

      <div class="opt-card">
        <div class="opt-card-header opt-general">
          Inference Framework Selection
        </div>
        <div class="opt-card-body">
          <ul>
            <li><strong>Ollama:</strong> Easiest setup; wraps llama.cpp with automatic hardware detection (CUDA, Metal, ROCm). Good for development and single-user. Not optimised for high-throughput multi-user serving.</li>
            <li><strong>LM Studio:</strong> GUI-based; good for non-technical users. Uses llama.cpp backend. GPU offload via "GPU Layers" slider. Match layers to VRAM — rule of thumb: each 7B layer ~= 100 MB VRAM.</li>
            <li><strong>llama.cpp (direct):</strong> Maximum control. Supports every acceleration path. Use <code>-ngl 99 -t 8 --ctx-size 4096 --mlock</code> as a good starting configuration.</li>
            <li><strong>vLLM:</strong> Best for production multi-user serving on NVIDIA. PagedAttention enables efficient KV-cache sharing. Not supported on Apple Silicon. Requires CUDA 11.8+.</li>
            <li><strong>ExLlamaV2:</strong> Fastest CUDA inference for quantised models. Uses EXL2 format (superior to GGUF on NVIDIA). Can exceed TensorRT-LLM speed for chat workloads at Q4–Q5.</li>
          </ul>
        </div>
      </div>

      <div class="opt-card">
        <div class="opt-card-header opt-general">
          Context Length and KV-Cache
        </div>
        <div class="opt-card-body">
          <ul>
            <li><strong>KV-cache size scales quadratically:</strong> A 7B model at 4K context uses ~500 MB KV-cache; at 128K context it uses ~16 GB. Reduce <code>--ctx-size</code> to free memory for the model itself.</li>
            <li><strong>KV quantisation (<code>--cache-type-k q8_0</code>):</strong> Quantising the KV cache from FP16 to Q8 or Q4 halves its memory footprint with minor quality impact. Supported in llama.cpp ≥ b2300.</li>
            <li><strong>Grouped-query attention (GQA) models:</strong> Llama 3, Mistral, and Qwen2+ use GQA — their KV cache is inherently smaller than older models. Prefer GQA models when context length matters.</li>
            <li><strong>Flash Attention 2:</strong> Required for efficient long-context inference. Enabled automatically in vLLM and supported in llama.cpp with <code>--flash-attn</code>.</li>
          </ul>
        </div>
      </div>

    </div>
  </div>

  <div class="opt-section">
    <h3>RAM / VRAM Impact on Model Access — Reference Table</h3>
    <p style="font-size:0.9em;color:#555;margin-top:0">
      This table shows what changes when you increase or reduce available RAM/VRAM. The key insight: 
      <strong>more memory expands which models fit; more bandwidth increases how fast they run.</strong>
      Adding RAM without adding bandwidth does not improve token speed — it only enables larger models.
    </p>
    <table class="opt-table">
      <thead><tr>
        <th>Available RAM / VRAM</th>
        <th>Largest model (Q4_K_M)</th>
        <th>Token speed change vs. prior tier</th>
        <th>70B Q4 accessible?</th>
        <th>Notes</th>
      </tr></thead>
      <tbody>
        <tr><td>4 GB</td><td>~3B Q4</td><td>—</td><td class="impact-negative">No</td><td>OS overhead leaves ~2.5 GB usable. 3B models run well.</td></tr>
        <tr><td>8 GB</td><td>~7B Q4</td><td class="impact-positive">+0% speed (same BW), +model tier</td><td class="impact-negative">No</td><td>Most popular tier. 7B Q4 = ~4.4 GB. Small context headroom.</td></tr>
        <tr><td>16 GB</td><td>~13B Q4</td><td class="impact-positive">+0% speed, +model tier</td><td class="impact-negative">No</td><td>13B Q4 ≈ 7.9 GB. 8K context adds ~1 GB. Comfortable headroom.</td></tr>
        <tr><td>24 GB</td><td>~20B Q4</td><td class="impact-positive">+0% speed, +model tier</td><td class="impact-negative">No</td><td>Maximum for consumer discrete GPUs (NVIDIA RTX 4090, AMD RX 7900 XTX). 20B Q4 ≈ 12 GB.</td></tr>
        <tr><td>32 GB</td><td>~26B Q4, 70B Q2</td><td class="impact-positive">+0% speed, marginal 70B Q2</td><td style="color:#856404;font-weight:600">Marginal (Q2 only)</td><td>RTX 5090 ceiling. Q2 70B runs but quality suffers. Strong for 13–26B models.</td></tr>
        <tr><td>48 GB</td><td>~40B Q4, 70B Q3</td><td class="impact-positive">+0% speed, +model tier</td><td style="color:#856404;font-weight:600">Q3 only (~3.8 t/s)</td><td>RTX A6000. 70B Q3 just fits; expect lower quality than Q4.</td></tr>
        <tr style="background:#fffdf0;"><td><strong>64 GB+</strong></td><td><strong>70B Q4 (full)</strong></td><td class="impact-positive"><strong>+0% speed, full 70B access</strong></td><td class="impact-positive"><strong>Yes — Q4_K_M</strong></td><td>First tier enabling 70B Q4 fully. Apple M-series, Ryzen AI Max. 70B Q4 ≈ 40 GB; leaves 20+ GB for context.</td></tr>
        <tr><td>96 GB</td><td>~80B Q4, 120B Q3</td><td class="impact-positive">+0% speed, 120B tier</td><td class="impact-positive">Yes — with headroom</td><td>Dual 3090/A6000, M2 Ultra 96 GB. Llama 3.1 405B in Q2 just fits.</td></tr>
        <tr><td>128 GB</td><td>~105B Q4, 120B Q3</td><td class="impact-positive">+0% speed (CPU BW limited)</td><td class="impact-positive">Yes — comfortably</td><td>Ryzen AI Max+ 395, M-series 128 GB. 70B + 32K context fits easily.</td></tr>
        <tr><td>192 GB</td><td>~160B Q4, 200B+ Q3</td><td class="impact-positive">+0% speed, 200B tier (Apple BW)</td><td class="impact-positive">Yes — fast (~18 t/s)</td><td>M3 Ultra 192 GB. Llama 3.1 405B Q2 ≈ 202 GB — just barely fits.</td></tr>
        <tr><td>512 GB</td><td>405B Q4 full</td><td class="impact-positive">+0% speed, 400B tier</td><td class="impact-positive">Yes</td><td>M3 Ultra 512 GB. 405B Q4 ≈ 230 GB. Full unquantised 70B fits.</td></tr>
      </tbody>
    </table>
  </div>

  <div class="opt-section">
    <h3>Key Takeaways by Use Case</h3>
    <table class="opt-table">
      <thead><tr><th>Goal</th><th>Recommended Path</th><th>Key Setting</th></tr></thead>
      <tbody>
        <tr><td>Fastest possible 7B inference</td><td>NVIDIA RTX 5090 + ExLlamaV2</td><td>EXL2 Q4 format, enable Flash Attention</td></tr>
        <tr><td>Run 70B on a budget</td><td>AMD Ryzen AI Max+ 395 mini PC (~$1,500)</td><td>Set BIOS iGPU VRAM to 128 GB, use Vulkan backend</td></tr>
        <tr><td>Quiet, large-model desktop</td><td>Apple Mac Studio M3 Ultra 192 GB + MLX</td><td>Use <code>mlx_lm.generate</code>, enable speculative decoding</td></tr>
        <tr><td>Multi-user production serving</td><td>NVIDIA A100/H100 + vLLM</td><td>Enable PagedAttention, continuous batching, tensor parallelism</td></tr>
        <tr><td>Maximum context length (&gt;64K tokens)</td><td>Apple M3 Ultra 192 GB or 512 GB</td><td>Set <code>--ctx-size 65536</code>, quantise KV cache to Q8</td></tr>
        <tr><td>Fine-tuning / LoRA training</td><td>NVIDIA GPU with 24+ GB VRAM</td><td>Use QLoRA (4-bit base + FP16 adapters), gradient checkpointing</td></tr>
        <tr><td>Best CPU-only on x86</td><td>Dual-channel DDR5 + Zen4/Raptor Lake</td><td>Compile llama.cpp with <code>-march=native</code>, use <code>-t &lt;physical cores&gt;</code></td></tr>
      </tbody>
    </table>
  </div>
</div>

<!-- ═══════════════════════════════════════════════════════════
     OPERATING SYSTEMS TAB
═══════════════════════════════════════════════════════════ -->
<div id="opsystems" class="tab-content">
  <h2>Operating Systems for Local AI Development</h2>
  <p style="max-width:820px;margin-bottom:1.5em">Your operating system determines which GPU acceleration paths, quantization formats, and developer tools are available. This page compares Windows, macOS, and Linux across the dimensions that matter most for running, developing, and fine-tuning local AI models.</p>

  <div class="opt-section">
    <h3>Feature Compatibility Matrix</h3>
    <table class="opt-table">
      <thead><tr>
        <th style="min-width:230px">Feature / Tool</th>
        <th>Windows 11</th>
        <th>macOS (Apple Silicon)</th>
        <th>Linux (Ubuntu 22.04+)</th>
      </tr></thead>
      <tbody>
        <tr><td><strong>NVIDIA CUDA</strong></td>
          <td><span class="speedup-tag speedup-high">Full</span></td>
          <td><span class="speedup-tag speedup-low">Not available</span></td>
          <td><span class="speedup-tag speedup-high">Full</span> &mdash; best driver control</td></tr>
        <tr><td><strong>AMD ROCm (discrete GPU)</strong></td>
          <td><span class="speedup-tag speedup-low">Not available</span> &mdash; ROCm for discrete is Linux-only</td>
          <td><span class="speedup-tag speedup-low">Not available</span></td>
          <td><span class="speedup-tag speedup-high">Full</span> &mdash; ROCm 6.0+ on RX 6000/7000</td></tr>
        <tr><td><strong>Apple MLX / Metal</strong></td>
          <td><span class="speedup-tag speedup-low">Not available</span></td>
          <td><span class="speedup-tag speedup-high">Native</span> &mdash; best inference path</td>
          <td><span class="speedup-tag speedup-low">Not available</span></td></tr>
        <tr><td><strong>DirectML (Microsoft)</strong></td>
          <td><span class="speedup-tag speedup-high">Native</span> &mdash; ONNX on NVIDIA, AMD, Intel GPU</td>
          <td><span class="speedup-tag speedup-low">Not available</span></td>
          <td><span class="speedup-tag speedup-low">Not available</span></td></tr>
        <tr><td><strong>Vulkan inference (llama.cpp)</strong></td>
          <td><span class="speedup-tag speedup-high">Full</span></td>
          <td><span class="speedup-tag speedup-low">Not available</span></td>
          <td><span class="speedup-tag speedup-high">Full</span></td></tr>
        <tr><td><strong>Ollama</strong></td>
          <td><span class="speedup-tag speedup-high">Full</span></td>
          <td><span class="speedup-tag speedup-high">Full</span></td>
          <td><span class="speedup-tag speedup-high">Full</span></td></tr>
        <tr><td><strong>LM Studio</strong></td>
          <td><span class="speedup-tag speedup-high">Full</span></td>
          <td><span class="speedup-tag speedup-high">Full</span></td>
          <td><span class="speedup-tag speedup-high">Full</span></td></tr>
        <tr><td><strong>vLLM (production serving)</strong></td>
          <td><span class="speedup-tag speedup-medium">Via WSL2 only</span></td>
          <td><span class="speedup-tag speedup-low">Not supported</span></td>
          <td><span class="speedup-tag speedup-high">Full</span> &mdash; native, best performance</td></tr>
        <tr><td><strong>ExLlamaV2</strong></td>
          <td><span class="speedup-tag speedup-high">Full</span> (CUDA)</td>
          <td><span class="speedup-tag speedup-low">Not available</span></td>
          <td><span class="speedup-tag speedup-high">Full</span> (CUDA)</td></tr>
        <tr><td><strong>Claude Code</strong></td>
          <td><span class="speedup-tag speedup-medium">WSL2 required</span></td>
          <td><span class="speedup-tag speedup-high">Native</span> &mdash; recommended</td>
          <td><span class="speedup-tag speedup-high">Native</span> &mdash; recommended</td></tr>
        <tr><td><strong>Aider coding agent</strong></td>
          <td><span class="speedup-tag speedup-medium">Works; WSL2 preferred</span></td>
          <td><span class="speedup-tag speedup-high">Full</span></td>
          <td><span class="speedup-tag speedup-high">Full</span></td></tr>
        <tr><td><strong>Docker + GPU passthrough</strong></td>
          <td><span class="speedup-tag speedup-medium">Via Docker Desktop + WSL2</span></td>
          <td><span class="speedup-tag speedup-low">No GPU passthrough to containers</span></td>
          <td><span class="speedup-tag speedup-high">Full</span> &mdash; NVIDIA Container Toolkit</td></tr>
        <tr><td><strong>Axolotl / Unsloth fine-tuning</strong></td>
          <td><span class="speedup-tag speedup-medium">Via WSL2 + CUDA</span></td>
          <td><span class="speedup-tag speedup-low">Not supported</span></td>
          <td><span class="speedup-tag speedup-high">Full</span> &mdash; recommended platform</td></tr>
        <tr><td><strong>GGUF quantization</strong></td>
          <td><span class="speedup-tag speedup-high">Full</span></td>
          <td><span class="speedup-tag speedup-high">Full</span></td>
          <td><span class="speedup-tag speedup-high">Full</span></td></tr>
        <tr><td><strong>EXL2 format</strong></td>
          <td><span class="speedup-tag speedup-high">Full</span> (CUDA)</td>
          <td><span class="speedup-tag speedup-low">Not available</span></td>
          <td><span class="speedup-tag speedup-high">Full</span> (CUDA)</td></tr>
        <tr><td><strong>GPTQ / AWQ formats</strong></td>
          <td><span class="speedup-tag speedup-high">Full</span> (CUDA)</td>
          <td><span class="speedup-tag speedup-low">Not available</span></td>
          <td><span class="speedup-tag speedup-high">Full</span> (CUDA)</td></tr>
        <tr><td><strong>MLX native format</strong></td>
          <td><span class="speedup-tag speedup-low">Not available</span></td>
          <td><span class="speedup-tag speedup-high">Native</span> &mdash; Apple Silicon only</td>
          <td><span class="speedup-tag speedup-low">Not available</span></td></tr>
        <tr><td><strong>BitsAndBytes / QLoRA</strong></td>
          <td><span class="speedup-tag speedup-medium">Via WSL2 (unreliable natively)</span></td>
          <td><span class="speedup-tag speedup-low">Not supported</span></td>
          <td><span class="speedup-tag speedup-high">Full</span> &mdash; recommended for QLoRA</td></tr>
        <tr><td><strong>Hugging Face transformers</strong></td>
          <td><span class="speedup-tag speedup-medium">Works; WSL2 for training</span></td>
          <td><span class="speedup-tag speedup-medium">Inference via MPS; no CUDA</span></td>
          <td><span class="speedup-tag speedup-high">Full</span></td></tr>
        <tr><td><strong>GPU driver management</strong></td>
          <td><span class="speedup-tag speedup-high">Easy</span> &mdash; NVIDIA App or AMD Adrenaline</td>
          <td><span class="speedup-tag speedup-high">Automatic</span> &mdash; macOS system updates</td>
          <td><span class="speedup-tag speedup-medium">Manual</span> &mdash; DKMS, kernel headers</td></tr>
      </tbody>
    </table>
  </div>

  <div class="opt-section">
    <h3>Platform Detail</h3>
    <div class="opt-cards">

      <div class="opt-card">
        <div class="opt-card-header os-header-windows">Windows 11</div>
        <div class="opt-card-body">
          <p style="font-size:0.88em;margin-top:0"><strong>Best for:</strong> NVIDIA CUDA users wanting a dual-purpose gaming and AI workstation. Widest consumer GPU support, easiest driver management, and the best GUI tool ecosystem.</p>
          <ul>
            <li><strong>CUDA works natively.</strong> Ollama, LM Studio, ExLlamaV2, and llama.cpp all run without WSL2 on NVIDIA GPUs. CUDA driver installation is handled automatically by the NVIDIA App.</li>
            <li><strong>WSL2 is near-mandatory for serious Python-based AI work.</strong> Tools like vLLM, Axolotl, Unsloth, and BitsAndBytes are Linux-first. Install WSL2 (Ubuntu 22.04 LTS) and do Python AI development inside it. NVIDIA GPU passthrough to WSL2 delivers near-native CUDA performance with minimal overhead.</li>
            <li><strong>Claude Code requires WSL2.</strong> It is not natively supported on PowerShell or Command Prompt. After enabling WSL2, install Node.js inside Ubuntu and run <code>npm install -g @anthropic-ai/claude-code</code>. This one step also resolves most Python ML compatibility issues.</li>
            <li><strong>AMD discrete GPU ROCm is not available on Windows.</strong> AMD users should use the Vulkan backend in llama.cpp/Ollama (which works well for inference) as an alternative. For PyTorch training with AMD discrete GPUs, Linux is required.</li>
            <li><strong>DirectML</strong> accelerates ONNX Runtime models (Phi-3, Mistral, Llama via ONNX) on any Windows GPU including Intel Arc. Slower than CUDA but hardware-agnostic &mdash; useful when CUDA is not an option.</li>
            <li><strong>Recommended setup:</strong> NVIDIA drivers natively &rarr; WSL2 + Ubuntu 22.04 &rarr; Ollama and LM Studio on native Windows for daily chat &rarr; Claude Code and Python tools (vLLM, transformers, axolotl) inside WSL2.</li>
          </ul>
        </div>
      </div>

      <div class="opt-card">
        <div class="opt-card-header os-header-mac">macOS (Apple Silicon)</div>
        <div class="opt-card-body">
          <p style="font-size:0.88em;margin-top:0"><strong>Best for:</strong> Developers who want a polished Unix environment, large unified-memory model capacity, low power draw, and the best Claude Code experience without Linux complexity.</p>
          <ul>
            <li><strong>MLX is the optimal inference path.</strong> Apple's open-source MLX framework is 20&ndash;30% faster than llama.cpp Metal for inference on Apple Silicon. Install via <code>pip install mlx-lm</code>, then run models directly with <code>mlx_lm.generate</code>. Ollama uses Metal/llama.cpp internally and is slightly slower but simpler to set up.</li>
            <li><strong>Unified memory is the key architectural advantage.</strong> CPU and GPU share one high-bandwidth pool &mdash; up to 512 GB on Mac Studio M3/M4 Ultra. A 70B model (40 GB Q4) loads without any VRAM/host memory boundary or PCIe transfer overhead.</li>
            <li><strong>Claude Code works natively.</strong> macOS is a first-class Claude Code platform. zsh, Homebrew, and all standard Unix utilities work out of the box. No WSL2 or compatibility layer needed.</li>
            <li><strong>No CUDA, EXL2, GPTQ/AWQ, vLLM, or BitsAndBytes.</strong> Axolotl and Unsloth for fine-tuning are not supported. MLX includes its own LoRA fine-tuning path (<code>mlx_lm.lora</code>) for Apple Silicon.</li>
            <li><strong>Supported quantization formats:</strong> GGUF (all variants via Ollama/llama.cpp) and MLX native formats (int4, int8, fp16, bf16). EXL2, GPTQ, AWQ, and BitsAndBytes are not available.</li>
            <li><strong>RAM is soldered and non-upgradeable.</strong> Choose your memory configuration at purchase. The 128 GB step (M4 Max MacBook Pro or Mac Studio) is the threshold for 70B Q4 models; 192&ndash;512 GB (Mac Studio M3/M4 Ultra) enables 120B+ models.</li>
            <li><strong>Power efficiency.</strong> An M3 Ultra running 70B at 18 t/s draws ~150 W. An equivalent-speed NVIDIA A100 draws 300&ndash;400 W. For always-on home servers this is a meaningful difference over time.</li>
          </ul>
        </div>
      </div>

      <div class="opt-card">
        <div class="opt-card-header os-header-linux">Linux (Ubuntu 22.04 / 24.04)</div>
        <div class="opt-card-body">
          <p style="font-size:0.88em;margin-top:0"><strong>Best for:</strong> Fine-tuning, production multi-user serving, AMD discrete GPU acceleration, multi-GPU setups, and developers who need the full toolchain without compatibility workarounds.</p>
          <ul>
            <li><strong>The reference platform for all AI/ML work.</strong> vLLM, Axolotl, Unsloth, TensorRT-LLM, and BitsAndBytes are developed and tested on Linux first. Some capabilities are Linux-only or substantially faster on Linux.</li>
            <li><strong>ROCm works properly only on Linux.</strong> AMD ROCm 6.0+ enables full GPU acceleration for llama.cpp, vLLM, and PyTorch on RX 6000/7000 series GPUs. On Windows, ROCm support for discrete GPUs is not available &mdash; use Vulkan there instead.</li>
            <li><strong>Claude Code and Aider are native.</strong> The full agentic experience &mdash; file editing, bash commands, multi-file reasoning, git integration &mdash; works without any wrapper or workaround.</li>
            <li><strong>Fine-tuning requires Linux.</strong> Axolotl (most widely used LoRA/QLoRA pipeline), Unsloth (2&times; faster LoRA with less VRAM), and BitsAndBytes (required for QLoRA) all require Linux + CUDA. Attempting native Windows fine-tuning consistently produces environment and CUDA library compatibility errors.</li>
            <li><strong>Docker + NVIDIA Container Toolkit</strong> enables fully reproducible GPU-accelerated environments. Deploy vLLM in one command: <code>docker run --gpus all vllm/vllm-openai:latest --model meta-llama/Llama-3.1-70B-Instruct</code> &mdash; the standard production pattern.</li>
            <li><strong>Driver management is the main friction point.</strong> NVIDIA drivers require matching kernel headers and DKMS modules. Before upgrading the kernel, confirm the driver package is compatible. Use <code>apt-mark hold linux-image-generic</code> to pin a stable kernel once the driver is working.</li>
            <li><strong>Full quantization access:</strong> GGUF, EXL2, GPTQ, AWQ, BitsAndBytes (4-bit/8-bit), and all llama.cpp-supported formats. Linux + CUDA gives access to every inference and training optimization path simultaneously.</li>
          </ul>
        </div>
      </div>

    </div>
  </div>

  <div class="opt-section">
    <h3>Recommended Setup by Goal</h3>
    <table class="opt-table">
      <thead><tr><th>Goal</th><th>Recommended OS</th><th>Reasoning</th></tr></thead>
      <tbody>
        <tr><td>Run local chat models (beginner)</td><td>Windows or macOS</td><td>Ollama and LM Studio work on all platforms. Windows has the widest consumer GPU support; macOS has the simplest environment setup.</td></tr>
        <tr><td>Fine-tune models (LoRA / QLoRA)</td><td>Linux (Ubuntu 22.04)</td><td>Axolotl, Unsloth, and BitsAndBytes require Linux + CUDA. Most tutorials, Docker images, and Hugging Face guides assume Ubuntu.</td></tr>
        <tr><td>Production multi-user model serving</td><td>Linux</td><td>vLLM with PagedAttention, Docker + NVIDIA Container Toolkit, and production monitoring (Prometheus, Grafana) are all Linux-native.</td></tr>
        <tr><td>Daily Claude Code use</td><td>macOS or Linux</td><td>Native first-class support on both. On Windows, WSL2 setup is required and file I/O across the WSL2/Windows boundary is slower.</td></tr>
        <tr><td>AMD discrete GPU inference</td><td>Linux</td><td>ROCm is Linux-only for discrete GPUs. On Windows, use Vulkan in llama.cpp/Ollama &mdash; functional but ~10&ndash;15% slower than ROCm on Linux for the same hardware.</td></tr>
        <tr><td>Run 70B+ parameter models</td><td>macOS (Apple Silicon) or Linux (multi-GPU)</td><td>Apple unified memory and NVIDIA NVLink multi-GPU are the two viable paths to 70B+ at usable speed without an extreme budget.</td></tr>
        <tr><td>Gaming + AI on one machine</td><td>Windows</td><td>Windows remains the primary gaming platform. NVIDIA CUDA AI tools work natively on Windows alongside games without issue.</td></tr>
        <tr><td>Power-efficient always-on server</td><td>macOS (Apple Silicon)</td><td>Apple Silicon offers the best performance-per-watt ratio for LLM inference. A Mac mini M4 Pro can idle at under 10 W while ready to serve requests.</td></tr>
        <tr><td>Maximum quantization format access</td><td>Linux</td><td>Linux + CUDA is the only platform where GGUF, EXL2, GPTQ, AWQ, BitsAndBytes, and ONNX with CUDA EP are all simultaneously available.</td></tr>
      </tbody>
    </table>
  </div>
</div>

<!-- ═══════════════════════════════════════════════════════════
     SOFTWARE TAB
═══════════════════════════════════════════════════════════ -->
<div id="software" class="tab-content">
  <h2>Software Guide &mdash; Inference, Coding Assistants, Fine-tuning &amp; Cloud Tools</h2>
  <p style="max-width:820px;margin-bottom:1.5em">A structured catalog of the tools available across each stage of AI work: running local models, developing with AI assistance, fine-tuning models, and accessing cloud AI services. All tools listed are open source or have a meaningful free tier unless noted.</p>

  <!-- LOCAL INFERENCE ENGINES -->
  <div class="opt-section">
    <h3>Local Inference Engines</h3>
    <p style="font-size:0.88em;color:#555;margin-top:0">These tools load model files from disk and run them on your CPU or GPU. They are the foundation of local AI use.</p>
    <div class="opt-cards">

      <div class="opt-card">
        <div class="opt-card-header opt-general">Ollama</div>
        <div class="opt-card-body">
          <div class="sw-tool-desc">The easiest way to run local models. Wraps llama.cpp with automatic GPU detection and exposes a REST API on localhost:11434 compatible with the OpenAI API spec.</div>
          <span class="sw-tag sw-free">Open source</span>
          <span class="sw-tag sw-all">Windows / macOS / Linux</span>
          <ul>
            <li><strong>Model library:</strong> <code>ollama pull llama3.1:70b</code> downloads, quantizes, and runs in one command. 200+ curated models available.</li>
            <li><strong>GPU:</strong> Auto-detects CUDA (NVIDIA), Metal (Apple), and ROCm/Vulkan (AMD). No configuration needed for most setups.</li>
            <li><strong>API compatibility:</strong> Exposes <code>/v1/chat/completions</code> endpoint, so any OpenAI-compatible client (Open WebUI, Continue.dev, Cursor) connects immediately.</li>
            <li><strong>Limitation:</strong> Not optimised for high-throughput multi-user serving. Single-user development and local apps are the sweet spot.</li>
          </ul>
        </div>
      </div>

      <div class="opt-card">
        <div class="opt-card-header opt-general">LM Studio</div>
        <div class="opt-card-body">
          <div class="sw-tool-desc">GUI application for discovering, downloading, and running GGUF models. Built-in model browser, GPU layer slider, and a local server with OpenAI-compatible API.</div>
          <span class="sw-tag sw-free">Free (closed source)</span>
          <span class="sw-tag sw-all">Windows / macOS / Linux</span>
          <ul>
            <li><strong>Best for beginners:</strong> No command line required. Browse models from Hugging Face directly in the app, download with one click, adjust GPU layers via slider.</li>
            <li><strong>GPU offload:</strong> Drag the "GPU Layers" slider to send as many transformer layers as fit into VRAM. Rule of thumb: each 7B layer &asymp; 100 MB VRAM.</li>
            <li><strong>Local server mode:</strong> Enables a persistent server that other apps (Open WebUI, Continue.dev) can connect to.</li>
            <li><strong>Limitation:</strong> Closed source; slightly behind llama.cpp direct on performance. Does not support vLLM-style batching.</li>
          </ul>
        </div>
      </div>

      <div class="opt-card">
        <div class="opt-card-header opt-general">Jan.ai</div>
        <div class="opt-card-body">
          <div class="sw-tool-desc">Open-source desktop app providing a full ChatGPT-style local interface with model management, extension support, and a local API server.</div>
          <span class="sw-tag sw-free">Open source</span>
          <span class="sw-tag sw-all">Windows / macOS / Linux</span>
          <ul>
            <li><strong>Full offline operation:</strong> Designed to work with no internet connection. Model files are stored locally; no data leaves the machine.</li>
            <li><strong>Extension ecosystem:</strong> Supports plugins for additional functionality such as web search, document retrieval, and remote model endpoints.</li>
            <li><strong>API server:</strong> Exposes an OpenAI-compatible API on localhost:1337, allowing integration with any client that speaks the OpenAI format.</li>
            <li><strong>Good alternative to LM Studio</strong> for users who prefer open-source software with the same ease of use.</li>
          </ul>
        </div>
      </div>

      <div class="opt-card">
        <div class="opt-card-header opt-general">GPT4All</div>
        <div class="opt-card-body">
          <div class="sw-tool-desc">Fully offline AI assistant desktop app. No internet access required at any point, including model downloads (which can be done offline via direct GGUF import).</div>
          <span class="sw-tag sw-free">Open source</span>
          <span class="sw-tag sw-all">Windows / macOS / Linux</span>
          <ul>
            <li><strong>Air-gap friendly:</strong> The strongest choice for environments where internet connectivity must be fully severed after setup.</li>
            <li><strong>LocalDocs feature:</strong> Index local documents (PDF, Word, text) and ask questions about them with retrieval-augmented generation (RAG).</li>
            <li><strong>CPU-first design:</strong> Works well on CPU-only systems. GPU acceleration via Vulkan is available but less optimised than Ollama/LM Studio.</li>
          </ul>
        </div>
      </div>

      <div class="opt-card">
        <div class="opt-card-header opt-general">KoboldCPP</div>
        <div class="opt-card-body">
          <div class="sw-tool-desc">A single-binary fork of llama.cpp with a built-in web UI, advanced sampling controls, and strong support for long-form creative writing and roleplay use cases.</div>
          <span class="sw-tag sw-free">Open source</span>
          <span class="sw-tag sw-all">Windows / macOS / Linux</span>
          <ul>
            <li><strong>Single executable:</strong> Download one file and run it &mdash; no Python environment or package manager required.</li>
            <li><strong>Advanced sampling:</strong> Includes mirostat, tail-free sampling, and Typical P sampling not found in all frontends &mdash; particularly useful for creative/story generation.</li>
            <li><strong>SillyTavern/Agnai integration:</strong> The community-standard backend for character-based AI chat interfaces.</li>
          </ul>
        </div>
      </div>

      <div class="opt-card">
        <div class="opt-card-header opt-general">llama.cpp (direct)</div>
        <div class="opt-card-body">
          <div class="sw-tool-desc">The foundational C++ inference engine that powers Ollama, LM Studio, and KoboldCPP. Run it directly for maximum control over every parameter.</div>
          <span class="sw-tag sw-free">Open source</span>
          <span class="sw-tag sw-all">Windows / macOS / Linux</span>
          <ul>
            <li><strong>Maximum control:</strong> Exposes every optimization flag: <code>-ngl</code> (GPU layers), <code>--ctx-size</code>, <code>--cache-type-k</code>, <code>--flash-attn</code>, <code>-t</code> (threads), <code>--mlock</code>, and more.</li>
            <li><strong>Supports all GPU backends</strong> in one codebase: CUDA, Metal, ROCm (HIPrt), Vulkan, OpenCL, and CPU-only (AVX2, AVX-512).</li>
            <li><strong>GGUF quantization server:</strong> Includes <code>llama-quantize</code> to convert and quantize models locally without uploading to a cloud service.</li>
            <li><strong>Best choice when</strong> you are benchmarking, scripting, or need a parameter not exposed by higher-level tools.</li>
          </ul>
        </div>
      </div>

      <div class="opt-card">
        <div class="opt-card-header opt-nvidia">vLLM</div>
        <div class="opt-card-body">
          <div class="sw-tool-desc">Production-grade inference server for NVIDIA GPUs. Uses PagedAttention for efficient KV-cache memory management and continuous batching for high-throughput multi-user serving.</div>
          <span class="sw-tag sw-free">Open source</span>
          <span class="sw-tag sw-lin">Linux (CUDA required)</span>
          <ul>
            <li><strong>PagedAttention:</strong> Manages KV-cache in variable-size pages, dramatically reducing memory fragmentation and enabling 2&ndash;5&times; higher throughput than llama.cpp for concurrent users.</li>
            <li><strong>Continuous batching:</strong> Processes multiple requests simultaneously without waiting for a full batch, reducing latency under load.</li>
            <li><strong>OpenAI-compatible API:</strong> Drop-in replacement for OpenAI endpoints &mdash; connect any client without code changes.</li>
            <li><strong>Requires Linux + CUDA.</strong> Not supported on macOS or Windows without WSL2. The standard choice for production NVIDIA deployments.</li>
          </ul>
        </div>
      </div>

      <div class="opt-card">
        <div class="opt-card-header opt-nvidia">ExLlamaV2</div>
        <div class="opt-card-body">
          <div class="sw-tool-desc">The fastest CUDA inference engine for quantized models. Uses the EXL2 quantization format, which achieves higher quality than GGUF Q4 at the same file size via per-row quantization.</div>
          <span class="sw-tag sw-free">Open source</span>
          <span class="sw-tag sw-win">Windows</span>
          <span class="sw-tag sw-lin">Linux</span>
          <ul>
            <li><strong>EXL2 format:</strong> Per-row mixed-precision quantization. A 4-bit EXL2 model typically outperforms a GGUF Q4_K_M on the same hardware in quality benchmarks.</li>
            <li><strong>Speed:</strong> Consistently outperforms TensorRT-LLM on chat workloads at Q4&ndash;Q5. Best choice for maximum CUDA inference speed with quantized models.</li>
            <li><strong>Speculative decoding support:</strong> Use a small draft model to propose tokens &mdash; can yield near 2&times; speed on certain workloads.</li>
            <li><strong>Tabby API server:</strong> ExLlamaV2 is the backend for TabbyAPI, a high-performance OpenAI-compatible server used by many frontends.</li>
          </ul>
        </div>
      </div>

    </div>
  </div>

  <!-- AI CODING ASSISTANTS -->
  <div class="opt-section">
    <h3>AI Coding Assistants</h3>
    <p style="font-size:0.88em;color:#555;margin-top:0">Tools that integrate AI into the code-writing and editing workflow, from inline completions to full agentic coding agents that can edit multiple files and run commands.</p>
    <div class="opt-cards">

      <div class="opt-card">
        <div class="opt-card-header opt-general">Claude Code (Anthropic)</div>
        <div class="opt-card-body">
          <div class="sw-tool-desc">Terminal-based agentic coding assistant. Reads and edits files, runs shell commands, searches the codebase, and writes and executes code autonomously within a directory. Uses Claude models via API.</div>
          <span class="sw-tag sw-freemium">Pay-per-token</span>
          <span class="sw-tag sw-mac">macOS native</span>
          <span class="sw-tag sw-lin">Linux native</span>
          <span class="sw-tag sw-win">WSL2 on Windows</span>
          <ul>
            <li><strong>Full agentic loop:</strong> Can read project files, propose and apply edits, run tests, and iterate on errors without constant user prompting. Most effective on large codebases.</li>
            <li><strong>Git integration:</strong> Understands git history, creates commits, and works across branches. Best used in a git-tracked repository.</li>
            <li><strong>Windows note:</strong> Requires WSL2 (Ubuntu 22.04+). Running inside WSL2 gives the same experience as Linux. File edits across the <code>/mnt/c</code> mount work but are slower than editing files inside the WSL2 filesystem.</li>
            <li><strong>Model selection:</strong> Defaults to Claude Sonnet; can be configured to use Claude Opus for complex reasoning tasks at higher cost.</li>
          </ul>
        </div>
      </div>

      <div class="opt-card">
        <div class="opt-card-header opt-general">GitHub Copilot</div>
        <div class="opt-card-body">
          <div class="sw-tool-desc">The most widely used AI coding assistant. Provides inline code completion and a chat panel within VS Code, JetBrains IDEs, Neovim, and others. Uses GPT-4o and Claude models depending on task.</div>
          <span class="sw-tag sw-paid">$10/month (individual)</span>
          <span class="sw-tag sw-all">All platforms</span>
          <ul>
            <li><strong>Inline completion:</strong> Suggests single lines or entire function bodies as you type with low latency. Works in virtually all languages.</li>
            <li><strong>Copilot Chat:</strong> Ask questions about the codebase, get explanations, generate tests, and fix errors directly in the IDE sidebar.</li>
            <li><strong>Copilot Workspace / Agent:</strong> Multi-step task completion &mdash; describe a feature and Copilot proposes a plan, creates files, and opens PRs (enterprise tier).</li>
            <li><strong>Model choice:</strong> GitHub now lets users select the underlying model (GPT-4o, Claude Sonnet, Gemini 2.0) per session.</li>
          </ul>
        </div>
      </div>

      <div class="opt-card">
        <div class="opt-card-header opt-general">Cursor</div>
        <div class="opt-card-body">
          <div class="sw-tool-desc">AI-first code editor forked from VS Code. Adds a multi-file edit "Composer" mode, codebase-wide context awareness, and tight model integration beyond what VS Code extensions provide.</div>
          <span class="sw-tag sw-freemium">Free tier / $20 month pro</span>
          <span class="sw-tag sw-all">Windows / macOS / Linux</span>
          <ul>
            <li><strong>Composer mode:</strong> Describe a change in natural language; Cursor proposes and applies diffs across multiple files simultaneously &mdash; the main differentiator from GitHub Copilot.</li>
            <li><strong>Codebase indexing:</strong> Indexes your entire repository for semantic search, giving the model accurate context even in large codebases.</li>
            <li><strong>Model selection:</strong> Supports Claude 3.5/3.7 Sonnet, GPT-4o, and o3 &mdash; swappable per-task. Can also point to a local Ollama endpoint for completions.</li>
            <li><strong>Tab / Ghost text:</strong> A distinctive predictive editing feature that anticipates your next edit based on recent changes &mdash; faster than standard autocomplete.</li>
          </ul>
        </div>
      </div>

      <div class="opt-card">
        <div class="opt-card-header opt-general">Continue.dev</div>
        <div class="opt-card-body">
          <div class="sw-tool-desc">Open-source AI coding extension for VS Code and JetBrains. Unique in that it supports local Ollama models as well as cloud APIs, giving you AI coding assistance with no data leaving your machine.</div>
          <span class="sw-tag sw-free">Open source</span>
          <span class="sw-tag sw-all">Windows / macOS / Linux</span>
          <ul>
            <li><strong>Local model support:</strong> Connect to Ollama, LM Studio, or any OpenAI-compatible API. Run Qwen2.5-Coder 32B locally for code completion with full privacy.</li>
            <li><strong>Configurable via JSON:</strong> <code>config.json</code> specifies models for chat, completion, and embedding independently &mdash; mix local and cloud models.</li>
            <li><strong>Tab autocomplete:</strong> Uses a separate fast local model (e.g. Qwen2.5-Coder 1.5B) for low-latency line completions while using a larger model for chat.</li>
            <li><strong>Best choice for</strong> developers who want GitHub Copilot-style features but with local models and no subscription.</li>
          </ul>
        </div>
      </div>

      <div class="opt-card">
        <div class="opt-card-header opt-general">Windsurf (Codeium)</div>
        <div class="opt-card-body">
          <div class="sw-tool-desc">AI-first code editor from Codeium, similar to Cursor. Offers a "Cascade" agent mode for multi-step task completion and deep IDE integration. Strong free tier.</div>
          <span class="sw-tag sw-freemium">Free tier / $15 month pro</span>
          <span class="sw-tag sw-all">Windows / macOS / Linux</span>
          <ul>
            <li><strong>Cascade agent:</strong> Handles multi-file edits, runs terminal commands, reads error output, and iterates &mdash; similar to Claude Code but embedded in the IDE.</li>
            <li><strong>Generous free tier:</strong> Unlimited code completions and a meaningful number of agent interactions per month at no cost &mdash; competitive with Cursor.</li>
            <li><strong>Codeium extension also available</strong> as a VS Code/JetBrains plugin if you don't want to switch editors, providing completions without the full Windsurf agent.</li>
          </ul>
        </div>
      </div>

      <div class="opt-card">
        <div class="opt-card-header opt-general">Aider</div>
        <div class="opt-card-body">
          <div class="sw-tool-desc">Terminal-based AI coding agent emphasizing git-awareness. Applies changes as atomic git commits, supports multiple files per session, and works with any LLM including local Ollama models.</div>
          <span class="sw-tag sw-free">Open source</span>
          <span class="sw-tag sw-all">Windows / macOS / Linux</span>
          <ul>
            <li><strong>Git-first workflow:</strong> Every change is committed with a descriptive message, making it easy to review, revert, or cherry-pick AI edits.</li>
            <li><strong>Local model support:</strong> Connect to an Ollama endpoint and use Qwen2.5-Coder 32B or DeepSeek-Coder V2 for fully offline coding assistance.</li>
            <li><strong>Benchmark leading:</strong> Consistently at the top of the SWE-Bench leaderboard for open-source coding agents, particularly in architect + editor mode (uses two models cooperatively).</li>
            <li><strong>Works well alongside Claude Code</strong> &mdash; use Aider for git-structured file edits and Claude Code for broader exploration and command execution.</li>
          </ul>
        </div>
      </div>

    </div>
  </div>

  <!-- CHAT FRONTENDS -->
  <div class="opt-section">
    <h3>Local Chat Frontends &amp; Self-Hosted UIs</h3>
    <p style="font-size:0.88em;color:#555;margin-top:0">Web or desktop interfaces that connect to Ollama, LM Studio, or vLLM and provide a chat experience similar to ChatGPT but running entirely on your hardware.</p>
    <div class="opt-cards">

      <div class="opt-card">
        <div class="opt-card-header opt-general">Open WebUI</div>
        <div class="opt-card-body">
          <div class="sw-tool-desc">The most popular self-hosted ChatGPT-style web interface. Connects to Ollama and OpenAI-compatible APIs. Runs as a Docker container or Python package. Feature parity with ChatGPT's web UI including document upload, image analysis, and multi-model switching.</div>
          <span class="sw-tag sw-free">Open source</span>
          <span class="sw-tag sw-all">All platforms (browser-based)</span>
          <ul>
            <li><strong>One-command install:</strong> <code>docker run -d -p 3000:80 --add-host=host.docker.internal:host-gateway ghcr.io/open-webui/open-webui:main</code> &mdash; connects to a running Ollama instance automatically.</li>
            <li><strong>RAG support:</strong> Upload PDFs, web URLs, or YouTube transcripts; the model retrieves relevant chunks during conversation.</li>
            <li><strong>Multi-model conversations:</strong> Chat with multiple models side by side and compare responses.</li>
            <li><strong>User management:</strong> Supports multiple user accounts with role-based access &mdash; suitable for sharing a local model server within a team.</li>
          </ul>
        </div>
      </div>

      <div class="opt-card">
        <div class="opt-card-header opt-general">LibreChat</div>
        <div class="opt-card-body">
          <div class="sw-tool-desc">Self-hosted multi-provider chat interface supporting OpenAI, Anthropic, Google, Azure, and local models simultaneously in one UI. Strong feature set with branching conversations, code execution, and plugin support.</div>
          <span class="sw-tag sw-free">Open source</span>
          <span class="sw-tag sw-all">All platforms (browser-based, Docker)</span>
          <ul>
            <li><strong>Multi-provider in one UI:</strong> Switch between ChatGPT, Claude, Gemini, and a local Ollama model within the same conversation history.</li>
            <li><strong>Branching conversations:</strong> Fork any message to explore alternative responses without losing the original thread.</li>
            <li><strong>Best choice</strong> when you want to compare cloud and local model outputs or need team access to multiple API providers from a single shared interface.</li>
          </ul>
        </div>
      </div>

      <div class="opt-card">
        <div class="opt-card-header opt-general">Chatbox</div>
        <div class="opt-card-body">
          <div class="sw-tool-desc">Desktop application for AI chat supporting OpenAI, Anthropic, Google, Azure, and local Ollama endpoints. Stores conversations locally; no cloud sync unless configured. Good low-friction option for non-technical users.</div>
          <span class="sw-tag sw-freemium">Free (basic) / Pro subscription</span>
          <span class="sw-tag sw-all">Windows / macOS / Linux</span>
          <ul>
            <li><strong>Desktop-native:</strong> Installs like a normal application; no Docker or server setup needed. Connect to Ollama by pointing it to <code>http://localhost:11434</code>.</li>
            <li><strong>Privacy-first defaults:</strong> Conversation history stored locally by default. Suitable for sensitive work where cloud storage of chat history is undesirable.</li>
          </ul>
        </div>
      </div>

      <div class="opt-card">
        <div class="opt-card-header opt-general">Msty</div>
        <div class="opt-card-body">
          <div class="sw-tool-desc">Desktop AI assistant that aggregates local Ollama models and cloud APIs in a polished UI. Notable for its "Knowledge" feature &mdash; attach local documents to conversations &mdash; and side-by-side model comparison.</div>
          <span class="sw-tag sw-freemium">Free tier available</span>
          <span class="sw-tag sw-all">Windows / macOS / Linux</span>
          <ul>
            <li><strong>Model comparison view:</strong> Query multiple models simultaneously and view responses side by side in the same window.</li>
            <li><strong>Knowledge library:</strong> Attach PDF, Word, or text files to a conversation with built-in RAG. No separate setup required.</li>
            <li><strong>Good choice</strong> for users who want a more polished interface than Open WebUI without the Docker setup complexity.</li>
          </ul>
        </div>
      </div>

    </div>
  </div>

  <!-- CLOUD AND API ACCESS -->
  <div class="opt-section">
    <h3>Cloud AI Services &amp; API Access</h3>
    <p style="font-size:0.88em;color:#555;margin-top:0">Services that provide access to large frontier models via web interface or API. Useful when a task exceeds local hardware capacity or when the latest proprietary model is needed.</p>
    <table class="opt-table">
      <thead><tr>
        <th>Service</th>
        <th>Models Available</th>
        <th>Pricing</th>
        <th>Privacy / Notes</th>
      </tr></thead>
      <tbody>
        <tr>
          <td><strong>Claude.ai (Anthropic)</strong></td>
          <td>Claude 3.5 Haiku, Claude Sonnet 4, Claude Opus 4</td>
          <td>Free tier / $20&ndash;$25/month Pro / API pay-per-token</td>
          <td>Strong system prompt adherence. Pro tier includes extended context (200K tokens), Projects (persistent memory), and document upload. API keys required for Claude Code.</td>
        </tr>
        <tr>
          <td><strong>ChatGPT (OpenAI)</strong></td>
          <td>GPT-4o, o3, o3-mini, GPT-4.5</td>
          <td>Free (GPT-4o limited) / $20/month Plus / API pay-per-token</td>
          <td>Widest tool-use ecosystem. GPT-4o includes image generation (DALL-E 3), web browsing, and code execution in one model. o3 is the strongest reasoning model for complex multi-step problems.</td>
        </tr>
        <tr>
          <td><strong>DuckDuckGo AI Chat</strong></td>
          <td>Claude 3 Haiku, GPT-4o mini, Llama 3, Mistral</td>
          <td>Free (no account required)</td>
          <td>Privacy-focused: DuckDuckGo does not store conversations and removes identifying metadata before forwarding to model providers. Best for quick queries where privacy is a priority.</td>
        </tr>
        <tr>
          <td><strong>Perplexity</strong></td>
          <td>Sonar (proprietary), Claude, GPT-4o</td>
          <td>Free / $20/month Pro</td>
          <td>AI search engine that cites sources with inline references. Excellent for research queries, real-time information, and technical documentation lookups. Pro tier adds file upload and image generation.</td>
        </tr>
        <tr>
          <td><strong>Phind</strong></td>
          <td>Phind-70B (code-focused), GPT-4o</td>
          <td>Free (limited) / $20/month Pro</td>
          <td>Developer-focused search and chat. Specialises in code questions, pulls from Stack Overflow, GitHub, and documentation. Strong at answering "how do I do X in Y framework" questions with working code.</td>
        </tr>
        <tr>
          <td><strong>Google Gemini</strong></td>
          <td>Gemini 2.0 Flash, Gemini 2.5 Pro</td>
          <td>Free / $20/month Advanced / API pay-per-token</td>
          <td>Gemini 2.5 Pro has the longest context window of any frontier model (1M tokens). Excellent for large document analysis. Deep Google Workspace integration. Gemini 2.0 Flash is extremely cost-effective via API.</td>
        </tr>
      </tbody>
    </table>
  </div>

  <!-- FINE-TUNING AND TRAINING -->
  <div class="opt-section">
    <h3>Fine-tuning &amp; Training Frameworks</h3>
    <p style="font-size:0.88em;color:#555;margin-top:0">Tools for customizing pre-trained models on your own data using LoRA, QLoRA, or full fine-tuning. Requires NVIDIA GPU (Linux) or Apple Silicon (MLX path only).</p>
    <div class="opt-cards">

      <div class="opt-card">
        <div class="opt-card-header opt-nvidia">Axolotl</div>
        <div class="opt-card-body">
          <div class="sw-tool-desc">The most widely used open-source LoRA/QLoRA fine-tuning framework. YAML-based configuration means no Python scripting needed for most use cases. Supports virtually all major model architectures.</div>
          <span class="sw-tag sw-free">Open source</span>
          <span class="sw-tag sw-lin">Linux + CUDA</span>
          <ul>
            <li><strong>YAML configuration:</strong> Specify model, dataset, LoRA rank, batch size, learning rate, and all training parameters in a single <code>config.yml</code>. Run with <code>axolotl train config.yml</code>.</li>
            <li><strong>Broad format support:</strong> Accepts ShareGPT, Alpaca, instruction, and completion formats out of the box. Custom tokenization templates for any chat format.</li>
            <li><strong>QLoRA support:</strong> Load the base model in 4-bit BitsAndBytes precision while training LoRA adapters in FP16 &mdash; fine-tune a 70B model on a single A100 80GB.</li>
            <li><strong>DeepSpeed integration:</strong> Enables multi-GPU and CPU memory offloading for training models that don't fit in VRAM alone.</li>
          </ul>
        </div>
      </div>

      <div class="opt-card">
        <div class="opt-card-header opt-nvidia">Unsloth</div>
        <div class="opt-card-body">
          <div class="sw-tool-desc">Accelerated LoRA fine-tuning library that rewrites attention kernels in Triton for 2&times; speed and up to 70% less VRAM compared to standard HuggingFace PEFT. The go-to tool for consumer GPU fine-tuning.</div>
          <span class="sw-tag sw-free">Open source</span>
          <span class="sw-tag sw-lin">Linux + CUDA</span>
          <ul>
            <li><strong>2&times; faster than HF PEFT</strong> at the same quality. Fine-tune Llama 3.1 8B on a 24 GB GPU in under 2 hours on a typical instruction dataset.</li>
            <li><strong>70% less VRAM.</strong> Fine-tune a 7B model on 6 GB VRAM; a 13B on 12 GB. Enables fine-tuning on consumer RTX 3060/4060 GPUs that previously could not run training at all.</li>
            <li><strong>Colab-native:</strong> Unsloth provides free Google Colab notebooks for every major model &mdash; fine-tune without owning any GPU hardware.</li>
            <li><strong>Export to GGUF:</strong> Built-in export to GGUF quantization formats (Q4_K_M, Q5_K_M, Q8_0) immediately after training.</li>
          </ul>
        </div>
      </div>

      <div class="opt-card">
        <div class="opt-card-header opt-general">LLaMA-Factory</div>
        <div class="opt-card-body">
          <div class="sw-tool-desc">Web UI and CLI for fine-tuning over 100 model architectures with LoRA, QLoRA, DoRA, ORPO, DPO, and RLHF. Provides a visual training dashboard, dataset management, and evaluation tools in one interface.</div>
          <span class="sw-tag sw-free">Open source</span>
          <span class="sw-tag sw-lin">Linux + CUDA (primary)</span>
          <ul>
            <li><strong>Web UI (LlamaBoard):</strong> Configure and launch fine-tuning jobs from a browser without any command line or Python knowledge.</li>
            <li><strong>Advanced alignment methods:</strong> Supports RLHF, DPO, ORPO, and reward model training &mdash; not just supervised fine-tuning. The most comprehensive fine-tuning toolkit available.</li>
            <li><strong>Multi-GPU and distributed training</strong> via DeepSpeed and FSDP with minimal configuration.</li>
          </ul>
        </div>
      </div>

      <div class="opt-card">
        <div class="opt-card-header opt-apple">MLX Fine-tuning (Apple)</div>
        <div class="opt-card-body">
          <div class="sw-tool-desc">Apple's MLX framework includes a built-in LoRA fine-tuning path via <code>mlx_lm.lora</code>. The only fine-tuning option available on Apple Silicon &mdash; no CUDA, Axolotl, or Unsloth needed.</div>
          <span class="sw-tag sw-free">Open source</span>
          <span class="sw-tag sw-mac">macOS (Apple Silicon only)</span>
          <ul>
            <li><strong>Native Apple Silicon acceleration:</strong> Uses Metal for all matrix operations &mdash; no overhead from cross-platform abstraction layers.</li>
            <li><strong>LoRA and QLoRA supported</strong> on most Llama, Mistral, Phi, and Qwen architecture models. Uses MLX native int4 quantization for the base model during QLoRA.</li>
            <li><strong>Limitation:</strong> Dataset format support is narrower than Axolotl. Works best for instruction fine-tuning on simple JSONL datasets. Complex alignment training (DPO, RLHF) is not yet fully supported.</li>
            <li><strong>Run with:</strong> <code>mlx_lm.lora --model &lt;path&gt; --train --data ./data --iters 1000 --batch-size 4</code></li>
          </ul>
        </div>
      </div>

    </div>
  </div>

  <!-- MODEL DISCOVERY AND MANAGEMENT -->
  <div class="opt-section">
    <h3>Model Discovery, Evaluation &amp; Experiment Tracking</h3>
    <table class="opt-table">
      <thead><tr>
        <th>Tool</th>
        <th>Purpose</th>
        <th>Cost</th>
        <th>Key Features</th>
      </tr></thead>
      <tbody>
        <tr>
          <td><strong>Hugging Face Hub</strong></td>
          <td>Model repository &amp; discovery</td>
          <td>Free (storage limits) / Pro</td>
          <td>800,000+ models, datasets, and Spaces. Filter by task, license, and quantization format. The standard place to find GGUF, GPTQ, and AWQ quantized model variants. Direct download links for Ollama and llama.cpp.</td>
        </tr>
        <tr>
          <td><strong>Ollama Model Library</strong></td>
          <td>Curated models for Ollama</td>
          <td>Free</td>
          <td>Curated, verified GGUF models runnable with one command. Smaller and more opinionated than HF Hub. Includes all popular models (Llama 3, Mistral, Qwen, Phi, Gemma) pre-tested with Ollama.</td>
        </tr>
        <tr>
          <td><strong>LMSYS Chatbot Arena</strong></td>
          <td>Blind model evaluation leaderboard</td>
          <td>Free</td>
          <td>Community-ranked model leaderboard based on blind A/B comparisons. The most reliable quality ranking because it reflects human preferences, not just academic benchmarks. Good for choosing between similar model sizes.</td>
        </tr>
        <tr>
          <td><strong>Weights &amp; Biases (W&amp;B)</strong></td>
          <td>Experiment tracking &amp; ML monitoring</td>
          <td>Free (personal) / $50/month Teams</td>
          <td>Log training loss curves, hyperparameters, GPU metrics, and model checkpoints. Integrates with Axolotl, Unsloth, and transformers natively. The standard tool for tracking fine-tuning runs.</td>
        </tr>
        <tr>
          <td><strong>MLflow</strong></td>
          <td>Open-source experiment tracking</td>
          <td>Free (self-hosted)</td>
          <td>Local alternative to W&amp;B. Track experiments, compare model versions, and serve models via its built-in REST API. No cloud dependency &mdash; runs on your own machine or server.</td>
        </tr>
        <tr>
          <td><strong>lm-evaluation-harness</strong></td>
          <td>Standardized model benchmarking</td>
          <td>Free (open source)</td>
          <td>Eleuther AI's framework for running standard benchmarks (MMLU, HellaSwag, ARC, TruthfulQA) against local models. The tool used to generate most public benchmark numbers. Run with <code>lm_eval --model hf --model_args pretrained=&lt;model&gt; --tasks mmlu</code>.</td>
        </tr>
      </tbody>
    </table>
  </div>

</div>

<script>
  function openTab(evt, tabName) {
    var i, tabcontent, tablinks;
    tabcontent = document.getElementsByClassName("tab-content");
    for (i = 0; i < tabcontent.length; i++) {
      tabcontent[i].style.display = "none";
    }
    tablinks = document.getElementsByClassName("tab-button");
    for (i = 0; i < tablinks.length; i++) {
      tablinks[i].className = tablinks[i].className.replace(" active", "");
    }
    document.getElementById(tabName).style.display = "block";
    evt.currentTarget.className += " active";
  }

  function sortTable(n) {
    var table, rows, switching, i, x, y, shouldSwitch, dir, switchcount = 0;
    table = document.getElementById("hardwareTable");
    switching = true;
    var currentDir = table.getAttribute("data-sort-dir-" + n) || "asc";
    dir = (currentDir === "asc") ? "desc" : "asc";
    table.setAttribute("data-sort-dir-" + n, dir);
    
    // Clear existing sort indicators
    var headers = table.getElementsByTagName("TH");
    for (i = 0; i < headers.length; i++) {
        headers[i].innerHTML = headers[i].innerHTML.replace(/ (↑|↓)$/, "");
    }

    while (switching) {
      switching = false;
      rows = table.rows;
      for (i = 1; i < (rows.length - 1); i++) {
        shouldSwitch = false;
        x = rows[i].getElementsByTagName("TD")[n];
        y = rows[i + 1].getElementsByTagName("TD")[n];
        
        var xContent = x.innerText;
        var yContent = y.innerText;

        // For numeric columns, parse the numbers
        if (n === 0 || n === 4 || n === 6 || n === 7) {
            xContent = parseFloat(xContent.replace(/[^0-9.\-]+/g,""));
            yContent = parseFloat(yContent.replace(/[^0-9.\-]+/g,""));
        } else {
            xContent = xContent.toLowerCase();
            yContent = yContent.toLowerCase();
        }

        if (dir == "asc") {
          if (xContent > yContent) {
            shouldSwitch = true;
            break;
          }
        } else if (dir == "desc") {
          if (xContent < yContent) {
            shouldSwitch = true;
            break;
          }
        }
      }
      if (shouldSwitch) {
        rows[i].parentNode.insertBefore(rows[i + 1], rows[i]);
        switching = true;
        switchcount++;
      }
    }
    // Add sort indicator to header
    var sortedHeader = headers[n];
    if (dir === 'asc') {
        sortedHeader.innerHTML += ' &uarr;';
    } else {
        sortedHeader.innerHTML += ' &darr;';
    }
  }

  function filterTable() {
    var input, filter, table, tr, td, i, txtValue;
    var searchInput = document.getElementById("searchInput").value.toUpperCase();
    var modelFilter = document.getElementById("modelFilter").value;
    var typeFilter = document.getElementById("typeFilter").value.toUpperCase();
    table = document.getElementById("hardwareTable");
    tr = table.getElementsByTagName("tr");

    for (i = 1; i < tr.length; i++) { // Start from 1 to skip header
        // Ignore rows that are part of the notes
        if (tr[i].classList.contains('notes-row')) {
            continue;
        }
        tr[i].style.display = ""; // Reset display
        
        let isVisible = true;

        // Search filter
        let rowText = tr[i].textContent || tr[i].innerText;
        if (rowText.toUpperCase().indexOf(searchInput) === -1) {
            isVisible = false;
        }

        // Model filter
        if (isVisible && modelFilter) {
            let modelCell = tr[i].getElementsByTagName("td")[5];
            if (modelCell) {
                let modelText = modelCell.innerText;
                if (!modelText.includes(modelFilter)) {
                    isVisible = false;
                }
            }
        }

        // Type filter
        if (isVisible && typeFilter) {
            let typeCell = tr[i].getElementsByTagName("td")[2];
            if (typeCell) {
                if (typeCell.innerText.toUpperCase().indexOf(typeFilter) === -1) {
                    isVisible = false;
                }
            }
        }
        
        tr[i].style.display = isVisible ? "" : "none";
    }
  }

  /* ══════════════════════════════════════════════════════════════
     GPU RANKINGS DATA + LOGIC
  ══════════════════════════════════════════════════════════════ */
  const GPU_DATA = [
    // name, mfr, vram, bw (GB/s), tg7b (t/s), tg70b (t/s or null), price ($USD), tier, bestFor
    { name:"RTX 5090",           mfr:"NVIDIA", vram:32,  bw:1792, tg7b:250, tg70b:null,  price:1999,  tier:"consumer",     bestFor:"Fastest consumer GPU for ≤30B models" },
    { name:"RTX 4090",           mfr:"NVIDIA", vram:24,  bw:1008, tg7b:140, tg70b:null,  price:1200,  tier:"consumer",     bestFor:"Best value NVIDIA for 7B–24B models" },
    { name:"RTX 3090",           mfr:"NVIDIA", vram:24,  bw:936,  tg7b:130, tg70b:null,  price:600,   tier:"consumer",     bestFor:"Budget NVIDIA; great 24GB VRAM value" },
    { name:"RTX 5090 (Laptop)",  mfr:"NVIDIA", vram:24,  bw:960,  tg7b:140, tg70b:null,  price:3500,  tier:"laptop",       bestFor:"Portable CUDA workload; watch thermals" },
    { name:"RTX 4090 (Laptop)",  mfr:"NVIDIA", vram:16,  bw:576,  tg7b:80,  tg70b:null,  price:2500,  tier:"laptop",       bestFor:"Mobile CUDA inference up to 13B" },
    { name:"A100 80GB (PCIe)",   mfr:"NVIDIA", vram:80,  bw:1935, tg7b:220, tg70b:18,    price:5000,  tier:"professional", bestFor:"70B unquantized; production inference" },
    { name:"RTX 6000 Ada (48GB)",mfr:"NVIDIA", vram:48,  bw:960,  tg7b:200, tg70b:8,     price:6000,  tier:"professional", bestFor:"High-VRAM pro GPU; NVLink + ECC" },
    { name:"RTX A6000 (48GB)",   mfr:"NVIDIA", vram:48,  bw:768,  tg7b:180, tg70b:7,     price:3000,  tier:"professional", bestFor:"72B Q4 capable; strong multi-GPU value" },
    { name:"H100 80GB (PCIe)",   mfr:"NVIDIA", vram:80,  bw:2000, tg7b:260, tg70b:22,    price:25000, tier:"datacenter",   bestFor:"High-throughput production serving" },
    { name:"RX 7900 XTX",        mfr:"AMD",    vram:24,  bw:960,  tg7b:135, tg70b:null,  price:800,   tier:"consumer",     bestFor:"Best AMD discrete; rival to RTX 4090" },
    { name:"RX 7900 XT",         mfr:"AMD",    vram:20,  bw:800,  tg7b:115, tg70b:null,  price:650,   tier:"consumer",     bestFor:"Budget AMD; solid 20B performer" },
    { name:"Ryzen AI Max+ 395",  mfr:"AMD",    vram:128, bw:256,  tg7b:90,  tg70b:6,     price:1800,  tier:"laptop/mini",  bestFor:"Cheapest 70B-capable x86 system" },
    { name:"M3 Ultra (192GB)",   mfr:"Apple",  vram:192, bw:800,  tg7b:120, tg70b:20,    price:8000,  tier:"desktop",      bestFor:"Up to 120B+ models; silent & efficient" },
    { name:"M3 Ultra (512GB)",   mfr:"Apple",  vram:512, bw:800,  tg7b:120, tg70b:20,    price:10000, tier:"desktop",      bestFor:"200B+ models; unmatched capacity" },
    { name:"M3 Max (128GB)",     mfr:"Apple",  vram:128, bw:300,  tg7b:60,  tg70b:5,     price:4000,  tier:"laptop",       bestFor:"Portable 70B inference; macOS dev" },
    { name:"M4 Max (128GB)",     mfr:"Apple",  vram:128, bw:410,  tg7b:75,  tg70b:6,     price:4200,  tier:"laptop",       bestFor:"Improved MacBook for 70B inference" },
    { name:"M3 Pro (36GB)",      mfr:"Apple",  vram:36,  bw:150,  tg7b:35,  tg70b:null,  price:2200,  tier:"laptop",       bestFor:"Entry Apple; up to 30B Q4" },
  ];

  // Compute derived scores
  GPU_DATA.forEach(g => {
    // Value score: tokens/sec per $100
    g.valueScore = g.price > 0 ? parseFloat((g.tg7b / g.price * 100).toFixed(2)) : 0;
    // Effectiveness: weighted blend of normalized bw (40%), tg7b (40%), vram (20%)
    // Will normalize after computing raw values
  });
  const maxBw   = Math.max(...GPU_DATA.map(g => g.bw));
  const maxTg7b = Math.max(...GPU_DATA.map(g => g.tg7b));
  const maxVram = Math.max(...GPU_DATA.map(g => g.vram));
  GPU_DATA.forEach(g => {
    g.score = parseFloat(((g.bw/maxBw)*40 + (g.tg7b/maxTg7b)*40 + (g.vram/maxVram)*20).toFixed(1));
  });
  // Assign rank by default score desc
  const sorted = [...GPU_DATA].sort((a,b) => b.score - a.score);
  sorted.forEach((g, i) => { g.rank = i + 1; });

  let gpuActiveMfr = 'all';
  let gpuSortField = 'score';
  let gpuSortDir   = 'desc';
  let gpuInitDone  = false;

  function initGpuTab() {
    if (!gpuInitDone) { gpuInitDone = true; renderGpuTable(); }
  }

  function setMfrFilter(mfr) {
    gpuActiveMfr = mfr;
    ['all','nvidia','amd','apple'].forEach(id => {
      const el = document.getElementById('mfr-' + id);
      el.className = 'mfr-btn';
    });
    const activeId = 'mfr-' + (mfr === 'NVIDIA' ? 'nvidia' : mfr === 'AMD' ? 'amd' : mfr === 'Apple' ? 'apple' : 'all');
    const activeClass = mfr === 'NVIDIA' ? 'active-nvidia' : mfr === 'AMD' ? 'active-amd' : mfr === 'Apple' ? 'active-apple' : 'active-all';
    document.getElementById(activeId).className = 'mfr-btn ' + activeClass;
    renderGpuTable();
  }

  function gpuSortCol(field) {
    if (gpuSortField === field) {
      gpuSortDir = gpuSortDir === 'desc' ? 'asc' : 'desc';
    } else {
      gpuSortField = field;
      gpuSortDir = (field === 'price' || field === 'name' || field === 'mfr' || field === 'bestFor') ? 'asc' : 'desc';
    }
    document.getElementById('gpuSortSelect').value = ['score','tg7b','bw','vram','price','valueScore'].includes(field) ? field : 'score';
    renderGpuTable();
  }

  function renderGpuTable() {
    const sortBy   = document.getElementById('gpuSortSelect').value || gpuSortField;
    gpuSortField   = sortBy;
    const search   = (document.getElementById('gpuSearch').value || '').toLowerCase();

    let data = GPU_DATA.filter(g => {
      if (gpuActiveMfr !== 'all' && g.mfr !== gpuActiveMfr) return false;
      if (search && !g.name.toLowerCase().includes(search) && !g.mfr.toLowerCase().includes(search) && !g.bestFor.toLowerCase().includes(search)) return false;
      return true;
    });

    const numericDesc = ['score','tg7b','bw','vram','valueScore'];
    const numericAsc  = ['price'];
    data.sort((a, b) => {
      let av = a[sortBy], bv = b[sortBy];
      if (typeof av === 'string') av = av.toLowerCase();
      if (typeof bv === 'string') bv = bv.toLowerCase();
      if (av === null) av = -1;
      if (bv === null) bv = -1;
      if (numericDesc.includes(sortBy)) return bv - av;
      if (numericAsc.includes(sortBy))  return av - bv;
      return av < bv ? -1 : av > bv ? 1 : 0;
    });

    const tbody = document.getElementById('gpuTableBody');
    tbody.innerHTML = '';
    const maxScore      = Math.max(...GPU_DATA.map(g => g.score));
    const maxValueScore = Math.max(...GPU_DATA.map(g => g.valueScore));

    data.forEach((g, idx) => {
      const badgeClass = g.mfr === 'NVIDIA' ? 'badge-nvidia' : g.mfr === 'AMD' ? 'badge-amd' : 'badge-apple';
      const barWidth   = Math.round((g.score / 100) * 100);
      const valBarW    = Math.round((g.valueScore / maxValueScore) * 100);
      const tg70bStr   = g.tg70b !== null ? g.tg70b : '<span style="color:#aaa">—</span>';
      const tr = document.createElement('tr');
      tr.innerHTML = `
        <td>${g.rank}</td>
        <td><strong>${g.name}</strong></td>
        <td><span class="mfr-badge ${badgeClass}">${g.mfr}</span></td>
        <td>${g.vram}</td>
        <td>${g.bw.toLocaleString()}</td>
        <td>${g.tg7b}</td>
        <td>${tg70bStr}</td>
        <td>$${g.price.toLocaleString()}</td>
        <td>
          <div class="score-bar-wrap">
            <div class="score-bar" style="width:${valBarW}%;background:#fd7e14;"></div>
            <span class="score-num">${g.valueScore}</span>
          </div>
        </td>
        <td>
          <div class="score-bar-wrap">
            <div class="score-bar" style="width:${barWidth}%;"></div>
            <span class="score-num">${g.score}/100</span>
          </div>
        </td>
        <td style="font-size:0.88em;">${g.bestFor}</td>
      `;
      tbody.appendChild(tr);
    });

    // Update header sort arrows
    const headers = document.querySelectorAll('#gpuTable thead th');
    const colMap = ['rank','name','mfr','vram','bw','tg7b','tg70b','price','valueScore','score','bestFor'];
    headers.forEach((th, i) => {
      th.innerHTML = th.innerHTML.replace(/ [↑↓]$/, '');
      if (colMap[i] === gpuSortField) th.innerHTML += gpuSortDir === 'desc' ? ' ↓' : ' ↑';
    });
  }

  /* ══════════════════════════════════════════════════════════════
     BEST MATCH FINDER LOGIC
  ══════════════════════════════════════════════════════════════ */

  // Systems dataset for recommendation engine
  const SYSTEMS = [
    { name:"HP Z8 G4 (Quad A100 40GB)",    os:["linux","windows"], ff:"desktop",  minPrice:25000, maxPrice:35000, maxModel:120, tg7b:540,  tg70b:50,  priority:["speed","capacity"] },
    { name:"Dell Precision 7960 Tower",    os:["linux","windows"], ff:"desktop",  minPrice:15000, maxPrice:25000, maxModel:120, tg7b:450,  tg70b:40,  priority:["speed","capacity"] },
    { name:"Apple Mac Studio (M3 Ultra 512GB)", os:["macos"],   ff:"desktop",  minPrice:9500,  maxPrice:11000, maxModel:200, tg7b:120,  tg70b:20,  priority:["capacity","value"] },
    { name:"Apple Mac Studio (M3 Ultra 192GB)", os:["macos"],   ff:"desktop",  minPrice:7500,  maxPrice:9500,  maxModel:120, tg7b:120,  tg70b:18,  priority:["capacity","value"] },
    { name:"Lenovo ThinkStation P620 (2×A6000)", os:["linux","windows"], ff:"desktop", minPrice:12000, maxPrice:18000, maxModel:70, tg7b:300, tg70b:20, priority:["capacity","speed"] },
    { name:"HP OMEN 45L (RTX 5090)",       os:["linux","windows"], ff:"desktop",  minPrice:3500,  maxPrice:5500,  maxModel:30,  tg7b:250,  tg70b:null, priority:["speed","value"] },
    { name:"Alienware Aurora R16 (RTX 4090)", os:["linux","windows"], ff:"desktop", minPrice:3000, maxPrice:5000, maxModel:30, tg7b:140, tg70b:null, priority:["speed","value"] },
    { name:"DIY RTX 5090 Build",           os:["linux","windows"], ff:"desktop",  minPrice:2500,  maxPrice:4000,  maxModel:30,  tg7b:250,  tg70b:null, priority:["speed","value"] },
    { name:"DIY RTX 4090 Build",           os:["linux","windows"], ff:"desktop",  minPrice:2000,  maxPrice:3500,  maxModel:30,  tg7b:140,  tg70b:null, priority:["speed","value"] },
    { name:"DIY RTX 3090 Build (used)",    os:["linux","windows"], ff:"desktop",  minPrice:1200,  maxPrice:2000,  maxModel:14,  tg7b:130,  tg70b:null, priority:["value"] },
    { name:"Apple MacBook Pro 16 (M4 Max 128GB)", os:["macos"], ff:"laptop",   minPrice:3800,  maxPrice:5000,  maxModel:70,  tg7b:75,   tg70b:6,   priority:["capacity","portable"] },
    { name:"Apple MacBook Pro 16 (M3 Max 128GB)", os:["macos"], ff:"laptop",   minPrice:3200,  maxPrice:4200,  maxModel:70,  tg7b:60,   tg70b:5,   priority:["capacity","portable"] },
    { name:"Asus ROG Strix Scar 18 (RTX 5090 Laptop)", os:["linux","windows"], ff:"laptop", minPrice:3500, maxPrice:5000, maxModel:30, tg7b:140, tg70b:null, priority:["speed","portable"] },
    { name:"Asus ROG Flow Z13 / Ryzen AI Max+ 395", os:["linux","windows"], ff:"laptop", minPrice:1400, maxPrice:2800, maxModel:70, tg7b:90, tg70b:6, priority:["capacity","value","portable"] },
    { name:"Minisforum AI MAX+ 395 Mini PC", os:["linux","windows"], ff:"laptop", minPrice:1200, maxPrice:2000, maxModel:70, tg7b:90, tg70b:6, priority:["capacity","value","portable"] },
    { name:"ASUS Zenbook A14 (Snapdragon X Elite)", os:["windows"], ff:"laptop",  minPrice:900,  maxPrice:1600,  maxModel:7,   tg7b:25,   tg70b:null, priority:["portable"] },
  ];

  let selectedOs = 'any';

  function setOs(os) {
    selectedOs = os;
    ['macos','windows','linux','any'].forEach(id => {
      document.getElementById('os-' + id).classList.remove('active');
    });
    document.getElementById('os-' + os).classList.add('active');
  }

  function runFinder() {
    const budget    = parseInt(document.getElementById('budgetSlider').value);
    const modelSize = parseInt(document.getElementById('modelSize-select').value);
    const ff        = document.getElementById('ff-select').value;
    const priority  = document.getElementById('priority-select').value;

    // Filter candidates
    let candidates = SYSTEMS.filter(s => {
      if (selectedOs !== 'any' && !s.os.includes(selectedOs)) return false;
      if (ff !== 'any' && s.ff !== ff) return false;
      if (s.minPrice > budget) return false;
      if (s.maxModel < modelSize) return false;
      return true;
    });

    const resultsEl = document.getElementById('finderResults');

    if (candidates.length === 0) {
      resultsEl.innerHTML = `
        <div class="no-result">
          <strong>No exact match found.</strong><br>
          Your current filters (OS: ${selectedOs}, budget: $${budget.toLocaleString()}, model size: ${modelSize}B) don't match any system in our database.
          Try increasing your budget, relaxing the OS requirement, or choosing a smaller model size.
        </div>`;
      return;
    }

    // Score candidates by priority
    candidates.forEach(s => {
      let pts = 0;
      if (priority === 'speed')    pts = s.tg7b;
      if (priority === 'value')    pts = s.tg7b / s.minPrice * 1000;
      if (priority === 'capacity') pts = s.maxModel + (s.tg70b || 0) * 2;
      if (priority === 'portable') pts = (s.ff === 'laptop' ? 50 : 0) + s.tg7b / s.minPrice * 500;
      s._pts = pts;
    });
    candidates.sort((a, b) => b._pts - a._pts);

    const best = candidates[0];
    const alts = candidates.slice(1, 4);

    const priorityLabel = { speed:"Maximum Speed", value:"Best Value", capacity:"Largest Model Support", portable:"Portability" }[priority];
    const osLabel = selectedOs === 'any' ? 'Any OS' : selectedOs === 'macos' ? 'macOS' : selectedOs === 'windows' ? 'Windows' : 'Linux';
    const modelLabel = modelSize >= 120 ? '120B+' : modelSize + 'B';

    const whyMap = {
      speed:    `delivers the highest token generation speed (~${best.tg7b} t/s on 7B models) within your budget`,
      value:    `gives the best tokens-per-dollar ratio within your $${budget.toLocaleString()} budget`,
      capacity: `supports up to ${best.maxModel}B parameter models${best.tg70b ? ', including 70B at ~' + best.tg70b + ' t/s' : ''}, the largest available in range`,
      portable: `is the best portable option${best.ff === 'laptop' ? ' (laptop form factor)' : ''} with solid inference speed`
    };

    resultsEl.innerHTML = `
      <div class="result-card">
        <div class="result-rank">Best match for ${osLabel} &middot; ${modelLabel} target &middot; ${priorityLabel}</div>
        <div class="result-name">${best.name}</div>
        <div class="result-meta">
          <span class="result-meta-chip">From $${best.minPrice.toLocaleString()}</span>
          <span class="result-meta-chip">~${best.tg7b} t/s (7B Q4)</span>
          <span class="result-meta-chip">Up to ${best.maxModel >= 200 ? '200B+' : best.maxModel + 'B'} models</span>
          <span class="result-meta-chip">${best.ff === 'laptop' ? 'Portable' : 'Desktop'}</span>
          <span class="result-meta-chip">${best.os.join(' / ')}</span>
        </div>
        <div class="result-why">
          <strong>Why this one?</strong> This system ${whyMap[priority]}.
          ${modelSize >= 70 && best.maxModel >= 70 ? ' It can handle your target model size (' + modelLabel + ') fully in memory.' : ''}
          ${priority === 'value' && best.os.includes('windows') ? ' Building a custom PC around this GPU can cut costs further — see the DIY tab.' : ''}
        </div>
      </div>
      ${alts.length > 0 ? `
      <div class="alt-results">
        <h4>Also consider</h4>
        ${alts.map(s => `
          <div class="alt-card">
            <div>
              <div class="alt-name">${s.name}</div>
              <div class="alt-reason">${s.tg7b} t/s · up to ${s.maxModel}B · ${s.ff} · ${s.os.join('/')}</div>
            </div>
            <div class="alt-price">from $${s.minPrice.toLocaleString()}</div>
          </div>`).join('')}
      </div>` : ''}
    `;
  }

  /* ══════════════════════════════════════════════════════════════
     COMPARE SYSTEMS DATA + LOGIC
  ══════════════════════════════════════════════════════════════ */

  // Each entry: id, name, mfr, type (unified/discrete/cpu), 
  // memGB (usable memory for models), bwGBs (memory bandwidth GB/s),
  // cpuCores, gpuCores, tg7b, tg13b, tg70b (null = can't fit),
  // price, notes
  const CMP_DATA = {
    // ── Apple Mac Studio ──────────────────────────────────────
    m3_ultra_base: {
      name: 'Mac Studio M3 Ultra', sub: '24-core CPU / 60-core GPU / 192 GB',
      mfr: 'apple', memGB: 192, bwGBs: 800, cpuCores: 24, gpuCores: 60, unified: true,
      tg7b: 115, tg13b: 68, tg70b: 18, tg120b: 9, price: 8000,
      framework: 'MLX / llama.cpp Metal', vramGB: 192,
      notes: 'Baseline M3 Ultra. 60-core GPU. Single-user 70B runs comfortably.'
    },
    m3_ultra_high: {
      name: 'Mac Studio M3 Ultra', sub: '24-core CPU / 76-core GPU / 192 GB',
      mfr: 'apple', memGB: 192, bwGBs: 800, cpuCores: 24, gpuCores: 76, unified: true,
      tg7b: 120, tg13b: 72, tg70b: 20, tg120b: 10, price: 8500,
      framework: 'MLX / llama.cpp Metal', vramGB: 192,
      notes: '76-core GPU adds ~5% token speed vs 60-core at same bandwidth — GPU core count is secondary to memory bandwidth.'
    },
    m3_ultra_xl: {
      name: 'Mac Studio M3 Ultra', sub: '24-core CPU / 76-core GPU / 512 GB',
      mfr: 'apple', memGB: 512, bwGBs: 800, cpuCores: 24, gpuCores: 76, unified: true,
      tg7b: 120, tg13b: 72, tg70b: 20, tg120b: 11, price: 10000,
      framework: 'MLX / llama.cpp Metal', vramGB: 512,
      notes: '512 GB unified. Enables 405B and MoE 671B (quantised). Token speed identical to 192 GB — bandwidth is unchanged; only capacity increases.'
    },
    m4_ultra_base: {
      name: 'Mac Studio M4 Ultra', sub: '24-core CPU / 60-core GPU / 192 GB',
      mfr: 'apple', memGB: 192, bwGBs: 900, cpuCores: 24, gpuCores: 60, unified: true,
      tg7b: 130, tg13b: 78, tg70b: 22, tg120b: 11, price: 8500,
      framework: 'MLX / llama.cpp Metal', vramGB: 192,
      notes: 'M4 Ultra. ~12% bandwidth increase over M3 Ultra at same memory tier.'
    },
    m4_ultra_high: {
      name: 'Mac Studio M4 Ultra', sub: '24-core CPU / 76-core GPU / 192 GB',
      mfr: 'apple', memGB: 192, bwGBs: 900, cpuCores: 24, gpuCores: 76, unified: true,
      tg7b: 135, tg13b: 80, tg70b: 23, tg120b: 12, price: 9000,
      framework: 'MLX / llama.cpp Metal', vramGB: 192,
      notes: 'M4 Ultra 76-core GPU. Marginal gain over 60-core at same bandwidth.'
    },
    m4_ultra_xl: {
      name: 'Mac Studio M4 Ultra', sub: '24-core CPU / 76-core GPU / 512 GB',
      mfr: 'apple', memGB: 512, bwGBs: 900, cpuCores: 24, gpuCores: 76, unified: true,
      tg7b: 135, tg13b: 80, tg70b: 23, tg120b: 12, price: 11000,
      framework: 'MLX / llama.cpp Metal', vramGB: 512,
      notes: 'Maximum capacity M4 Ultra. 405B unquantised possible. Bandwidth, not capacity, sets speed.'
    },
    // ── Apple MacBook Pro ─────────────────────────────────────
    m4_max_14_32: {
      name: 'MacBook Pro M4 Max', sub: '14-core CPU / 32-core GPU / 64 GB',
      mfr: 'apple', memGB: 64, bwGBs: 410, cpuCores: 14, gpuCores: 32, unified: true,
      tg7b: 65, tg13b: 38, tg70b: null, tg120b: null, price: 3200,
      framework: 'MLX / llama.cpp Metal', vramGB: 64,
      notes: 'Entry M4 Max. 64 GB allows 30B Q4 comfortably; 70B Q4 requires ~40 GB so just fits with limited context.'
    },
    m4_max_16_40: {
      name: 'MacBook Pro M4 Max', sub: '16-core CPU / 40-core GPU / 128 GB',
      mfr: 'apple', memGB: 128, bwGBs: 410, cpuCores: 16, gpuCores: 40, unified: true,
      tg7b: 75, tg13b: 45, tg70b: 7, tg120b: null, price: 4200,
      framework: 'MLX / llama.cpp Metal', vramGB: 128,
      notes: 'Top M4 Max config. 128 GB enables 70B Q4 (~40 GB) with headroom. +25% GPU cores vs 32-core — contributes ~10% extra speed.'
    },
    m3_max_14_30: {
      name: 'MacBook Pro M3 Max', sub: '14-core CPU / 30-core GPU / 64 GB',
      mfr: 'apple', memGB: 64, bwGBs: 300, cpuCores: 14, gpuCores: 30, unified: true,
      tg7b: 55, tg13b: 32, tg70b: null, tg120b: null, price: 3000,
      framework: 'MLX / llama.cpp Metal', vramGB: 64,
      notes: 'Entry M3 Max. 300 GB/s bandwidth; noticeably slower than M4 Max on the same model.'
    },
    m3_max_16_40: {
      name: 'MacBook Pro M3 Max', sub: '16-core CPU / 40-core GPU / 128 GB',
      mfr: 'apple', memGB: 128, bwGBs: 300, cpuCores: 16, gpuCores: 40, unified: true,
      tg7b: 60, tg13b: 36, tg70b: 5, tg120b: null, price: 4000,
      framework: 'MLX / llama.cpp Metal', vramGB: 128,
      notes: 'Top M3 Max. 70B narrowly fits. M4 Max is ~25% faster at same memory tier due to bandwidth.'
    },
    m3_pro_12_18: {
      name: 'MacBook Pro M3 Pro', sub: '12-core CPU / 18-core GPU / 36 GB',
      mfr: 'apple', memGB: 36, bwGBs: 150, cpuCores: 12, gpuCores: 18, unified: true,
      tg7b: 35, tg13b: 18, tg70b: null, tg120b: null, price: 2200,
      framework: 'MLX / llama.cpp Metal', vramGB: 36,
      notes: 'M3 Pro. 36 GB max; 30B Q4 fits. Significantly slower bandwidth (150 GB/s) limits token speed.'
    },
    // ── NVIDIA ────────────────────────────────────────────────
    rtx5090: {
      name: 'NVIDIA RTX 5090', sub: '32 GB GDDR7 — 1,792 GB/s',
      mfr: 'nvidia', memGB: 32, bwGBs: 1792, cpuCores: null, gpuCores: null, unified: false,
      tg7b: 250, tg13b: 170, tg70b: null, tg120b: null, price: 1999,
      framework: 'CUDA / llama.cpp / ExLlamaV2', vramGB: 32,
      notes: 'Fastest consumer GPU. 32 GB caps at ~26B Q4 models. Needs 64+ GB system RAM for larger models via CPU offload (slow).'
    },
    rtx4090: {
      name: 'NVIDIA RTX 4090', sub: '24 GB GDDR6X — 1,008 GB/s',
      mfr: 'nvidia', memGB: 24, bwGBs: 1008, cpuCores: null, gpuCores: null, unified: false,
      tg7b: 140, tg13b: 95, tg70b: null, tg120b: null, price: 1200,
      framework: 'CUDA / llama.cpp / ExLlamaV2', vramGB: 24,
      notes: 'Previous gen consumer flagship. 24 GB supports up to ~20B Q4. Best value NVIDIA for 7–13B workloads.'
    },
    rtx3090: {
      name: 'NVIDIA RTX 3090', sub: '24 GB GDDR6X — 936 GB/s',
      mfr: 'nvidia', memGB: 24, bwGBs: 936, cpuCores: null, gpuCores: null, unified: false,
      tg7b: 130, tg13b: 85, tg70b: null, tg120b: null, price: 600,
      framework: 'CUDA / llama.cpp / ExLlamaV2', vramGB: 24,
      notes: 'Used market bargain. Nearly identical VRAM and bandwidth to RTX 4090. Poor at fine-tuning vs newer cards (3x slower).'
    },
    rtxa6000: {
      name: 'NVIDIA RTX A6000', sub: '48 GB GDDR6 — 768 GB/s',
      mfr: 'nvidia', memGB: 48, bwGBs: 768, cpuCores: null, gpuCores: null, unified: false,
      tg7b: 180, tg13b: 120, tg70b: 3, tg120b: null, price: 3000,
      framework: 'CUDA / llama.cpp / vLLM', vramGB: 48,
      notes: '48 GB VRAM. 70B just fits in Q3 but is slow (~3 t/s) due to lower bandwidth vs RTX 5090. Excellent for concurrent multi-user via vLLM.'
    },
    a100_80: {
      name: 'NVIDIA A100 80 GB', sub: '80 GB HBM2e — 1,935 GB/s',
      mfr: 'nvidia', memGB: 80, bwGBs: 1935, cpuCores: null, gpuCores: null, unified: false,
      tg7b: 220, tg13b: 155, tg70b: 18, tg120b: null, price: 5000,
      framework: 'CUDA / vLLM / TensorRT-LLM', vramGB: 80,
      notes: 'Data-center card. 80 GB HBM2e. Fastest bandwidth-per-dollar for 70B models. Requires PCIe x16 slot and external PSU connector.'
    },
    // ── AMD ───────────────────────────────────────────────────
    rx7900xtx: {
      name: 'AMD RX 7900 XTX', sub: '24 GB GDDR6 — 960 GB/s',
      mfr: 'amd', memGB: 24, bwGBs: 960, cpuCores: null, gpuCores: null, unified: false,
      tg7b: 135, tg13b: 88, tg70b: null, tg120b: null, price: 800,
      framework: 'ROCm / Vulkan / llama.cpp', vramGB: 24,
      notes: 'Best AMD discrete. Near-RTX 4090 performance at lower price. ROCm has occasional compatibility issues; Vulkan backend recommended for pure inference.'
    },
    ryzen_ai_max: {
      name: 'AMD Ryzen AI Max+ 395', sub: 'x86 + RDNA3.5 iGPU — 128 GB unified',
      mfr: 'amd', memGB: 128, bwGBs: 256, cpuCores: 16, gpuCores: 40, unified: true,
      tg7b: 90, tg13b: 52, tg70b: 6, tg120b: null, price: 1800,
      framework: 'Vulkan / ROCm / llama.cpp', vramGB: 128,
      notes: 'Cheapest 70B-capable x86 system. 256 GB/s bandwidth is the bottleneck — 3x slower than M3 Ultra on 70B. BIOS must set VRAM to 128 GB.'
    },
  };

  // Model Q4 sizes in GB, used for fit calculations
  const MODEL_SIZES = [
    { label:'1B Q4',   gb:0.8,  desc:'SmolLM2 1.7B'},
    { label:'3B Q4',   gb:2.0,  desc:'Llama 3.2 3B'},
    { label:'7B Q4',   gb:4.4,  desc:'Llama 3.1 8B, Mistral 7B'},
    { label:'13B Q4',  gb:8.0,  desc:'Qwen2.5 14B'},
    { label:'20B Q4',  gb:12.0, desc:'Phi-4 20B, DeepSeek-R1 20B'},
    { label:'30B Q4',  gb:17.0, desc:'Qwen 32B, Gemma 27B'},
    { label:'70B Q4',  gb:40.0, desc:'Llama 3.1 70B, Qwen 72B'},
    { label:'70B Q8',  gb:75.0, desc:'Llama 3.1 70B full quality'},
    { label:'120B Q4', gb:70.0, desc:'Llama 3.1 405B Q2 / MoE models'},
    { label:'405B Q4', gb:230.0,desc:'Llama 3.1 405B'},
  ];

  function getBarClass(mfr) {
    return mfr === 'nvidia' ? 'bar-nvidia' : mfr === 'amd' ? 'bar-amd' : 'bar-apple';
  }
  function getSysClass(mfr) {
    return mfr === 'nvidia' ? 'sys-nvidia' : mfr === 'amd' ? 'sys-amd' : 'sys-apple';
  }

  function pctDiff(base, val) {
    if (!base || !val) return null;
    return Math.round((val - base) / base * 100);
  }
  function deltaTag(pct) {
    if (pct === null) return '';
    if (Math.abs(pct) < 2) return `<span class="cmp-delta same">same</span>`;
    if (pct > 0) return `<span class="cmp-delta faster">+${pct}%</span>`;
    return `<span class="cmp-delta slower">${pct}%</span>`;
  }
  function barHtml(val, maxVal, mfr) {
    const w = Math.round(Math.min(val / maxVal, 1) * 140);
    return `<div class="cmp-bar-wrap"><div class="cmp-bar ${getBarClass(mfr)}" style="width:${w}px"></div><span style="font-size:0.85em;font-weight:600">${val.toLocaleString()}</span></div>`;
  }

  function runCompare() {
    const ids = ['cmpA','cmpB','cmpC'].map(id => document.getElementById(id).value).filter(v => v !== 'none');
    if (ids.length < 2) { alert('Please select at least two systems to compare.'); return; }

    const systems = ids.map(id => CMP_DATA[id]).filter(Boolean);
    const base = systems[0];

    const maxBw  = Math.max(...systems.map(s => s.bwGBs));
    const maxMem = Math.max(...systems.map(s => s.memGB));
    const maxTg7b = Math.max(...systems.map(s => s.tg7b));
    const maxTg70b = Math.max(...systems.filter(s => s.tg70b).map(s => s.tg70b), 1);

    // ── Header cards ──
    const colCount = systems.length;
    const headerHtml = `
      <div class="cmp-systems-bar" style="grid-template-columns: repeat(${colCount}, 1fr)">
        ${systems.map(s => `
          <div class="cmp-sys-header ${getSysClass(s.mfr)}">
            <div class="sys-name">${s.name}</div>
            <div class="sys-sub">${s.sub}</div>
            <div style="font-size:0.78em;color:#666;margin-top:0.3em">$${s.price.toLocaleString()} &middot; ${s.framework}</div>
          </div>`).join('')}
      </div>`;

    // ── Core specs table ──
    const makeRow = (label, vals, unit, maxVal, isLowerBetter) => {
      const cells = vals.map((v, i) => {
        if (v == null) return `<td>&mdash;</td>`;
        const pct = i === 0 ? null : pctDiff(vals[0], v);
        const adjustedPct = isLowerBetter && pct !== null ? -pct : pct;
        return `<td>
          ${barHtml(v, maxVal, systems[i].mfr)}
          ${unit ? `<div style="font-size:0.78em;color:#666">${unit}</div>` : ''}
          ${i > 0 ? deltaTag(adjustedPct) : '<span style="font-size:0.75em;color:#888">baseline</span>'}
        </td>`;
      });
      return `<tr><th class="row-label">${label}</th>${cells.join('')}</tr>`;
    };

    const specsHtml = `
      <div class="cmp-section">
        <div class="cmp-section-title">Core Hardware Specs</div>
        <table class="cmp-table">
          <thead><tr>
            <th class="row-label">Spec</th>
            ${systems.map(s => `<th>${s.name.split(' ').slice(-2).join(' ')}</th>`).join('')}
          </tr></thead>
          <tbody>
            ${makeRow('Memory / VRAM', systems.map(s=>s.memGB), 'GB', maxMem, false)}
            ${makeRow('Memory Bandwidth', systems.map(s=>s.bwGBs), 'GB/s', maxBw, false)}
            ${makeRow('Price', systems.map(s=>s.price), 'USD', Math.max(...systems.map(s=>s.price)), true)}
            <tr>
              <th class="row-label">Memory Type</th>
              ${systems.map(s => `<td>${s.unified ? 'Unified (CPU+GPU share)' : 'Discrete VRAM'}</td>`).join('')}
            </tr>
            <tr>
              <th class="row-label">CPU Cores</th>
              ${systems.map(s => `<td>${s.cpuCores ? s.cpuCores + ' cores' : 'N/A (add-in card)'}</td>`).join('')}
            </tr>
            <tr>
              <th class="row-label">GPU Cores</th>
              ${systems.map(s => `<td>${s.gpuCores ? s.gpuCores + ' cores' : 'CUDA/CU — varies'}</td>`).join('')}
            </tr>
            <tr>
              <th class="row-label">Notes</th>
              ${systems.map(s => `<td style="font-size:0.82em;color:#555">${s.notes}</td>`).join('')}
            </tr>
          </tbody>
        </table>
      </div>`;

    // ── Token speed table ──
    const tgVals = [
      {label:'7B Q4 (t/s)',   field:'tg7b',  maxV: maxTg7b},
      {label:'13B Q4 (t/s)',  field:'tg13b', maxV: Math.max(...systems.filter(s=>s.tg13b).map(s=>s.tg13b),1)},
      {label:'70B Q4 (t/s)',  field:'tg70b', maxV: maxTg70b},
      {label:'120B Q4 (t/s)', field:'tg120b',maxV: Math.max(...systems.filter(s=>s.tg120b).map(s=>s.tg120b),1)},
    ];

    const tokHtml = `
      <div class="cmp-section">
        <div class="cmp-section-title">Token Generation Speed (single-user, batch size 1, Q4_K_M)</div>
        <table class="cmp-table">
          <thead><tr>
            <th class="row-label">Model Size</th>
            ${systems.map(s => `<th>${s.name.split(' ').slice(-2).join(' ')}</th>`).join('')}
          </tr></thead>
          <tbody>
            ${tgVals.map(tv => {
              const vals = systems.map(s => s[tv.field]);
              const cells = vals.map((v, i) => {
                if (v == null) return `<td style="color:#bbb">Does not fit</td>`;
                const pct = i === 0 ? null : pctDiff(vals[0], v);
                return `<td>
                  ${barHtml(v, tv.maxV, systems[i].mfr)}
                  <div style="font-size:0.75em;color:#666">tokens/sec</div>
                  ${i > 0 ? deltaTag(pct) : '<span style="font-size:0.75em;color:#888">baseline</span>'}
                </td>`;
              });
              return `<tr><th class="row-label">${tv.label}</th>${cells.join('')}</tr>`;
            }).join('')}
          </tbody>
        </table>
        <p style="font-size:0.78em;color:#888;padding:0.5em 0.9em;margin:0">
          "Does not fit" = Q4 model exceeds 90% of available memory. Speed scales with bandwidth; capacity determines fit. 
          GPU cores affect speed by ~5–10% within the same chip family — bandwidth is the primary driver.
        </p>
      </div>`;

    // ── Model fit grid ──
    const fitHtml = `
      <div class="cmp-section">
        <div class="cmp-section-title">Model Size Compatibility (Green = fits fully in Q4, Yellow = fits in Q3/Q2 only, Red = too large)</div>
        ${systems.map(s => {
          const cells = MODEL_SIZES.map(m => {
            const usable = s.memGB * 0.88;
            const cls = m.gb <= usable ? 'fit-yes' : m.gb <= usable * 1.25 ? 'fit-partial' : 'fit-no';
            return `<div class="mf-cell ${cls}">
              <div class="mf-size">${m.label}</div>
              <div class="mf-label">${m.desc}</div>
              <div class="mf-label" style="margin-top:0.2em">${m.gb} GB Q4</div>
            </div>`;
          });
          return `<div style="padding:0 0 0 0.9em;margin-bottom:0.4em"><strong style="font-size:0.88em">${s.name} — ${s.sub}</strong></div>
          <div class="model-fit-grid">${cells.join('')}</div>`;
        }).join('<hr style="margin:0.5em 0;border:none;border-top:1px solid #eee">')}
      </div>`;

    // ── RAM impact explanation ──
    const ramImpactHtml = `
      <div class="cmp-section">
        <div class="cmp-section-title">What More RAM / VRAM Changes for Each System</div>
        <table class="cmp-table">
          <thead><tr>
            <th class="row-label">System</th>
            <th>Current Memory</th>
            <th>Effect of more RAM on speed</th>
            <th>Effect of more RAM on model access</th>
            <th>Upgrade path</th>
          </tr></thead>
          <tbody>
            ${systems.map(s => {
              const speedEffect = s.unified
                ? 'None — bandwidth is fixed by chip die. More RAM adds capacity only, not speed.'
                : 'None — VRAM bandwidth is fixed. More VRAM (next-tier card) could improve speed only if bandwidth also increases.';
              const capacityEffect = s.memGB >= 192
                ? `Already at 70B+ tier. Adding RAM enables 200B+ and 405B models.`
                : s.memGB >= 64
                  ? `+64 GB would unlock 70B Q4 (needs ~40 GB). +128 GB allows 120B Q3.`
                  : s.memGB >= 32
                    ? `+32 GB reaches 70B Q4 threshold (~40 GB needed). Significant step up.`
                    : `Need 3× more RAM to reach 70B Q4 tier.`;
              const upgrade = s.unified && s.mfr === 'apple'
                ? 'Apple RAM is soldered at purchase — choose higher config at ordering time.'
                : s.mfr === 'nvidia'
                  ? 'Upgrade to next VRAM tier (e.g. RTX 4090→A6000→A100) or add a second GPU (use vLLM with tensor parallel).'
                  : s.mfr === 'amd' && s.unified
                    ? 'RAM is soldered in Ryzen AI Max laptops. Select 128 GB at purchase and set BIOS iGPU VRAM allocation to 96–128 GB.'
                    : 'Consult system spec sheet.';
              return `<tr>
                <td style="font-weight:600;font-size:0.88em">${s.name}<br><span style="font-weight:400;color:#666">${s.sub}</span></td>
                <td>${s.memGB} GB ${s.unified ? 'unified' : 'VRAM'}</td>
                <td class="impact-neutral">${speedEffect}</td>
                <td>${capacityEffect}</td>
                <td style="font-size:0.84em;color:#555">${upgrade}</td>
              </tr>`;
            }).join('')}
          </tbody>
        </table>
      </div>`;

    // ── Factor weighting explanation ──
    const factorIntro = `
      <div class="cmp-section">
        <div class="cmp-section-title">What Each Factor Contributes to AI Inference Performance</div>
        <table class="cmp-table">
          <thead><tr>
            <th>Factor</th><th>Impact on Speed</th><th>Impact on Model Access</th><th>Notes</th>
          </tr></thead>
          <tbody>
            <tr>
              <td><strong>Memory Bandwidth</strong></td>
              <td><span class="speedup-tag speedup-high">Primary driver</span></td>
              <td>None</td>
              <td>Token generation speed scales linearly with bandwidth. Doubling bandwidth doubles t/s.</td>
            </tr>
            <tr>
              <td><strong>RAM / VRAM Capacity</strong></td>
              <td><span class="speedup-tag speedup-neutral">No effect</span></td>
              <td><span class="speedup-tag speedup-high">Primary driver</span></td>
              <td>More capacity lets larger models fit. Does not speed up inference of models that already fit.</td>
            </tr>
            <tr>
              <td><strong>GPU Compute (CUDA/GPU cores)</strong></td>
              <td><span class="speedup-tag speedup-low">Secondary</span></td>
              <td>None</td>
              <td>For inference at batch-size 1, compute is rarely the bottleneck. Matters more for batched/parallel requests and training.</td>
            </tr>
            <tr>
              <td><strong>CPU Core Count</strong></td>
              <td><span class="speedup-tag speedup-medium">Moderate (CPU-only)</span></td>
              <td>None</td>
              <td>On CPU-only inference, more physical cores allow parallel matrix rows. Limited by memory bandwidth ceiling.</td>
            </tr>
            <tr>
              <td><strong>Quantisation Level</strong></td>
              <td><span class="speedup-tag speedup-high">Major (software)</span></td>
              <td><span class="speedup-tag speedup-high">Major (software)</span></td>
              <td>Reduces bytes per weight — directly cuts bandwidth demand and memory footprint. Q4 is ~3× faster than FP16 for same bandwidth.</td>
            </tr>
            <tr>
              <td><strong>Storage Speed (NVMe)</strong></td>
              <td><span class="speedup-tag speedup-neutral">No effect at runtime</span></td>
              <td>None</td>
              <td>Only affects initial model load time. A 7 GB model with a 3 GB/s NVMe loads in ~2s vs ~10s on SATA. No effect on tokens/sec.</td>
            </tr>
            <tr>
              <td><strong>CPU Architecture (AVX-512, AMX)</strong></td>
              <td><span class="speedup-tag speedup-medium">Moderate (CPU-only)</span></td>
              <td>None</td>
              <td>AVX-512 VNNI (Icelake+) can give 1.4× CPU inference vs AVX2. Intel AMX (Sapphire Rapids) provides further acceleration for INT8.</td>
            </tr>
          </tbody>
        </table>
      </div>`;

    document.getElementById('compareMain').innerHTML =
      headerHtml + factorIntro + specsHtml + tokHtml + fitHtml + ramImpactHtml;
  }
</script>
</body>
</html>